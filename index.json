[{"title":"课程资料","url":"/courses/course_catalog/","section":"courses","date":"0001-01-01","summary":"AI课程链接 更多AI方面的课程与学习资料，我整理并收藏了如下主题的网站链接，欢迎访问！\nAI入门-学习 https://www.deeplearning.ai/ https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ https://karpathy.ai/zero-to-hero.html https://learn.nvidia.com/ https://tensorflow.google.cn/tutorials?hl=zh-cn https://dafriedman97.github.io/mlbook/content/introduction.html https://d2l.ai/ https://andrewkchan.dev/posts/yalm.html?utm_source=hackernewsletter\u0026amp;utm_medium=email\u0026amp;utm_term=data https://www.promptingguide.ai/zh https://blog.frognew.com/2025/01/agents-whitepaper-by-google.html https://cloud.google.com/discover/what-is-prompt-engineering?hl=zh_cn https://www.skills.google/ 微软-AI编程课程 https://www.linkedin.com/learning/ethics-in-the-age-of-generative-ai/generative-ai-and-ethics-the-urgency-of-now https://learn.microsoft.com/en-us/collections/nq2b20y286pnj https://www.linkedin.com/learning/generative-ai-the-evolution-of-thoughtful-online-search/how-finding-and-sharing-information-online-has-evolved https://learn.microsoft.com/en-us/collections/0d6so53jrrgpq https://microsoft.github.io/AI-For-Beginners/ https://microsoft.github.io/generative-ai-for-beginners/#/ Anthropic的AI课程 https://docs.anthropic.com/en/home https://www.anthropic.com/engineering/building-effective-agents https://docs.anthropic.com/en/docs/claude-code/tutorials https://github.com/anthropics/prompt-eng-interactive-tutorial https://github.com/anthropics/courses/blob/master/prompt_evaluations/README.md https://github.com/anthropics/courses/blob/master/real_world_prompting/README.md https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/README.md#anthropic-api-fundamentals https://www.anthropic.com/learn/claude-for-work https://www.anthropic.com/ai-fluency/overview https://www.anthropic.com/engineering/claude-code-best-practices https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview https://docs.anthropic.com/zh-CN/docs/claude-code/overview https://docs.anthropic.com/en/docs/claude-code/overview ","description":"网络课程推荐","tags":[],"categories":[],"content":"AI课程链接 更多AI方面的课程与学习资料，我整理并收藏了如下主题的网站链接，欢迎访问！\nAI入门-学习 https://www.deeplearning.ai/ https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ https://karpathy.ai/zero-to-hero.html https://learn.nvidia.com/ https://tensorflow.google.cn/tutorials?hl=zh-cn https://dafriedman97.github.io/mlbook/content/introduction.html https://d2l.ai/ https://andrewkchan.dev/posts/yalm.html?utm_source=hackernewsletter\u0026amp;utm_medium=email\u0026amp;utm_term=data https://www.promptingguide.ai/zh https://blog.frognew.com/2025/01/agents-whitepaper-by-google.html https://cloud.google.com/discover/what-is-prompt-engineering?hl=zh_cn https://www.skills.google/ 微软-AI编程课程 https://www.linkedin.com/learning/ethics-in-the-age-of-generative-ai/generative-ai-and-ethics-the-urgency-of-now https://learn.microsoft.com/en-us/collections/nq2b20y286pnj https://www.linkedin.com/learning/generative-ai-the-evolution-of-thoughtful-online-search/how-finding-and-sharing-information-online-has-evolved https://learn.microsoft.com/en-us/collections/0d6so53jrrgpq https://microsoft.github.io/AI-For-Beginners/ https://microsoft.github.io/generative-ai-for-beginners/#/ Anthropic的AI课程 https://docs.anthropic.com/en/home https://www.anthropic.com/engineering/building-effective-agents https://docs.anthropic.com/en/docs/claude-code/tutorials https://github.com/anthropics/prompt-eng-interactive-tutorial https://github.com/anthropics/courses/blob/master/prompt_evaluations/README.md https://github.com/anthropics/courses/blob/master/real_world_prompting/README.md https://github.com/anthropics/courses/blob/master/anthropic_api_fundamentals/README.md#anthropic-api-fundamentals https://www.anthropic.com/learn/claude-for-work https://www.anthropic.com/ai-fluency/overview https://www.anthropic.com/engineering/claude-code-best-practices https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview …"},{"title":"基于Claude Agent SDK的文档摘要处理工具开发记录","url":"/labs/development-journey/","section":"labs","date":"2026-01-25","summary":"基于Claude Agent SDK的文档摘要处理工具开发记录 这篇文档记录了我如何一步步构建一个基于 Claude Agent SDK 的文档摘要生成工具。整个过程并非一帆风顺——从最初的 API 选型困惑，到摘要格式的反复调整，再到文件命名规则的多次重构，每一步都伴随着问题的发现与解决。\n起点：一个简单的需求 事情的起点很简单：我有一个 Clippings 文件夹，里面存放着从各处收集的文章（接近300篇）。这些文章越积越多，却很少被真正阅读和消化。我希望有一个工具，能够批量处理这些文章，为每篇生成一张摘要卡片——既保留核心信息，又节省阅读时间（最后，我将其上传到自己的网站中，即精选阅读模块）。\n最初的代码框架已经存在，使用的是智谱 AI 的 GLM 模型（这是我最初的设想，但还没有完全调通）。但我希望切换到 Claude Agent SDK，因为它提供了更强大的 agent 能力，而且我已经配置好了 Claude Code 环境。接下来的工作是由amp完成的，因为我有免费赠送的10美刀额度，整个thread都使用的smart模型（调用Claude Opus 4.5, 大概花费$7）\n第一个挑战：理解 Claude Agent SDK 当我说\u0026quot;使用 Claude Agent SDK\u0026quot;时，虽然我之前用过几次，但其实对它的详细用法印象并不深刻，只有残存于记忆中的模糊印象。我让amp帮我通过查阅最新文档，了解到 Claude Agent SDK 是一个基于 Claude Code 构建的 Python/TypeScript 库，核心 API 是异步的 query() 函数：\nfrom claude_agent_sdk import query, ClaudeAgentOptions async for message in query( prompt=\u0026#34;你的提示词\u0026#34;, options=ClaudeAgentOptions(allowed_tools=[]) ): # 处理响应 与传统的同步 API 调用不同，这里返回的是一个异步迭代器，需要用 async for 来遍历消息流。这意味着整个处理逻辑都需要重构为异步模式。\n","description":"记录基于Claude Agent SDK构建文档摘要生成工具的完整开发过程，包括API选型、格式设计、文件命名等挑战的解决方案。","tags":["development","claude-sdk","documentation","automation"],"categories":[],"content":"基于Claude Agent SDK的文档摘要处理工具开发记录 这篇文档记录了我如何一步步构建一个基于 Claude Agent SDK 的文档摘要生成工具。整个过程并非一帆风顺——从最初的 API 选型困惑，到摘要格式的反复调整，再到文件命名规则的多次重构，每一步都伴随着问题的发现与解决。\n起点：一个简单的需求 事情的起点很简单：我有一个 Clippings 文件夹，里面存放着从各处收集的文章（接近300篇）。这些文章越积越多，却很少被真正阅读和消化。我希望有一个工具，能够批量处理这些文章，为每篇生成一张摘要卡片——既保留核心信息，又节省阅读时间（最后，我将其上传到自己的网站中，即精选阅读模块）。\n最初的代码框架已经存在，使用的是智谱 AI 的 GLM 模型（这是我最初的设想，但还没有完全调通）。但我希望切换到 Claude Agent SDK，因为它提供了更强大的 agent 能力，而且我已经配置好了 Claude Code 环境。接下来的工作是由amp完成的，因为我有免费赠送的10美刀额度，整个thread都使用的smart模型（调用Claude Opus 4.5, 大概花费$7）\n第一个挑战：理解 Claude Agent SDK 当我说\u0026quot;使用 Claude Agent SDK\u0026quot;时，虽然我之前用过几次，但其实对它的详细用法印象并不深刻，只有残存于记忆中的模糊印象。我让amp帮我通过查阅最新文档，了解到 Claude Agent SDK 是一个基于 Claude Code 构建的 Python/TypeScript 库，核心 API 是异步的 query() 函数：\nfrom claude_agent_sdk import query, ClaudeAgentOptions async for message in query( prompt=\u0026#34;你的提示词\u0026#34;, options=ClaudeAgentOptions(allowed_tools=[]) ): # 处理响应 与传统的同步 API 调用不同，这里返回的是一个异步迭代器，需要用 async for 来遍历消息流。这意味着整个处理逻辑都需要重构为异步模式。\n我们写了一个简单的测试脚本来验证 SDK 是否正常工作：\nasync def test_basic_query(): async for message in query( prompt=\u0026#34;请用中文简单介绍一下你自己\u0026#34;, options=ClaudeAgentOptions(allowed_tools=[]) ): if hasattr(message, \u0026#39;result\u0026#39;): print(f\u0026#34;结果: {message.result}\u0026#34;) 测试通过后，我们才开始正式改造主程序。\n设计工作流程 接下来需要设计清晰的工作流程。最终确定的目录结构是：\nClippings/ ├── toread/ # 待处理的文章 ├── summary/ # 生成的摘要卡片 └── _archived/ # 已处理的原文 工作流程很直观：从 toread 读取文章，生成摘要后保存到 summary，同时将原文移动到 _archived。这样既保留了原文的可追溯性，又让待处理队列保持清晰。\n摘要格式的演进 摘要的格式经历了几轮调整。\n最初的设计包含\u0026quot;核心观点\u0026quot;、\u0026ldquo;关键要点\u0026rdquo;、\u0026ldquo;金句摘录\u0026rdquo;、\u0026ldquo;延伸思考\u0026quot;四个部分，采用分点列举的形式。但这种格式显得有些机械，更像是填空题而非真正的内容理解。\n经过讨论，我们将格式调整为三个部分：\n摘要：100字左右的精炼概括 内容框架与概述：用自然段落描述文章脉络，而非分点罗列 核心概念及解读：提取关键词并结合文章进行解读 这种格式更贴近人类的阅读习惯——先了解大意，再理解结构，最后掌握核心概念。\n关于模板文件，我们发现一个有趣的问题：如果模板中包含具体的示例文字（比如\u0026quot;文章开篇从某个现象切入\u0026hellip;\u0026quot;），AI 可能会照搬这些措辞，产生硬编码效果。解决方案是用占位符替代具体示例：\n## 内容框架与概述 {用2-4个自然段落概述文章结构与核心论述} 漫长的等待：日志系统的引入 第一次批量处理时，我们遇到了一个实际问题：命令执行后长时间没有任何输出，直到所有文章处理完毕才一次性返回结果。对于几十篇文章的批量处理，这意味着要等待很长时间而不知道进度如何。\n解决方案是引入日志系统。在每次批处理开始时，先创建一个日志文件，列出所有待处理的文件：\n# 摘要生成任务日志 | 序号 | 文件名 | 状态 | …"},{"title":"博客 Telegram 频道自动化推送实战记录","url":"/labs/telegram-blog-integration-journey-v2/","section":"labs","date":"2026-01-24","summary":"详细记录了如何通过 GitHub Actions、Node.js 脚本与 Telegram Bot API 实现博客更新自动推送到频道，并配置 Instant View 的完整过程与踩坑经验。涵盖了从基础 Bot 申请到解决时序竞争、Instant View 适配及 Web 端兼容性等深度细节。","description":"","tags":["DevOps","Telegram","GitHub Actions","Automation","Instant View"],"categories":[],"content":"为了方便读者及时获取更新，近期我为博客增加了 Telegram 频道自动推送功能。目标很明确：每当有新文章发布时，GitHub Actions 自动触发脚本，将文章标题、摘要和链接发送到订阅频道，并且支持 Instant View（即时预览）以提供丝滑的阅读体验。\n本文整合了整个开发过程中的实战经验，特别是解决“时序竞争”和“Instant View 适配”等棘手问题的方案。\n1. 基础架构设计 整个自动化链路的基础逻辑如下：\nGit 检测：在 CI/CD 流程中检测本次 Commit 新增的 Markdown 文件。 内容解析：读取文件 Frontmatter，提取标题、日期、标签和摘要。 API 推送：调用 Telegram Bot API 将消息发送到指定频道。 1.1 申请机器人与频道配置 这是最基础的一步：\n在 Telegram 找 @BotFather 申请 Bot，获取 API Token。 建立频道（Channel），将 Bot 拉入并设为管理员。 通过转发消息给 @getidsbot 获取频道的 Chat ID（通常以 -100 开头）。 1.2 自动化脚本逻辑 使用 Node.js 编写脚本 (scripts/telegram-notify.js)。核心难点在于准确识别“新文章”。最初我只检测 Added (A) 状态的文件，后来发现重命名文件（如修正文件名拼写）会导致漏推。\n解决方案：将 git diff 的过滤器升级为 ACR，覆盖 Add、Copy、Rename 三种情况。\n// 示例：获取变更文件列表 let command = \u0026#39;git diff --name-only --diff-filter=ACR HEAD~1 HEAD\u0026#39;; 2. 核心挑战与踩坑记录 虽然原理简单，但在实际落地过程中遇到了不少意想不到的坑。\n2.1 时序陷阱：通知发了，预览却挂了 现象： 消息成功发送到了频道，但无论是在 App 端还是 Web 端，都没有显示文章的预览卡片（Preview Card）。\n原因分析： 这是一个典型的**竞争条件（Race Condition）**问题。 最初的工作流顺序是：构建 -\u0026gt; 发布 -\u0026gt; 通知。 看似合理，但 GitHub Pages 的部署（Deploy）步骤完成后，CDN 的缓存刷新和全球分发需要时间。当脚本立即触发 Telegram Bot 发送消息时，Telegram 的爬虫服务器（Crawler）立刻去抓取该链接，此时新文章在 CDN 节点上可能还未生效（返回 404）。一旦爬虫抓取失败，它就会缓存这个失败结果，导致预览卡片无法生成。\n解决方案： 调整 GitHub Actions 的执行顺序，并强制增加等待时间。\n顺序调整：确保 通知 步骤严格在 部署 步骤成功之后执行。 增加延迟：在两者之间插入 sleep 60，给 GitHub Pages 的 CDN 留出足够的传播时间。 - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v4 # ... 配置省略 ... - name: Wait for deployment propagation run: sleep 60 # 关键：等待 CDN 生效 - name: Check for new content and notify if: success() run: node scripts/telegram-notify.js 2.2 Instant View 的“严苛”规范 为了提升阅读体验，我配置了 Telegram Instant View。但在测试规则时报错：Element \u0026lt;img\u0026gt; is not supported in \u0026lt;p\u0026gt;。\n原因： Hugo 或其他 Markdown 渲染器通常会将图片包裹在 \u0026lt;p\u0026gt; 标签中（例如 \u0026lt;p\u0026gt;\u0026lt;img ...\u0026gt;\u0026lt;/p\u0026gt;）。然而，Telegram Instant View 的规范非常严格，不允许块级的 \u0026lt;img\u0026gt; 元素出现在 \u0026lt;p\u0026gt; 内部。\n解决方案： 在 IV 模板规则中使用 @split_parent 指令，强制将图片从段落中“剥离”出来。\n# Fix: \u0026lt;img\u0026gt; inside \u0026lt;p\u0026gt; is not allowed in Instant View @split_parent: //p/img 此外，对于博客中嵌入的复杂 HTML 组件（如交互式图表、Hugo Shortcodes），Instant View 无法渲染，直接展示会显得凌乱。因此需要专 …"},{"title":"“下沉贴”的爆火与“流量黑洞”中的自嗨","url":"/post/low_tech_guide_post_boom/","section":"post","date":"2025-10-30","summary":"面对“下沉贴”的爆火，起号内容的频出，我陷入了沉思。不能说读者不对，毕竟东西能火一定是有原因的。\n但又总觉着那里不对，好像是件很讽刺的事情：本来X还自诩为信息排泄链的源头，产出高质量内容，现在却在吸取来自尾端的东西，似乎还甘之如饴\u0026hellip;\n有时候惊叹自己的无知。但稍微了解后感想：还是继续保持无知吧。\n","description":"","tags":["X平台","内容创作","病毒传播","科技限制","社区互动"],"categories":["技术"],"content":"面对“下沉贴”的爆火，起号内容的频出，我陷入了沉思。不能说读者不对，毕竟东西能火一定是有原因的。\n但又总觉着那里不对，好像是件很讽刺的事情：本来X还自诩为信息排泄链的源头，产出高质量内容，现在却在吸取来自尾端的东西，似乎还甘之如饴\u0026hellip;\n有时候惊叹自己的无知。但稍微了解后感想：还是继续保持无知吧。\n让我惊叹的事情是最近在推上爆火的两个帖子，一个是注册美区Apple ID的教程，一个是注册Gmail的教程，前者260万流量，后者刚开贴，也有20万。这种流量的爆炸让我震惊，惊叹自己常自诩为大明白，竟然对一些“显而易见”的现实如此无知。\n美区的Apple ID，我在两年多之前注册过，当时GPT刚出iOS版的客户端，我下载不了。一位朋友说美区的可以下载，于是我就在网上随便搜了一下教程，然后按照教程把美区的ID注册了，下载了GPT客户端使用至今。\n昨天一看，发现这个爆火的帖子和我当年用的教程几乎一模一样，包括我的ID地址也选的俄勒冈州。当然，这个帖子引发的热议让我想起来了这个账号，于是我把iPhone切到美区，充了个礼品卡，开了一个X的basic会员。\n如果说开美区的Apple ID还有些难度，那么注册Gmail也需要教程就有点让我震撼了。\n我大概有四个Gmail，其中有两个是在去年年底注册，三个是美区的地址，前段时间白嫖了几个月的Gemini pro，爽了一把。\n我在注册的过程中没有碰到任何技术困难，但是现在大家如此渴望一个详细的教程，不知是不是因为现在中美关系紧张，Google对中国的用户越发不友好。\n这个经验帖可能考虑了一些复杂的情况，比如注册时需要手机验证码，国内的手机号会经常出现收不到验证码的情况，说明这个帖子应该有一定的实用价值，但是能如此爆火就有点让我吃惊。\n我刚才转发了一下那个帖子并发表了惊叹，可能带着一些吐槽。我说“现在的X平台都那么下沉了吗？能翻墙出来，还不会注册Gmail”。随后贴主在我的转发下评论“嗯，是的，他们确实不知道”。然后他又说自己在招一对一学员，问我要不要学习起号，我婉拒。一方面，被上门推销，有种被惦记的感觉，略感不适。另外，这种询问让我感到有点受刺激，毕竟我发推这么多关注者也不到1000，确实挺失败，被推销起号教学，像是一种嘲讽。\n我研究了一下，这位贴主fo数这几天从300涨到了3000，也算是一时间最火的“起号达人”。不过看到他在此前发了1000多条，并没有多少粉，说明也是突然找到了流量密码。那么他的“秘笈”应该不难推测，大概就是下沉吧。\n他的几个爆火贴都是极度下沉的内容，这些内容大概是多年前百度贴吧或百度知道里很平常的内容，与之相比，CSDN、知乎以及各大博客里的内容都显得过于高级。在技术方面，我一直都爱折腾，对这些内容很熟悉，许多基本技能也都是在那上面学会的。不过没想到那些东西在X上还能爆火，说明不同时代的人可能所处的平台环境不一样，但是内容需求都还是那些。\n那么，我似乎也可以搞一搞，毕竟我现在也可以写长贴了，而且那种“教程卡片”我也早就驾轻就熟。但做那些确实兴致不高。我承认，那些技能我很熟练，因为算得上是“基本功”，但是确实没有兴趣回头再梳理一遍的动力了，即便很可能带来流量。\n我喜欢的是技术，尤其是新的技术。最近越发悲剧性地发现，我所想写的都是些“流量黑洞”，但如果非得做出选择，我可以放弃流量。\nX终究也只是马斯克的地盘，我不可能在这里永居，说不定哪天就被封了。但是技术学到手了，短期能够让我快乐，长期来看说不定能够用来做些什么。\n嗯，我是来学习的而不是来传教的。因为下沉贴而涨粉，吸引到的也不见得是自己想交的朋友。来X，更主要的原因还是可以观察乃至有机会与真正的技术大佬交往。\n如果要获得流量，那就必须要顺从于算法。但是越是了解算法和“流量密码”，我就越想逃离。我习惯了在各大平台中流浪，我从公众号逃离了，从知乎逃离了，从小红书逃离了，从抖音逃离了，甚至B站，我也越来越感觉无趣。\n我在X待了几个月，并不是因为这里能涨粉，而是因为这里有种“社区感”，我能结交到一些有趣的人。但是，最终，我的宿命可能还是逃离，因为曾经温暖过我的“社区感”似乎随着天气渐凉而逐渐降温，不断吹来的是原以为独属于国内社交平台的流量文化风潮，带来的是一种熟悉而腐败的气息。\n当然，可能X平台本来就是如此，只是他的算法是的用户不像国内平台那样总是推送不想看的内容。如此一来，既然不适，可以用自己的行动来清洗调整时间线。我尽量尝试，但又感觉越来越难调整。\n我从小不善交际，但是从来没为这种状态感到沮丧，可能是很早就习惯了孤独。或许我终究只是自嗨，因为能与我同频在一起狂欢的人，一共也没几个，而我现在有800多关注, 或许已经是我需求的极限了。\n对我而言，逃离与乃至“流浪”对我来说不是很糟糕。能嗨起来很重要，管它是不是自嗨呢？\n"},{"title":"「Scripts」Module 1: Introduction to Agentic Workflows","url":"/courses/andrew-ng-agentic-ai/lecture/lec-01/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ Andrew Ng: Agentic AI Module 1: Introduction to Agentic Workflows 1.0 Introduction 0:02 Welcome to this course on Agentic AI. When I coined the term agentic to describe what I saw 0:05 as an important and rapidly growing trend in how people were building on-base applications, 0:10 what I did not realize was that a bunch of marketers would get hold of this term 0:14 and use it as a sticker and put this on almost everything in sight. And that has caused hype 0:20 on Agentic AI to skyrocket. The good news though is that ignoring the hype, the number of truly 0:27 valuable and useful applications built using Agentic AI has also grown very rapidly, even if 0:33 not quite as rapidly as the hype. And in this course, what I\u0026#39;d like to do is show you best 0:38 practices for building Agentic AI applications. And this will open up a lot of new opportunities 0:44 to you in terms of what you can now build. Today, agentic workflows are being used to build 0:49 applications like customer support agents, or to do deep research to help write deeply insightful 0:55 research reports, or to process tricky legal documents, or to look at patient input and render 1:02 or to suggest possible medical diagnoses. On many of my teams, a lot of the projects we built just 1:08 would be impossible without agentic workflows. And so knowing how to build applications with them 1:14 is one of the most important and valuable skills in AI today. It turns out that one of the biggest 1:20 differences I\u0026#39;ve seen between people that really know how to build agentic workflows compared to 1:24 people that are less effective at it is the ability to drive a disciplined development process, 1:30 specifically one focused on evals and error analysis. And in this course, I\u0026#39;ll tell you what 1:35 that means and show you what allows you to be really good at building these agentic workflows. 1:41 Being able to do this is one of the most important skills in AI today and will open up a lot more 1:46 opportunities, be it job opportunities or opportunities to just build amazing software 1:51 yourself. With that, let\u0026#39;s go on to the next video to dive more into what are agentic workflows. 1.1 What is an agentic AI 0:03 So what is Agentic AI and why are Agentic AI workflows so powerful? Let\u0026#39;s take a look. 0:05 The way that many of us use large language models or LLMs today is by prompting it to say, 0:11 write an essay for us on a certain topic X. And I think of that as akin to going to a human, 0:17 or in this case, going to an AI and asking it to please type out an essay for me by writing 0:22 from the first word to the last word all in one go and without ever using backspace. 0:27 It turns out that we as people, we don\u0026#39;t do our best writing like that by being forced to write 0:32 in this completely linear order and nor do AI models. But despite the difficulty of being 0:38 constrained to write in this way, our LLMs do surprisingly well. In contrast, with an agentic 0:43 workflow, this is what the process might look like. You may ask it to first write an essay 0:48 outline on a certain topic, then ask if it needs to do any web research. And after doing some web 0:53 research and maybe downloading some web pages, then to write the first draft and then to read 0:57 the first draft and see what parts need revision or more research and then revise the draft and so 1:02 on. And this type of workflow is more akin to doing some thinking and some research and then 1:06 doing some revision and then doing some more thinking and so on. And with this iterative 1:12 process, it turns out that an agentic workflow can take longer, but it delivers a much better 1:18 work product. So an agentic AI workflow is a process where an LLM based app executes multiple 1:25 steps to complete a task. In this example, you might use an LLM to write the first essay outline 1:31 and then you might use an LLM to decide what search terms to type into a web search engine or 1:37 really what search terms to call a web search API with in order to get back relevant web pages. 1:44 Based on that, you can feed the downloaded web pages into an LLM to have it write the first draft 1:51 and then maybe use another LLM to reflect and decide what needs more revision. And then depending 1:57 on how you design this workflow, perhaps you may even add a human in the loop step where the LLM 2:03 has the option to request human review, maybe of some key facts. And based on that, it may then 2:10 revise the draft and this process results in a much better work output. One of the key skills 2:16 you learn in this course is how to take a complex task like writing an essay and breaking it down 2:24 into smaller steps for agentic workflows to execute one step at a time to then get the work output 2:31 that you want. And knowing how to decompose the task into steps and how to build the components 2:36 to execute the individual steps well turns out to be a tricky but important skill that will 2:42 determine your ability to build agentic workflows for a huge range of exciting applications. In this 2:48 course, a running example that we\u0026#39;ll use and something that you build alongside me is a research 2:54 agent. So here\u0026#39;s an example of what it will look like. You can enter a research topic like how do I 3:00 build a new rocket company to compete with SpaceX. I don\u0026#39;t personally want to compete with SpaceX, 3:08 but if you want to, you can try asking a research agent to help your background research. 3:13 So this agent starts with planning out what research to use, including calling a web search 3:20 engine to download some web pages, and then to synthesize and rank findings, draft an outline, 3:26 have an editor-to-agent review for coherence, and then finally generate a comprehensive 3:30 markdown report, which it has done here, building a new rocket company to compete with SpaceX 3:35 with an intro, background, findings, and so on. I think it points out appropriately that 3:41 this is going to be a tough startup to build, so I\u0026#39;m not personally planning to do this, but if 3:47 you want to tackle something like this, maybe a research agent like this could help you with some 3:53 initial research. And by finding and downloading multiple sources and deeply thinking about it, 4:00 this actually ends up with a much more thoughtful report than just prompting an LLM to write an essay 4:07 for you would. One of the reasons I\u0026#39;m excited about this is because in my work, I\u0026#39;ve ended up 4:12 building quite a few specialized research agents, be it in legal documents for conflict legal 4:18 compliance, or for some healthcare sectors, or some business product research areas. And so I hope 4:24 that working through this example, you not only learn how to build agentic workflows for many 4:29 other applications, but that some of the ideas in building research agents will be directly useful 4:34 to you if you ever need to build a custom research agent yourself. Now, one of the often discussed 4:42 areas of AI agents is how autonomous are they? What you just saw here was a relatively complex, 4:49 highly autonomous Agentic AI workflow, but there are also other simpler workflows that are 4:55 incredibly valuable. Let\u0026#39;s go on to the next video to talk about the degree to which agentic 5:00 workflows can be autonomous, and does it give you a framework to think about how 5:05 you might go about building different applications and how easy or difficult they might be. 5:10 See you in the next video. 1.2 Degrees of autonomy 0:02 Agents can be autonomous to different degrees. A few years ago, I noticed within the AI community 0:06 that there was a growing controversial debate about what is an agent, and some people are 0:10 writing a paper saying I built an agent, and others will say, no, that\u0026#39;s not really a true 0:14 agent. And I felt this debate was unnecessary, which is why I started using the term agentic, 0:21 because I thought if we use it as an adjective rather than a binary, it\u0026#39;s either an agent or not, 0:26 then we\u0026#39;re going to have to acknowledge that systems can be agentic to different degrees. 0:31 And let\u0026#39;s just call it all agentic and move on with the real work of building these systems 0:36 rather than debating, you know, is this sufficiently autonomous to be an agent or not? 0:40 I remember when I prepared a talk on agentic reasoning, one of my team members actually 0:46 came to me and said, hey, Andrew, we don\u0026#39;t need yet another word. You know, we have agent, 0:50 why are you making up another word, agentic? But I decided to use it anyway. And then later on, 0:55 wrote an article in a given newsletter, The Batch, and then also posted on social media, 1:01 saying that instead of arguing over which word to include or exclude as being a true agent, 1:06 let\u0026#39;s acknowledge that different degrees to which systems can be agentic. And I think this helped 1:11 move past the debate on what is a true agent and let us just focus on actually building them. 1:18 Some agents can be less autonomous. So take the example of writing an essay about black holes. 1:25 You can have a relatively simple agent to come up with a few web search terms or web search queries. 1:31 Then you can hard code in that you call a web search engine, fetch some web pages, 1:37 and then use that to write an essay. And this is an example of a less autonomous agent 1:41 with a fully deterministic sequence of steps. And this will work okay. 1:46 In terms of notational convention, throughout this course, I\u0026#39;ll use the red color, 1:50 as you see here on the left, to denote the user input, such as a user query in this case, 1:56 or in later examples, maybe the input document into an agentic workflow. 2:00 The gray boxes denote calls to an LLM, and the green boxes, like the web search and the web 2:07 fetch boxes that you see here, indicate steps where other software is being used to carry out 2:13 an action, such as a web search API call or executing code to fetch the contents of a website. 2:18 Then an agent can be more autonomous, where, given a request to write an essay about black holes, 2:24 perhaps you let the LLM decide, does it want to do a web search, or does it want to search 2:29 recent news sources, or does it want to search for recent research papers on the website archive? 2:34 Based on that, maybe in this example, the LLM, not the human engineer, but the LLM chooses, 2:39 in this case, to call a web search engine, and then after that, you may let the LLM decide 2:44 how many web pages does it want to fetch, or if it fetches the PDF, does it need to 2:50 call a function, or also call a tool, to convert the PDF to text? 2:53 And in this case, maybe it fetches its top few web pages, then it can write an essay, 2:59 decide whether to reflect and improve, and maybe even go back to fetch more web pages, 3:03 and then to finally produce an output. 3:06 And so even for this example of a research agent, we can see that some agents can be 3:13 less autonomous, with a linear sequence of steps to be executed, determined by a programmer, 3:18 and some can be more autonomous, where you trust the LLM to make more decisions, 3:22 and the exact sequence of steps that happens may be even determined by the LLM, rather than 3:26 in advance by the programmer. 3:28 So for less autonomous systems, you will usually have all the steps predetermined in advance, 3:34 and any functions it calls, like web search, and we\u0026#39;ll call that tool use, as you learn 3:38 in the third module in this course, might be hard-coded by the human engineer, by you 3:43 or me, and most of the autonomy is in what text the LLM generates. 3:47 At the end of the spectrum would be highly autonomous agents, where the agent makes many 3:51 decisions autonomously, including, for example, deciding what is the sequence of steps it 3:56 will carry out in order to write the essay. 3:59 And there\u0026#39;s some highly autonomous agents that can even write new functions, or sometimes 4:04 create new tools that it can then execute. 4:06 And somewhere in between are semi-autonomous agents, where it can make some decisions, 4:11 choose tools, but the tools are usually more predefined. 4:14 As you look at different examples in this course, you learn how to build applications 4:18 anywhere on this spectrum of less to more highly autonomous, and you find that there 4:22 are tons of applications in the less autonomous end of the spectrum that are very valuable 4:26 being built for tons of businesses today, and at the same time, there are also applications 4:31 being worked on at the more highly autonomous end of the spectrum, but those are usually 4:36 less easily controllable, a little bit more unpredictable, and also a lot of active research 4:42 as well to figure out how to build these more highly autonomous agents. 4:46 And with that, let\u0026#39;s go on to the next video to dive deeper into this and to hear about 4:53 some of the benefits of using agents and why they allow us to do things that just were 4:57 not possible with earlier generations of base applications. 1.3 Benefits of agentic workflows 0:03 I think the one biggest benefit of agentic workflows is that it allows you to do many 0:04 tasks effectively that just previously were not possible. But there are other benefits as well, 0:10 including parallelism that lets you do certain things quite fast, as well as modularity that 0:14 lets you combine the best of three components from many different places to build an effective 0:19 workflow. Let\u0026#39;s take a look. My team collected some data on a coding benchmark that tests the 0:26 ability of different LLMs to write code to carry out certain tasks. The benchmark used in this case 0:32 is called Human Eval, and it turns out that GPT 3.5, this is a model that the first publicly 0:38 available version of Chat GPT was based on, if asked to write the code directly, to just type 0:42 out the computer program, gets 40% right on this benchmark. This is a positive k-metric. GPT 4 is 0:49 a much better model. Its performance leaps to 67% with this also non-agentic workflow. But it turns 0:56 out that as large as the improvement was from GPT 3.5 to GPT 4, that improvement is dwarfed by what 1:03 you can achieve by wrapping GPT 3.5 within an agentic workflow. Using different agentic techniques, 1:10 which you\u0026#39;ll learn about later in this course, you can prompt GPT 3.5 to write code and then maybe 1:15 reflect on the code and figure out if you can improve it. And using techniques like that, you can 1:20 actually get GPT 3.5 to get much higher levels of performance. And similarly, GPT 4 used in the 1:26 context of an agentic workflow also does much better. So even with today\u0026#39;s best LLMs, an agentic 1:33 workflow lets you get much better performance. And in fact, what we saw in this example was the 1:39 improvement from one generation of model to another, which is huge, is still not as big a difference 1:45 as implementing an agentic workflow on the previous generation of model. Another benefit of 1:51 using agentic workflows is that they can parallelize some tasks and thus do certain things much faster 1:57 than a human. For example, if you ask an agentic workflow to write an essay about black holes, you 2:04 might be able to have three LLMs run in parallel to generate ideas for web search terms to type 2:10 into the search engine. Based on the first web search, it may identify, say, three top results to 2:16 fetch. And based on the second web search, it may identify a second set of web pages to fetch and so 2:22 on. And it turns out that whereas a human doing this research would have to read these nine web 2:28 pages sequentially or one at a time, when you\u0026#39;re using an agentic workflow, you can actually 2:33 parallelize all nine web page downloads and then finally feed all these things into an LLM to write 2:39 an essay. So even though agentic workflows do take longer than truly non-agentic workflows or by 2:45 direct generation by just prompting a single time, if you were to compare this type of agentic 2:51 workflow to how a human would have to go about the task, the ability to parallelize downloading lots 2:56 of web pages can actually let it do certain tasks much faster than the non-parallel sequential way 3:02 that a single human might process this data. To build on this example, it turns out one of the 3:08 things I often do when building agentic workflows is look at the individual components like the LLM 3:14 and add or swap out components. So for example, maybe I look at the web search engine I use up 3:20 here and I might decide that I want to soften a new web search engine. When building agentic 3:24 workflows, there are actually multiple web search engines including Google, which you can access 3:29 by a server, as well as others like Bing, DuckDuckGo, Tavily, u.com. There are actually quite a lot of 3:35 options for web search engines designed for LLMs to use. Or maybe instead of just doing three web 3:40 searches, maybe on this step we can swap in a new news search engine so we can find out what\u0026#39;s the 3:47 latest news on recent breakthroughs on black hole science. And lastly, instead of using the same LLM 3:55 for all of the different steps, I will often try out different large language models and maybe try 4:01 out different LLM providers to see which one gives the best result for different steps of this system. 4:07 So to summarize, the main reason I use agentic workflows is it just gives much better performance 4:13 on many different applications. But in addition, it can also paralyze some tasks that humans would 4:20 otherwise have to do sequentially. And the modular design of many agentic workflows also lets us 4:27 add or update tools and sometimes swap out models. We\u0026#39;ve talked a lot about the key components of 4:33 building agentic workflows. Let\u0026#39;s now take a look at a range of Agentic AI applications to give you 4:39 a sense of the sorts of things people are already building and the sorts of things 4:43 you\u0026#39;ll build yourself. Let\u0026#39;s go on to the next video. 1.4 Agentic AI applications 0:00 Let\u0026#39;s take a look at some examples of Agentic AI applications. 0:04 One task that many businesses carry out is invoice processing. 0:09 So given an invoice like this, you might want to write software to extract the most important 0:14 fields, which for this application, let\u0026#39;s say is the biller, that would be tech flow solutions, 0:19 the biller address, the amount due, which is $3,000, and the due date, which looks like it is 0:25 August 20th, 2025. So in many finance departments, maybe a human would look at invoices and identify 0:33 the most important fields, who do we need to pay by when, and record these in a database to make 0:39 sure that payment is issued in time. If you were to implement this with an agentic workflow, 0:44 you might do so like this. You write input an invoice, then call a PDF to text conversion API 0:51 to turn the PDF into maybe formatted text, such as markdown text for the LLM to ingest. Then the LLM 0:57 will look at the PDF and figure out, is this actually an invoice or is this some other type 1:02 of document that they should just ignore? And if it is an invoice, then it will pull up the required 1:07 fields as well as use an API or use a tool to update the database in order to save the most 1:14 important fields in the database records. So one aspect of this agentic workflow is that 1:20 there is a clear process to follow, is identify the required fields and record in the database. 1:26 Tasks like these with a clear process you want followed tend to be maybe easier for agentic 1:31 workflows to carry out because it leads to a relatively step-by-step way to reliably carry 1:36 out this task. Here\u0026#39;s another example, maybe just a little bit harder. So if you want to build an 1:41 agent to respond to basic customer order inquiries, then the steps might be to extract the key 1:48 information, so figure out what exactly did the customer order, what\u0026#39;s the customer\u0026#39;s name, then 1:53 look up the relevant customer records, and then finally draft a response for human to review before 2:00 the email response is sent to the customer. So again, there\u0026#39;s a clear process here and we 2:04 will implement this step-by-step, where we take the email, feed it to an LLM to verify or to extract 2:10 the order details, and assuming the customer email is about an order, the LLM might then choose to 2:16 call an order\u0026#39;s database to then pull up that information. That information then goes to the LLM 2:22 to then draft an email response, and the LLM might choose to use a request review tool that, say, puts 2:29 this draft email from the LLM into queue for humans to review, so they can then be sent out after a 2:34 human has reviewed and approved it. So customer order inquiry agents like these are being built 2:40 and deployed in many businesses today. To look at a more challenging example, if you want to build a 2:45 customer service agent to respond not just to questions about an order they place, but to respond 2:51 to a more general set of questions, anything a customer may ask, and maybe the customer will ask, 2:57 do you have any black jeans or blue jeans? And to answer this question, you need to maybe make 3:03 multiple API calls to your database to first check the inventory for black jeans, then check inventory 3:08 for blue jeans, and then respond to the customer. So this is an example of a more challenging query, 3:14 where given a user input, you actually have to plan out what is the sequence of database queries 3:19 to check for inventory. Or if a user asks, I\u0026#39;d like to return the beach towel I bought, then to answer 3:25 this, maybe we need to verify that the customer actually bought a beach towel, and then double 3:31 check the return policy. Maybe our set returns only 30 days within the date of purchase, and only the 3:36 towel was unused. And if return is allowed, then have the agent issue a return packing slip, and 3:42 also set the database record to return pending. So in this example, if the required steps to process 3:48 the customer requests are not known ahead of time, then it results in a more challenging process, 3:54 where the LLM base application has to decide for itself that these are the three steps needed in 4:00 order to respond appropriately to this task. But you learn about some of the latest work on how to 4:06 approach this type of problem too. And to give one last example of maybe an especially difficult 4:12 type of agent to build, there\u0026#39;s a lot of work on computer use by agents, in which agents will 4:17 attempt to use a web browser and read a web page to figure out how to carry out a complex task. 4:24 In this example, I\u0026#39;ve asked an agent to check whether seats are available on two specific United 4:30 Airlines flights from San Francisco to Washington DC, or the DCA airport. The agent has access to 4:36 a web browser they can use to carry out this task. And in the video here, you can see it navigating 4:41 the United website independently, clicking on page elements and filling in the text fields on the page 4:46 to carry out the search that I requested. As it works, the agent reasons over the content of the 4:52 page to figure out the actions it needs to take to complete the task, and what it should do next. 4:57 In this case, there\u0026#39;s some trouble checking flights on the United site, and instead decides to 5:02 navigate to the Google Flights website to search for available flights. On the Google Flight, you 5:08 see here it finds several flight options that match the user\u0026#39;s query, and the agent then picks one and 5:13 is taken back to the United website, where it looks like it\u0026#39;s now on the correct web page, and so is 5:20 able to determine that yes, there are seats available on the flights that I asked about. So computer use 5:26 is an exciting cutting-edge area of research right now, and many companies are trying to get computer 5:30 use agents to work. While the agent you saw here did eventually figure out the answer, I often see 5:36 agents having trouble using web browsers well. For example, if a web page is slow to load, an agent 5:42 may fail to understand what\u0026#39;s going on, and many web pages are still beyond agents\u0026#39; abilities to 5:47 pause or to read accurately. But I think computer use agents, even though not yet reliable enough 5:53 to use mission-critical applications today, are an exciting and important area of future development. 5:58 So when I\u0026#39;m considering building Agentic AI workflows, the tasks that are easier will tend to be ones 6:05 where there is a clear step-by-step process, or if a business already has a standard procedure, a 6:11 standard offering procedure to follow, and then it can be quite a lot of work to take that procedure 6:15 and codify it up in an AI agent, but that tends to lead to easier implementations. One thing that 6:21 makes it easier is if you are using text-only assets, because LLM/language models have 6:27 grown up really processing text, and if you need to process other input modalities, it may well be 6:33 doable, but it maybe gets a little bit harder. And on the harder end of the spectrum, if the steps are 6:38 not known ahead of time of what\u0026#39;s needed to carry out a task, like you saw for the more advanced 6:42 customer service agent, then the agent may need to plan or solve as you go, and this tends to be 6:47 harder and more unpredictable and less reliable. And then as mentioned, if it needs to accept rich 6:52 multi-modal inputs such as sound, vision, audio, that also tends to be less reliable than the 6:58 only header process text. So I hope that gives you a sense of the types of applications you might build 7:05 with agentic workflows. When implementing one of these things yourself, one of the most important 7:10 skills is to look at a complex workflow and figure out what are the individual steps so you can 7:16 implement an agentic workflow to execute those steps one at a time. In the next video, we\u0026#39;ll talk 7:22 about task decomposition, that is, given a complex thing you want to do, like write a research report 7:28 or have a customer agent get back to customers, how do you break that down into discrete steps 7:34 to try to implement an agentic workflow? Let\u0026#39;s go see that in the next video. 1.5 Task decomposition: Identifying the steps in a workflow 0:00 People and businesses do a lot of stuff. How do you take this useful stuff that we do 0:06 and break it down into discrete steps for the agentic workflow to follow? Let\u0026#39;s take a look. 0:12 Take the example of building a research agent. If you want an AI system to write an essay on 0:18 a topic X, one thing you could do is prompt an LLM to have it generate an output directly. 0:24 But if you were to do this for topics that you want deeply researched, 0:28 you may find that the LLM output covers only the surface level points, or maybe covers only the 0:33 obvious facts, but doesn\u0026#39;t go as deep into the subject as you want it to. In this case, you 0:39 might then reflect on how you as a human would write an essay on a certain topic. Would you just 0:45 sit down and start writing, or would you take multiple steps, such as first write an essay 0:50 outline, and then search the rep, and then based on the input from the web search, write the essay. 0:55 As I take a task and decompose it into steps, one question I\u0026#39;m always asking myself is, 1:00 if I look at these steps one, two, and three, can each of them be done either by an LLM, or 1:07 by a short piece of code, or by a function call, or by a tool. In this case, I think an LLM can 1:13 maybe write a decent outline on many topics that I would want it to help me think through. So, 1:20 say probably okay on the first step, and then I know how to use an LLM to generate search terms 1:26 to search the web. So, I would say the second step is also doable, and then based on web search, 1:31 I think an LLM could input the web search results and write an essay. And so, this would be a 1:35 reasonable first attempt at an agentic workflow for writing an essay that goes deeper than just 1:41 direct generation. But if I were to then implement this agentic workflow and look at the results, 1:47 maybe you find that the results still aren\u0026#39;t good enough. It\u0026#39;s still not yet as deeply thoughtful. 1:51 Maybe the essays feel a little bit disjointed. This has actually happened to me. I once built 1:56 a research agent using this workflow, but when I read the output, it felt a bit disjointed. 2:00 You know, the start of the article didn\u0026#39;t feel completely consistent with the middle, 2:04 didn\u0026#39;t feel completely consistent with the end. In this case, what you might do is then reflect 2:09 on how you would change the workflow if you as a human found that the essay is a little bit 2:13 disjointed. One thing you could do is take the third step and further decompose, write the essay 2:21 into additional steps. So, instead of writing the essay on one go, you might instead have it 2:27 write the first draft, and then consider what parts need revision, and then revise the draft. 2:32 And this would be how I as a human might go about it, to not just write the final essay at my first 2:39 attempt, but write the first draft and then read over it, which is another step that the LLM is 2:44 pretty decent at. And then based on my own critique of my own essay, I\u0026#39;ll revise the draft. 2:49 So to recap, I started off with direct generation, just one step, decided it wasn\u0026#39;t good enough, 2:55 and so broke that down into three steps, and then maybe decided that still isn\u0026#39;t good enough, 3:00 and took one of the steps and further broken it down or decomposed it into three more steps, 3:05 resulting in this more complex, richer process for generating an essay. And depending on how 3:12 satisfied you are with the results of this process, you may choose to even modify this essay 3:18 generation process further. Let\u0026#39;s look at the second example of how to decompose complex tasks 3:23 into smaller steps. Take the example of responding to basic customer order inquiries. The first step 3:30 that a human customer specialization might carry out might be to first extract the key information, 3:36 such as who is this email from, what did they order, and what is the order number. And these 3:41 are things that an LLM could do. So I could just say, let\u0026#39;s have an LLM do that. The second step 3:46 would be to then find the relevant customer records. So to write and generate the relevant 3:51 database queries to pull up the order of what the customer had ordered and when I shipped and so on. 3:56 I think an LLM with the ability to call a function to query the orders database should be able to do 4:04 that. And lastly, having pulled up the customer record or the customer order record, I might then 4:09 write and send a response back to the customer. And I think with the information we pulled up, 4:13 this third step is also doable with an LLM if I give the option to call an API to send an email. 4:19 So this would be another example of taking a task of responding to customer email and breaking it 4:25 down into three individual steps where I can look at each of these steps and say, yep, I think an LLM 4:30 or one LLM with the ability to call a function to query a database or send an email should be able 4:35 to do that. Just one last example for the invoice processing. After a PDF invoice has been converted 4:42 to text, the first step is to pull out the required information, the name of the biller, the address, 4:47 the due date, the amount due, and so on. And now I should be able to do that. And then if I want to 4:53 check that the information was extracted and save it in a new database entry, then I think an LLM 4:58 should be able to help me call a function to update the database record. And so to implement this, 5:04 we implement an agentic workflow to carry out basically these two steps. When building agentic 5:09 workflows, I think of myself as having a number of building blocks. One important building block 5:15 would be large language models or maybe large multimodal models if I want to try to process 5:20 images or audio as well. And LLMs are good at generating text, deciding what to call, 5:25 maybe extracting information. For some highly specialized tasks, I might also use some other 5:31 AI models, such as an AI model for converting a PDF to text or for text-to-speech or for image 5:38 analysis. In addition to AI models, I also have access to a number of software tools, including 5:45 different APIs that I can call to do voice search, to get maybe real-time weather data, 5:51 to send emails, check calendar, and so on. And I might also have tools to retrieve information, 5:57 to pull up data from a database, or to invent RAG or retrieval augmented generation, 6:03 where I can look up a large text database and find the most relevant text. Or I might also 6:07 have tools to execute code. And this is a tool that lets an LLM write code and then run the code 6:13 on your computer to do a huge range of things. In case some of these tools seem a bit foreign to 6:18 you, don\u0026#39;t worry about it. We\u0026#39;ll go through the most important tools in much greater detail in 6:23 a later module. But I think of a lot of my work when I\u0026#39;m building an agent workflow as looking at 6:29 the work that the person or business is doing and then trying to figure out with these building 6:33 blocks, how can I sequence these building blocks together in order to carry out the tasks that I 6:39 want my system to carry out. And this is why having a good understanding of what building 6:44 blocks are available, which I hope you have a better sense of by the end of this course as well, 6:49 will allow you to better envision what agentic workflows you can build by combining these 6:54 building blocks together. So to summarize, one of the key skills in building agentic workflows is 6:59 to look at a bunch of stuff that maybe someone does and to identify the discrete steps that 7:06 it could be implemented with. And when I\u0026#39;m looking at the individual discrete steps, 7:10 one question I\u0026#39;m always asking myself is, can this step be implemented with either an 7:16 LLM or with one of the tools such as an API or a function call that I have access to? And in case 7:22 the answer is no, I\u0026#39;ll then often ask myself, how would I as a human do this step? And is it 7:28 possible to decompose this further or break this down into even smaller steps that then maybe is 7:34 more amenable to implementation with an LLM or with one of the software tools that I have? 7:40 So I hope this gives you a rough sense of how to think about task decomposition. In case you feel 7:45 like you don\u0026#39;t fully have it yet, don\u0026#39;t worry about it. We\u0026#39;ll go through many more examples 7:50 in this course and you have a much better understanding of this by the end of this 7:54 course. But it turns out that as you build agentic workflows, you find that often you 7:59 build an initial task decomposition, initial agentic workflow, and then you want to keep 8:05 on iterating and improving on it quite a few times until it delivers the level of performance 8:10 that you want. And to drive this improvement process, which I found important for many 8:15 projects, one of the key skills is to know how to evaluate your agentic workflow. So in the next 8:21 video, we\u0026#39;ll talk about evaluations or evals and discrete key components, how you can build, 8:28 and then also keep on improving your workflows to get the performance 8:31 that you want. Let\u0026#39;s talk about evals in the next video. 1.6 Evaluations agentic (evals) 0:04 I\u0026#39;ve worked with many different teams on building agentic workflows, and I\u0026#39;ve found that one 0:05 of the biggest predictors for whether someone is able to do it really well versus be less 0:10 efficient at it is whether or not they\u0026#39;re able to drive a really disciplined evaluation 0:16 process. 0:17 So, your ability to drive evals for your agentic workflow makes a huge difference in your ability 0:23 to build them effectively. 0:26 In this video, we\u0026#39;ll take a quick overview of how to build evals, and this is a subject 0:30 that we\u0026#39;ll actually go into much deeper in a later module in this course. 0:35 So, let\u0026#39;s take a look. 0:37 After building an agentic workflow like this one for responding to customer order inquiries, 0:43 it turns out that it\u0026#39;s very difficult to know in advance what are the things that could 0:47 go wrong. 0:48 And so, rather than trying to build evaluations in advance, what I recommend is you just look 0:53 for the outputs and manually look for things that you wish it was doing better. 1:00 For example, maybe you read a lot of outputs and find that it is unexpectedly mentioning 1:05 your competitors more than it should. 1:08 Many businesses don\u0026#39;t want their agents to mention competitors because it just creates 1:12 an awkward situation. 1:14 And if you read some of these outputs, maybe you find that it sometimes says, I\u0026#39;m glad 1:17 you shopped with us. 1:18 We\u0026#39;re much better than our competitor, ComproCo. 1:21 Or maybe sometimes they say, sure, it should be fun. 1:23 Unlike RivalCo, we make returns easy. 1:25 And you may look at this and go, gee, I really don\u0026#39;t want this to mention competitors. 1:30 This is an example of a problem that is really hard to anticipate in advance of building 1:36 this agentic workflow. 1:37 So, the best practice is really to build it first and then examine it to figure out where 1:42 it is not yet satisfactory, and then to find ways to evaluate as well as improve the system 1:47 to eliminate the ways that it is still not yet satisfactory. 1:51 Assuming your business considers it an error or a mistake to mention competitors 1:57 in this way, then as you work on eliminating these competitor mentions, one way to track 2:02 progress is to add an evaluation or an eval to track how often this error occurs. 2:08 So, if you have a named list of competitors like ComproCo, RivalCo, the other co, then 2:14 you can actually write code to just search in your own output for how often it mentions 2:20 these competitors by name and count up as a number, as a fraction of the overall responses, 2:26 how frequently it mistakenly mentions competitors. 2:29 One nice thing about the problem of competitor mentions is it\u0026#39;s an objective metric, meaning 2:35 either the competitor was mentioned or not. 2:38 And for objective criteria, you can write code to check for how often this specific 2:44 error occurs. 2:46 But because LLMs output free text, there are also going to be criteria by which you want 2:51 to evaluate this output that may be more subjective and where it\u0026#39;s harder to just write code 2:57 to output a black and white score. 2:59 In this case, using a LLM as a judge is a common technique to evaluate the output. 3:05 So, for example, if you\u0026#39;re building a research agent to do research on different topics, 3:10 then you can use another LLM and prompt it to maybe, say, assign the following essay 3:16 a quality score between 1 and 5, where 1 is the worst and 5 is the best essay. 3:21 Here, I\u0026#39;m using a Python expression to mean copy-paste the generated essay into this. 3:27 So, you can prompt the LLM to read the essay and assign it a quality score. 3:32 Then I\u0026#39;m going to ask the research agent to write a number of different research reports, 3:37 for example, on recent developments in black hole science or using robots to harvest fruit. 3:43 And then in this example, maybe the judge LLM assigns the essay on black holes a score 3:48 of 3, the essay on robot harvesting a score of 4, and as you work on improving your research 3:54 agent, hopefully you see these scores go up over time. 3:58 It turns out, by the way, that LLMs are actually not that good at these 1 to 5 scale ratings. 4:03 You can give it a shot, but I personally tend not to use this technique that much myself. 4:08 But in a later module, you\u0026#39;ll learn some better techniques to have an LLM output more accurate 4:13 scores than asking it to output scores on a 1 to 5 scale, although some people will 4:17 do this, maybe an initial cut as an LLM as judge type of eval. 4:22 Just to give a preview of some of the Agentic AI evals you\u0026#39;ll learn about later in this course, 4:28 you\u0026#39;ve already heard me talk about how you can write codes to evaluate objective criteria, 4:33 such as did it mention a competitor or not, or use an LLM as a judge for more subjective 4:37 criteria such as what\u0026#39;s the quality of this essay. 4:39 But later, you learn about two major types of evals. 4:42 One is end-to-end, where you measure the output quality of the entire agent, as well as component 4:48 level evals, where you might measure the quality of the output of a single step in the agentic 4:53 workflow. 4:54 It turns out that these are useful for driving different parts of your development process. 4:58 One thing I do a lot as well is just examine the intermediate outputs, or sometimes we 5:03 call these the traces of the LLM, in order to understand where it is falling short of 5:09 my expectations. 5:10 And we call this error analysis, where we just read through the intermediate outputs 5:14 of every single step to try to spot opportunities for improvement. 5:17 And it turns out being able to do evals and error analysis is a really key skill. 5:22 So we have much more to say about this in the fourth module in this course. 5:27 We\u0026#39;re nearly to the end of this first module. 5:29 Before moving on, I just want to share with you what I think are the most important design 5:33 patterns for building agentic workflows. 5:35 Let\u0026#39;s go take a look at that in the next video. 1.7 Agentic design patterns 0:04 We build agentic workflows by taking building blocks and putting them together to sequence 0:04 out these complex workflows. In this video, I\u0026#39;d like to share with you a few of the key design 0:10 patterns, which are patterns for how you can think about combining these building blocks into more 0:16 complex workflows. Let\u0026#39;s take a look. I think four key design patterns for building agentic 0:21 workflows are reflection, two-use, planning, and multi-agent collaboration. Let me briefly go over 0:27 what they mean, and then we\u0026#39;ll actually go through most of these in-depth latent discourse as well. 0:33 The first of the major design patterns is reflection. So I might go to an LLM agent and 0:39 ask it to write code, and it turns out that an LLM might then generate code like this. It defines 0:45 here a Python function to do a certain task. I could then construct a prompt that looks like this. 0:50 I can say, here\u0026#39;s code intended for a certain task, and then copy-paste whatever the LLM had 0:55 just output back into this prompt. And then I ask it to check the code carefully for correctness, 0:59 style, and efficiency, and give constructive criticism. And it turns out that the same LLM model 1:03 prompted this way may be able to point out some problems with the code. And if I then take this 1:09 critique and feed it back to the model to say, looks like this is a bug, could you change the 1:15 code to fix it? Then it may actually come with a better version of the code. To give a preview of 1:21 tool use, if you\u0026#39;re able to run the code and see where the code fails, then feeding that back to 1:28 the LLM can also cause it to be able to iterate and generate a much better, say, v3 version 3 of the 1:35 code. So reflection is a common design pattern where you can ask the LLM to examine its own outputs 1:41 or maybe bring in some external sources of information, such as run the code and see if it 1:46 generates any error messages, and use that as feedback to iterate again and come up with a better 1:52 version of its output. And this design pattern isn\u0026#39;t magic. It does not result in everything 1:58 working 100% of the time. But sometimes it can be a nice bump in the performance of your system. 2:03 Now, I\u0026#39;ve drawn this as if it was a single LLM that I\u0026#39;m prompting, but to foreshadow multi-agent 2:09 workflows, you can also imagine instead of having the same model critique itself, you can imagine 2:15 having a critique agent. And all that is, is an LLM that\u0026#39;s been prompted with instructions like, 2:20 your role is to critique code, here\u0026#39;s code intended for a task, check the code carefully, and so on. 2:25 And the second critique agent, maybe point out errors or run unit tests. And by having two 2:31 simulated agents where each agent is just an LLM prompted to take on a certain persona, 2:36 you can have them go back and forth to iterate to get a better output. In addition to reflection 2:42 pattern, the second important design pattern is tool use. Where today, LLMs can be given tools, 2:49 meaning functions that they can call in order to get work done. For example, if you ask an LLM, 2:54 what\u0026#39;s the best coffee maker according to reviewers, and you give it a web search tool, 2:58 then it can actually search the internet to find much better answers. Or a code execution tool. 3:03 If you ask a math question like, if I invest $100 in compound interest, what do I have at the end? 3:08 It can then write code and execute code to compute an answer. Today, different developers 3:13 have given LLMs many different tools for everything from math or data analysis to gather information 3:19 by fetching things from the web or for various databases, to interface with productivity apps 3:24 like email, calendar, and so on, as well as to process images and much more. And the ability 3:30 of an LLM to decide what tools to use, meaning what functions to call, that lets the model get a lot 3:37 more done. The third of the four design patterns is planning. This is an example from a paper 3:43 called Hugging GPT, in which if you ask a system to please generate an image where a girl is reading 3:51 a book and a pose is the same as a boy in the image, then please describe the new image in 3:54 your voice. Then a model can automatically decide that to carry out this task, it first needs to 4:00 find a pose determination model to figure out the pose of the boy. Then to pose the image, 4:05 to generate a picture of a girl and image the text, and then finally text the speech. And so 4:10 in planning, an LLM decides what is the sequence of actions it needs to take. In this case, it is 4:17 a sequence of API calls so that it can then carry out the right sequence of steps in the right order 4:23 in order to carry out the task. So rather than the developer hard coding the sequence of steps 4:29 in advance, this actually lets the LLM decide what are the steps to take. Agents that plan today are 4:36 harder to control and somewhat more experimental, but sometimes they can give really delightful 4:40 results. And then finally, multi-agent workflows. Just as a human manager might hire a number of 4:47 others to work together on a complex project, in some cases it might make sense for you to hire 4:52 a set of multiple agents, maybe each of which specializes in a different role, and have them 4:58 work together to accomplish a complex task. The picture you see here on the left is taken from a 5:03 project called ChatDev, which is a software framework created by Chen Qian and collaborators. 5:09 In ChatDev, multiple agents with different roles, like chief executive officer, programmer, tester, 5:15 designer, and so on, collaborate together as if they were a virtual software company and can 5:22 collaboratively complete a range of software development tasks. Let\u0026#39;s consider another example. 5:28 If you want to write a marketing brochure, maybe you think of hiring a team of three people, such 5:33 as a researcher to do online research, a marketer to write the marketing text, and then finally an 5:39 editor to edit and polish the text. And so in a similar way, you might consider building a multi-agent 5:47 workflow in which you have a simulated research agent, a simulated marketer agent, and a simulated 5:53 editor agent that then come together to carry out this task for you. Multi-agent workflows are 6:00 more difficult to control since you don\u0026#39;t always know ahead of time what the agents will do, but 6:05 research has shown that they can result in better outcomes for many complex tasks, including things 6:11 like writing biographies or deciding on chess moves to make in the game. You learn more about 6:16 multi-agent workflows later in this course as well. And so with that, I hope you have a sense of what 6:21 agentic workflows can do, as well as of what are the key challenges of finding building blocks and 6:27 putting them together, maybe via these design patterns, in order to implement an agentic workflow. 6:33 And of course, also developing eval so you can see how well your system is doing and keep on improving 6:39 on it. In the next module, I\u0026#39;d like to share with you a deep dive into the first of these design 6:46 patterns, that is reflection, and you find that it\u0026#39;s a maybe surprisingly simple to implement 6:53 technique that can give the performance of your system sometimes a very nice bump. So let\u0026#39;s go on 6:59 to the next module to learn about the reflection design pattern. ","description":"```","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ Andrew Ng: Agentic AI Module 1: Introduction to Agentic Workflows 1.0 Introduction 0:02 Welcome to this course on Agentic AI. When I coined the term agentic to describe what I saw 0:05 as an important and rapidly growing trend in how people were building on-base applications, 0:10 what I did not realize was that a bunch of marketers would get hold of this term 0:14 and use it as a sticker and put this on almost everything in sight. And that has caused hype 0:20 on Agentic AI to skyrocket. The good news though is that ignoring the hype, the number of truly 0:27 valuable and useful applications built using Agentic AI has also grown very rapidly, even if 0:33 not quite as rapidly as the hype. And in this course, what I\u0026#39;d like to do is show you best 0:38 practices for building Agentic AI applications. And this will open up a lot of new opportunities 0:44 to you in terms of what you can now build. Today, agentic workflows are being used to build 0:49 applications like customer support agents, or to do deep research to help write deeply insightful 0:55 research reports, or to process tricky legal documents, or to look at patient input and render 1:02 or to suggest possible medical diagnoses. On many of my teams, a lot of the projects we built just 1:08 would be impossible without agentic workflows. And so knowing how to build applications with them 1:14 is one of the most important and valuable skills in AI today. It turns out that one of the biggest 1:20 differences I\u0026#39;ve seen between people that really know how to build agentic workflows compared to 1:24 people that are less effective at it is the ability to drive a disciplined development process, 1:30 specifically one focused on evals and error analysis. And in this course, I\u0026#39;ll tell you what 1:35 that means and show you what allows you to be really good at building these agentic workflows. 1:41 Being able to do this is one of the most important skills in …"},{"title":"「Scripts」Module 2: Reflection Design Pattern","url":"/courses/andrew-ng-agentic-ai/lecture/lec-02/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 2: Reflection Design Pattern 2.1 Reflection to improve outputs of a task 0:03 The reflection design pattern is something I\u0026#39;ve used in many applications, and it\u0026#39;s surprisingly easy to implement. 0:07 Let\u0026#39;s take a look. 0:08 Just as humans will sometimes reflect their own output and find a way to improve it, so can LLMs. 0:14 For example, I might write an email like this, and if I\u0026#39;m typing quickly, I might end up with a first draft that\u0026#39;s not great. 0:21 And if I read over it, I might say, 0:24 huh, next month isn\u0026#39;t that clear for what dates Tommy might be free for dinner, and there\u0026#39;s such a typo that I had, and also forgot to sign my name. 0:33 And this would let me revise the draft to be more specific in saying, hey, Tommy, are you free for dinner on the 5th to the 7th? 0:40 A similar process lets LLMs also improve their outputs. 0:44 You can prompt an LLM to write the first draft in email, and given email version 1, email v1, you can pass it to maybe the same model, 0:53 the same large language model, but with a different prompt, and tell it to reflect and write an improved second draft to then get you the final output, email v2. 1:02 Here, I have just hard-coded this workflow of prompting the LLMa once, and then prompting them again to reflect and improve, and that gives email v2. 1:12 It turns out that a similar process can be used to improve other types of outputs. 1:18 For example, if you are having an LLM write code, you might prompt an LLM to write code to do a certain task, and it may give you v1 of the code, 1:28 and then pass it to the same LLM or maybe a different LLM to ask it to check for bugs and write an improved second draft of the code. 1:36 Different LLMs have different strengths, and so sometimes I would choose different models for writing the first draft and for reflecting and trying to improve it. 1:46 For example, it turns out reasoning models, sometimes also called thinking models, are pretty good at finding bugs, 1:53 and so I\u0026#39;ll sometimes write the first draft of the code by direct generation, but then use a reasoning model to check for bugs. 2:00 Now, rather than just having an LLM reflect on the code, it turns out that if you can get external feedback, meaning new information from outside the LLM, 2:11 reflection becomes much more powerful. 2:14 In the case of code, one thing you can do is just execute the code to see what the code does, 2:20 and by examining the output, including any error messages of the code, this is incredibly useful information for the LLM to reflect and to find a way to improve his code. 2:30 So in this example, the LLM generated the first draft of the code, but when I run it, it generates a syntax error. 2:36 When you pass this code output and error logs back into the LLM and ask it to reflect on the feedback and write a new draft, 2:44 this gives it a lot of very useful information to come up with a much better version 2 of the code. 2:50 So the reflection design pattern isn\u0026#39;t magic. 2:53 It does not make an LLM always get everything right 100% of the time, but it can often give it maybe a modest bump in performance. 3:01 But one design consideration to keep in mind is reflection is much more powerful when there is new additional external information that you can ingest into the reflection process. 3:13 So in this example, if you can run the code and have that code output or error messages as an additional input to the reflection step, 3:20 that really lets the LLM reflect much more deeply and figure out what may be going wrong, if anything, 3:26 and results in a much better second version of the code than if there wasn\u0026#39;t this external information that you can ingest. 3:32 So one thing to keep in mind, whenever reflection has an opportunity to get additional information, that makes it much more powerful. 3:41 Now with that, let\u0026#39;s go on to the next video where I want to share with you a more systematic comparison of using reflection versus direct generation or something we sometimes call zero shot prompting. 3:54 Let\u0026#39;s go on to the next video. 2.2 why not just direct generation? 0:04 Let\u0026#39;s take a look at why we might prefer to use a reflection workflow rather than just 0:04 prompting an LLM once and having it directly generate the answer and be done with it. 0:09 With direct generation, you just prompt the LLM with an instruction and let it generate an answer. 0:14 So you can ask an LLM to write an essay about black holes and have it just generate the text, 0:19 or have it write the Python functions to calculate 0:22 compound interest and have it just write the code directly. 0:25 The prompt examples you see here are also called zero-shot prompting. 0:31 Let me explain what zero-shot means. In contrast to zero-shot prompting, 0:35 a related approach is to include one or more examples of what you want the output to look like 0:41 in your prompt. And this is known as one-shot prompting, if in the prompt you include 0:45 one example of a desired input-output pair, or two-shot or few-shot prompting, 0:50 depending on how many such examples you include in your prompt. 0:53 And so zero-shot prompting refers to if you include zero examples and if you don\u0026#39;t include 0:57 any examples of the desired outputs that you want. But don\u0026#39;t worry if you aren\u0026#39;t yet familiar 1:02 with these terms. The important thing is that in the examples you see here, you\u0026#39;re just prompting 1:07 the LLM to directly generate an answer in one go, which I\u0026#39;m also calling zero-shot prompting 1:13 because we include zero examples. It turns out that multiple studies have 1:17 shown that reflection improves on the performance of direct generation on a variety of tasks. 1:23 This diagram is adapted from the research paper by Madaan and others, and this shows a range of 1:30 different tasks being implemented with different models and with and without reflection. 1:35 The way to read this diagram is to look at these pairs of adjacent light followed by 1:41 dark-colored bars, where the light bar shows zero-shot prompting and the dark bar 1:46 shows the same model but with reflection. And the colors blue, green, and red show 1:52 experiments run with different models, such as GPT-3.5 and GPT-4. And what you see is, 1:58 for many different applications, the dark bar that is with reflection is quite a bit higher 2:04 than the light bar. But of course, your knowledge may vary depending on your specific application. 2:10 Here are some more examples where reflection might be helpful. 2:14 If you are generating structured data, such as an HTML table, sometimes it may have incorrect 2:20 formatting of the output. So a reflection prompt to validate the HTML code could be helpful. 2:26 If it\u0026#39;s basic HTML, this may not help that much, since LLMs are pretty good at basic HTML. But 2:31 especially if you have more complex structured outputs, like maybe a JSON data structure with 2:36 a lot of nesting, then reflection may be more likely to spot bugs. Or if you ask an LLM to 2:41 generate a sequence of steps that comprise a set of instructions to do something, such as how to 2:45 brew a perfect cup of tea, sometimes the LLM may miss steps and a reflection prompt to ask to check 2:51 instructions for coherence and completeness might help spot errors. Or something that I\u0026#39;ve actually 2:56 worked on was using an LLM to generate domain names, but sometimes the names it generates has 3:01 an unintended meaning or may be really hard to pronounce. And so I\u0026#39;ve used reflection prompts 3:06 to double check if the domain name has any problematic connotations or problematic meanings, 3:11 or if the name is hard to pronounce. And we actually used this at one of my team\u0026#39;s AI fund 3:16 to help brainstorm domain names for startups that we\u0026#39;re working on. I want to show you a couple of 3:21 examples of reflection prompts. For brainstorming domain names, you might ask it to review the 3:26 domain names you suggested, and then ask it to check if each name is easy to pronounce. Check 3:30 if each name might mean something negative in English or other languages, and then output a 3:35 short list of only the names that satisfy these criteria. Or to improve an email, you can write a 3:41 reflection prompt to tell it to review the email first draft, check the tone, verify all fact states 3:46 and promises are accurate. This would make sense in the context of the LLM having been fed a number 3:51 of facts and dates and so on in order to write the email drafts. All this would be provided as part 3:56 of the LLM context. And then based on any problems it may find, write the next draft of the email. 4:02 So some tips for writing reflection prompts. It helps to clearly indicate that you want it to 4:07 review or to reflect on the first draft of the output. And if you can specify a clear set of 4:13 criteria, such as whether the domain name is easy to pronounce and whether it may have negative 4:17 connotations or for email, check the tone and verify the facts. Then that guides the LLM better 4:22 in reflecting and critiquing on the criteria that you care the most about. I found that one of the 4:28 ways I\u0026#39;ve learned to write better prompts is to read a lot of other prompts that other people 4:34 have written. Sometimes I\u0026#39;ll actually download open source software and go and find the prompts 4:40 in a piece of software that I think is especially well done to just go and read the prompts that 4:45 the authors have written. So that I hope you have a sense of how to write a basic reflection prompt 4:51 and that maybe you even try it out in your own work to see if it helps give you better performance. 4:57 In the next video, I\u0026#39;d like to share with you a fun example where we\u0026#39;ll start to look at 5:02 multi-modal inputs and outputs. We\u0026#39;ll have an algorithm reflect 5:06 on an image being generated or a chart being generated. Let\u0026#39;s go take a look. 2.3 chart generation workflow 0:02 In the coding lab that you see in this module, you play with a chart generation workflow where 0:05 you use an agent to generate nice-looking diagrams. It turns out reflection can significantly improve 0:11 the quality of this output. Let\u0026#39;s take a look. In this example, I have data from a coffee machine 0:18 showing when different drinks, such as a latte coffee or hot chocolate or a cappuccino coffee 0:24 and so on, were sold and for what price. And we want to have an agent create a plot 0:29 comparing Q1 or first quarter coffee sales in 2024 and 2025. So one way to do it would be to 0:35 write a prompt that asks an LLM to create a plot comparing Q1 coffee sales in 2024 and 2025 0:41 using the data stored in a spreadsheet as a CSV file or comma-separated values as a spreadsheet 0:48 file. And an LLM might write Python code like this to generate the plot. And with this v1 of the code, 0:55 if you execute it, it may generate a plot like this. When I ran the code to the LLM output, 1:01 it actually generated this the first time. And this is a stacked bar plot, which is not a very 1:07 easy way to visualize things and it just doesn\u0026#39;t look a very good plot. But what you can do is then 1:12 give it v1 of the code as well as the plot that this code generated and feed it into a 1:19 multimodal model that is an LLM that can also accept image inputs and ask it to examine the 1:26 image that was generated by this code and then to critique the image, find a way to come up with 1:31 better visualization, and update the code to just generate a clearer, better plot. Multimodal LLMs 1:37 can use visual reasoning, so it can actually look visually at this figure to find ways to improve it. 1:44 And when I did this, it actually generated a bar graph that isn\u0026#39;t this stacked bar graph, 1:49 but a more regular bar graph that separates out the 2034 and 2035 coffee sales in what I thought 1:54 was a more pleasing and clearer way. When you get to the coding lab, please feel free to mess around 2:00 with the problems and see if you can get maybe even better looking graphs in these. Because 2:05 different LLMs have different strengths and weaknesses, sometimes I\u0026#39;ll use different LLMs 2:09 for the initial generation and for the reflection. So, for example, you may use one LLM to generate 2:14 the initial code, maybe open it as GPT-4o or GPT-5 or some model like that, and just prompt it like a 2:21 prompt like this to write Python code to generate visualization and so on. And then the reflection 2:26 prompts might be something like this, where you tell the LLM to play the role of an expert data 2:31 analyst that provides constructive feedback and then give it the version 1 of the code, 2:36 the part that was generated, maybe also the computational history from how the code was 2:40 generated, and ask it to critique it for specific criteria. Remember, when you give it specific 2:45 criteria like readability, clarity, and completeness, it helps the LLM better figure out what to do. 2:50 And then ask it to write new code to implement your improvements. One thing you may find is that 2:56 sometimes using a reasoning model for reflection may work better than a non-reasoning model. So 3:02 when you\u0026#39;re trying out different models for the initial generation and the reflection, these are 3:07 different configurations that you might toggle or try different combinations of. 3:12 So when you get to the coding lab, I hope you have fun visualizing coffee sales. Now, when you\u0026#39;re 3:17 building an application, one thing you may be wondering is, does reflection actually improve 3:22 performance on your specific application? From various studies, reflection improves performance 3:28 by a little bit on some, by a lot on some others, and maybe barely any at all on some other 3:34 applications. And so it\u0026#39;ll be useful to understand its impact on your application and also give you 3:39 guidance on how to tune either the initial generation or the reflection prompt to try to get 3:45 better performance. In the next video, let\u0026#39;s take a look at evals or evaluations 3:50 for reflection workflow. Let\u0026#39;s go on to the next video. 2.4 Evaluating the impact of reflection 0:04 Reflection often improves the performance of the system, but before I commit to keeping it, 0:04 I would usually want to double check how much it actually improves the performance, because 0:09 it does slow down the system a little bit by needing to take an extra step. 0:13 Let\u0026#39;s take a look at evals for reflection workflows. 0:16 Let\u0026#39;s look at an example of using reflection to improve the database query that an LLM writes 0:23 to fetch data to answer questions. Let\u0026#39;s say you run a retail store, 0:27 and you may get questions like, which color product has the highest total sales? 0:32 To answer a question like this, you might have an LLM generate a database query. 0:36 If you\u0026#39;ve heard of database languages like SQL, SQL, it may generate a query in that type of 0:42 language. But if you\u0026#39;re not familiar with SQL, don\u0026#39;t worry about it. 0:45 But after writing a database query, instead of using that directly to fetch information from 0:51 the database, you may have an LLM, the same or different LLM, reflect on the version one database 0:57 query and update it to maybe an improved one, and then execute that database query against the 1:02 database to fetch information to finally have an LLM answer the question. 1:07 So the question is, does using a second LLM to reflect and improve 1:12 on the database or SQL query actually improve the final output? 1:16 In order to evaluate this, I might collect a set of questions or set of prompts together with 1:23 ground truth answers. So maybe one would be, how many items are sold in May 2025? 1:28 What\u0026#39;s the most expensive item in the inventory? How many styles are carried in my store? 1:33 And I write down for maybe 10, 15 prompts, the ground truth answer. 1:40 Then you can run this workflow without reflection. So without reflection would mean to take the SQL 1:46 query generated by the first LLM and to just see what answer it gives. And with reflection would 1:51 mean to take the database query generated after the second LLM has reflected on it to see what 1:57 answer that fetches from the database. And then we can measure the percentage of correct answers 2:03 from no reflection and with reflection. In this example, no reflection gets the answers 2:08 right 87% of the time, with reflection gets it right 95% of the time. And this would suggest that 2:14 reflection is meaningfully improving the quality of the database queries I\u0026#39;m able to get to pull 2:21 out the correct answer. One thing that developers often end up doing as well is rewrite the reflection 2:26 prompt. So for example, do you want to add to reflection prompt an instruction to make the 2:32 database query run faster or make it clearer? Or you may just have different ideas for how to 2:38 rewrite either the initial generation prompt or the reflection prompt. Once you put in place 2:44 evals like this, you can quickly try out different ideas for these prompts and measure the percentage 2:50 correct your system has as you change the prompts in order to get a sense of which prompts work 2:55 best for your application. So if you\u0026#39;re trying out a lot of prompts, building evals is important. 3:02 It really helps you have a systematic way to choose between the different prompts you might 3:07 be considering. But this example is one of when you can use objective evals because there is a 3:14 right answer. The number of items sold was 1,301 and the answer is either right or wrong. How about 3:21 applications where you need more subjective rather than objective evaluations? In the plotting 3:27 example that we saw in the last video, without reflection we had the stack bar graph, with reflection 3:32 we had this graph. But how do we know which plot is actually better? I know I like the latter one 3:38 better, but with different graphs varying on different dimensions, how do we figure out which 3:43 one is better? And measuring which of these plots is better is more of a subjective criteria rather 3:51 than a purely black and white objective criteria. So for these more subjective criteria, one thing 3:58 you might do is use an LLM as a judge. And maybe a basic approach to do this might be to feed both 4:04 plots into an LLM, a multi-modal LLM that can accept two images as input, and just ask it which image 4:10 is better. It turns out this doesn\u0026#39;t work that well. I\u0026#39;ll share an even better idea in a second. But one 4:16 thing you could do might be to also give it some criteria by which to evaluate the two plots, such 4:21 as clarity, how nice looking they are, and so on. But it turns out that there\u0026#39;s some known issues of 4:26 using LLMs to compare two inputs to tell you which one is better. First, it turns out the answers are 4:32 often not very good. It could be sensitive to the exact wording of the prompt of the LLM as a judge, 4:37 and sometimes the rank ordering doesn\u0026#39;t correspond that well to human expert judgment. And one 4:43 manifestation of this is many LLMs will have a position bias. Many LLMs, it turns out, will often 4:48 pick the first option more often than the second option. And in fact, I\u0026#39;ve worked a lot of LLMs 4:54 where given two choices, whichever choice I present first, it will say the first choice is better. 5:01 And maybe some LLMs prefer the second option, but I think most LLMs prefer the first option. 5:06 Instead of asking an LLMs to compare a pair of inputs, grading with a rubric can give more 5:11 consistent results. So, for example, you might prompt an LLM to tell it, given a single image, 5:18 assess the attached image against the quality rubric, and the rubric or grading criteria may 5:23 have clear criteria like does the plot have a clear title, are the access labels present, 5:27 is it an appropriate chart type, and so on, with a handful of criteria like this. And it turns out 5:32 that instead of asking the LLM to grade something on a scale of 1 to 5, which it tends not to be 5:38 well calibrated on, if you instead give it, say, 5 binary criteria, 5-0-1 criteria, and have it give 5:45 5 binary scores, and you add up those scores to get the number from 1 to 5 or 1 to 10 if you have 5:51 10 binary criteria, that tends to give more consistent results. And so if we\u0026#39;re to gather a 5:58 handful, say 10-15 user queries for different visualizations that the user may want to have 6:04 of the coffee machine sales, then you can have it generate images without reflection or generate 6:11 images with reflection, and use a rubric like this to score each of the images to then check 6:17 the degree to which or whether or not the images generated with reflection are really better than 6:23 the ones without reflection. And then once you\u0026#39;ve built up a set of evals like this, if ever you 6:29 want to change the initial generation prompt or you want to change the reflection prompt, you can 6:33 also rerun this eval to see if, say, updating one of your prompts allows the system to generate images 6:40 that scores more points according to this rubric. And so this too gives you a way to keep on tuning 6:47 your prompts to get better and better performance. What you may find when building evaluations for 6:53 reflection or for other agentic workflows is that when there is an objective criteria, code-based 6:58 evaluation is usually easier to manage. And in the example that we saw with the database query, we 7:04 built up a database of ground truth examples and ground truth outputs and just wrote code to see 7:10 how often the system generated the right answer in a really objective evaluation metric. In contrast, 7:17 for small subjective tasks, you might use an element as a judge but it usually takes a little 7:22 bit more tuning, such as having to think through what rubric you may want to use to get the LLM 7:27 as a judge to be well calibrated or to output reliable evals. So I hope that gives you a sense 7:33 of how to build evals to evaluate reflections or more generally even to evaluate different 7:38 agentic workflows. Knowing how to do evals well is really important for how you build agentic 7:45 workflows effectively and you hear me say more about this in later videos as well. But now that 7:51 you have a sense of how to use reflection, what I hope to do in the next video is a deep dive into 7:57 one aspect of it, which is when you can get additional information from outside and this 8:03 turns out to make reflection work much better. So in the final video of this module, let\u0026#39;s take a 8:08 look at that technique for making your reflection workflows work much better. I\u0026#39;ll see you in the 8:14 next video. 2.5 Using external feedback 0:03 Reflection with external feedback, if you can get it, is much more powerful than reflection 0:05 using the LLM as the only source of feedback. Let\u0026#39;s take a look. 0:09 When I\u0026#39;m building an application, and if I\u0026#39;m just prompt engineering for direct generation 0:14 of a zero-shot prompting, this is what performance might look like over time, 0:18 where initially, as I tune the prompt, the performance improves for a while, 0:21 but then after a while, it sort of plateaus or flattens out, and despite further engineering 0:27 the prompt, it\u0026#39;s just hard to get that much better level of performance. 0:31 So instead of wasting all this time on tuning the prompt, sometimes it\u0026#39;d be better if only 0:35 earlier on in the process, I had started adding reflection, and sometimes that gives a bump in 0:41 performance. Sometimes it\u0026#39;s smaller, sometimes a bigger bump, but that adds complexity. 0:46 But if I had started adding in reflection, maybe at this point in the process, 0:49 and then started tuning the reflection prompt, then maybe I end up with a performance that 0:54 looks like this. But it turns out that if I\u0026#39;m able to get external feedback, 0:58 so that the only source of new information isn\u0026#39;t just an LLM reflecting on the same 1:03 information as it had before, but some new external information, then sometimes, 1:08 as I continue to tune the prompts and tune the external feedback, you end up with 1:12 an even much higher level of performance. So something to consider if you are working 1:17 on prompt engineering, and you feel that your efforts are seeing diminishing returns, 1:23 that you\u0026#39;re tuning a lot of prompts, but it\u0026#39;s just not getting that much better, 1:26 then maybe consider if there\u0026#39;s reflection, or even better, if there\u0026#39;s some external feedback 1:31 you can interject to bump the performance curve off this fattening out red line to maybe 1:36 some higher trajectory of performance improvement. Just as a reminder, we saw earlier, 1:42 one source of feedback for if you\u0026#39;re writing code would be if you were to just execute the code 1:47 and see what output it generates, output or error messages, and feed that output back to the LLM 1:53 to let it have that new information to reflect, and then use that information to write a new 1:58 version of the code. Here are a few more examples of when software codes or tools can create new 2:05 information to help the reflection process. If you\u0026#39;re using LLM to write emails, and it 2:10 sometimes mentions competitors\u0026#39; names, then if you write codes or build a software tool 2:15 to just carry out pattern matching, maybe via regular expression pattern matching to search 2:20 for competitors\u0026#39; names in the output, then whenever you find a competitor\u0026#39;s name, you just feed that 2:25 back to the LLM as a criticism or as input. That\u0026#39;s very useful information to tell it to just rewrite 2:32 the text without mentioning those competitors. Or as another example, you might use web search 2:39 or look at other trusted sources in order to fact-check an essay. So if you\u0026#39;re a research 2:44 agent that says the Taj Mahal was built in 1648, technically the Taj Mahal was actually commissioned 2:51 in 1631, and it was finished in 1648. So maybe this isn\u0026#39;t exactly incorrect, but it doesn\u0026#39;t 2:58 capture the accurate history either. In order to more accurately represent when this beautiful 3:04 building was built, if you do a web search to cuddle the snippet explaining exactly the period 3:11 that the Taj Mahal was built and give that as additional input to your reflection agent, then 3:16 it may be able to use that to write a better version of the text on the history of the Taj Mahal. 3:21 One last example, if you\u0026#39;re using an LLM to write copy, maybe for a blog post or for a research 3:27 paper abstract, but what it writes is sometimes over the word limit. LLMs are still not very good 3:32 at following exact word limits. Then if you implement a word count tool, just write code to 3:37 count the exact number of words, and if it exceeds the word limit, then feed that word count back to 3:44 the LLM and ask it to try again. Then this helps it to more accurately hit the desired length of 3:51 the output you wanted to generate. So in each of these three examples, you can write a piece of 3:57 code to help find additional facts about the initial output to then give those facts, be it 4:04 that you found the competitor\u0026#39;s name or information web search or the exact word count, to feed into 4:09 the reflection LLM in order to help it do a better job thinking about how to improve the output. 4:17 Reflections are powerful too, and I hope you find it useful in a lot of your own work. In the next 4:23 module, we\u0026#39;ll build on this to talk about tool use, where in addition to the handful of tool examples 4:29 you saw, you learn how to systematically get your LLM to call different functions, and this will make 4:35 your agenting applications much more powerful. I hope you enjoyed learning about reflection. 4:41 I\u0026#39;m going to now reflect on what you just learned. I hope to see you in the next video. ","description":"```","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 2: Reflection Design Pattern 2.1 Reflection to improve outputs of a task 0:03 The reflection design pattern is something I\u0026#39;ve used in many applications, and it\u0026#39;s surprisingly easy to implement. 0:07 Let\u0026#39;s take a look. 0:08 Just as humans will sometimes reflect their own output and find a way to improve it, so can LLMs. 0:14 For example, I might write an email like this, and if I\u0026#39;m typing quickly, I might end up with a first draft that\u0026#39;s not great. 0:21 And if I read over it, I might say, 0:24 huh, next month isn\u0026#39;t that clear for what dates Tommy might be free for dinner, and there\u0026#39;s such a typo that I had, and also forgot to sign my name. 0:33 And this would let me revise the draft to be more specific in saying, hey, Tommy, are you free for dinner on the 5th to the 7th? 0:40 A similar process lets LLMs also improve their outputs. 0:44 You can prompt an LLM to write the first draft in email, and given email version 1, email v1, you can pass it to maybe the same model, 0:53 the same large language model, but with a different prompt, and tell it to reflect and write an improved second draft to then get you the final output, email v2. 1:02 Here, I have just hard-coded this workflow of prompting the LLMa once, and then prompting them again to reflect and improve, and that gives email v2. 1:12 It turns out that a similar process can be used to improve other types of outputs. 1:18 For example, if you are having an LLM write code, you might prompt an LLM to write code to do a certain task, and it may give you v1 of the code, 1:28 and then pass it to the same LLM or maybe a different LLM to ask it to check for bugs and write an improved second draft of the code. 1:36 Different LLMs have different strengths, and so sometimes I would choose different models for writing the first draft and for reflecting and trying to improve it. 1:46 For example, it turns out reasoning models, sometimes also …"},{"title":"「Scripts」Module 3: Tool Use","url":"/courses/andrew-ng-agentic-ai/lecture/lec-03/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 3: Tool Use 3.1 What are tools? 0:00 In this module, you learn about tool use by LLMs, and that means letting your LLM decide when it might want to request to have a function called to take some action, or gather some information, or do something else. 0:12 Just as we as humans can do a lot more with tools than we can with just our bare hands, LLMs too can also do a lot more with access to tools. 0:23 But rather than using hammers and spanners and pliers, when we give tools, that is, functions, for an LLM to request a call, that\u0026#39;s what lets it do a lot more. 0:33 Let\u0026#39;s take a look. 0:34 If you were to ask an LLM that\u0026#39;s been trained maybe many months ago, what time is it right now? 0:40 Well, that trained model does not know exactly what time it is, and so hopefully it responds, sorry, I do not have access to the current time. 0:48 But if you were to write a function and give the LLM access to this function, then that lets it respond with a more useful answer. 0:56 When we let LLMs call functions, or more precisely, let an LLM request to call functions, that\u0026#39;s what we mean by tool use, and the tools are just functions that we provide to the LLM that it can request to call. 1:08 In detail, this is how tool use works. 1:11 In this example, I\u0026#39;m going to give the getCurrentTime function that I showed on the previous slide to the LLM. 1:18 When you then prompt it, what time is it, the LLM can decide to call the getCurrentTime function. 1:23 That will return the current time, which is then fed back to the LLM in the conversational history, and finally the LLM can output is, say, 3.20pm. 1:33 So the sequence of steps is, there\u0026#39;s the input prompt. 1:36 The LLM, in this case, looks at the set of tools, which is just one tool in this example, but looks at the set of tools available, and it will decide in this example to call the tool. 1:47 The tool is a function that then returns a value, that value is fed back to the LLM, and then finally the LLM generates its output. 1:54 Now, one important aspect of tool use is, we can leave it up to the LLM to decide whether or not to use any of the tools. 2:03 So for the same setup, if I was asking it, how much caffeine is in green tea, the LLM doesn\u0026#39;t need to know the current time to answer this, and so it can generate an answer directly, 2:12 green tea typically has this much caffeine, and it does so without invoking the getCurrentTime function. 2:18 In my slides, I\u0026#39;m going to use this notation with this dashed box on top of the LLM to indicate that we\u0026#39;re providing a set of tools to the LLM for the LLM to choose to use when it deems appropriate. 2:30 This is as opposed to some examples you saw in earlier videos, where I, as a developer, had hard-coded in, for example, that I will always do a web search at this point in the research agent. 2:41 In contrast, the getCurrentTime function call is not hard-coded in, it\u0026#39;s up to the LLM to decide whether or not it wants to request a call to the getCurrentTime function. 2:51 And again, we\u0026#39;re going to use this dashed box notation to indicate when we\u0026#39;re giving one or more tools to the LLM for the LLM to decide what tools, if any, it wants to call. 3:02 Here are some more examples of when tool use may help an LLM-based app generate better answers. 3:08 If you were to ask it, can you find some Italian restaurants near Mountain View, California? 3:12 If it has a web search tool, then an LLM might elect to call a web search engine for a query, restaurants near Mountain View, California, and use the results that fetches to generate the output. 3:23 Or if you are running a retail store and you want to be able to answer questions like, show me customers who bought white sunglasses, if your LLM is given access to a query database tool, then it might look up the table of sales for what entries had a pair of white sunglasses sold and then use that to then generate the output. 3:44 Finally, if you wanted to do an interest rate calculation, if I were to deposit $500 and after 10 years, an interest rate of 5%, what would I have? 3:53 If you happen to have an interest calculation tool, then it could invoke the interest calculation function to calculate that. 4:01 Or it turns out, one thing you see later is letting an LLM write code, like just write a mathematical expression like this, and then to evaluate it, that would be another way to let an LLM calculate the right answer. 4:16 So as a developer, it\u0026#39;ll be up to you to think through what are the sorts of things you want an application to really do, and then to create the functions or the tools that are needed to make them available to the LLM to let it use the appropriate tools to complete the sorts of tasks that maybe a restaurant recommender or a retail question answer or a finance assistant may want to do. 4:39 So depending on your application, you may have to implement and make different tools available to your LLM. 4:46 So far, most of the examples we\u0026#39;ve gone through made only one tool or one function available to the LLM. 4:52 But there are many use cases where you want to make multiple tools or multiple functions available for the LLM for it to choose which of any to call. 4:59 For example, if you\u0026#39;re building a calendar assistant agent, you might then want it to be able to fulfill requests like, please find a free slot on Thursday in my calendar and make an appointment with Alice. 5:11 So in this example, we might make available to the LLM a tool or a function to make an appointment, that is, to send a calendar invite, to check the calendar to see when I might be free, as well as to delete the appointment if it ever wants to cancel an existing calendar entry. 5:26 And so given the set of instructions, the LLM would first decide that of the different tools available, probably the first one it should use is check calendar. 5:35 So call a check calendar function that will return when I am free on Thursday. 5:40 Based on that information, which is fed back to the LLM, it can then decide that the next step is to pick a slot, let\u0026#39;s say 3 p.m., and then to call the make appointment function to send a calendar invite to Alice, as well as to add it to my calendar. 5:56 The output of that, which hopefully is a confirmation that the calendar entry was sent out successfully, is fed back to the LLM, and then lastly, the LLM might tell me your appointment is set up with Alice at 3 p.m. Thursday. 6:08 Being able to give your LLM access to tools is a pretty big deal. It will make your applications much more powerful. 6:15 In the next video, we\u0026#39;ll take a look at how to write functions, how to create tools to then make them available to your LLM. Let\u0026#39;s go on to the next video. 3.2 Creating a tool 0:01 The process of how an LLM decides to call a function maybe seems a little bit mysterious 0:05 initially because an LLM is just trained to generate output text or output text tokens. 0:11 So how does that work? In this video, I\u0026#39;d like to walk through with you step-by-step 0:15 what the process of getting an LLM to be able to get a function called really looks like. 0:21 Let\u0026#39;s take a look. 0:22 So tools are just codes or functions that an LLM can request to be executed, 0:27 like this getCurrentTime function that we saw from the previous video. 0:31 Now, today\u0026#39;s leading LLMs are all trained directly to use tools, 0:36 but I want to walk through with you what it would look like if you had to write prompts yourself 0:41 to tell it when to use tools, and this is what we had to do in an earlier era 0:46 before LLMs were trained directly to use tools. 0:49 And even though we don\u0026#39;t do it exactly this way anymore, 0:51 this will hopefully give you a better understanding of the process, 0:54 and we\u0026#39;ll walk through the more modern syntax in the next video. 0:57 If you\u0026#39;ve implemented this function to getCurrentTime, 1:00 then in order to give this tool to the LLM, you might write a prompt like this. 1:05 You may tell it, LLM, you have access to a tool called getCurrentTime. 1:09 To use it, I want you to print out the following text. 1:12 Print out all caps function and then print out getCurrentTime. 1:15 And if I ever see this text, all caps function and then getCurrentTime, 1:19 that\u0026#39;s when I know you want me to call the getCurrentTime function for you. 1:23 When a user asks, what time is it? 1:25 The LLM will then realize it needs to call or request to get called the getCurrentTime function. 1:29 And so the LLM will then output what it was told. 1:32 It\u0026#39;ll output all caps function: getCurrentTime. 1:35 Now, I then have to have written code to look at the output of the LLM 1:40 to see if there is this all caps function. 1:42 And if so, then I need to pull out the argument of this getCurrentTime 1:47 to figure out what function the LLM wants to call. 1:49 And then I need to write code to actually call the getCurrentTime function 1:53 and then pull out the output, which is, let\u0026#39;s say, 8 a.m. 1:57 And then it is the developer written code, my code, 2:00 that has to take 8 a.m. and feed that time, 8 a.m., 2:04 back into the LLM as part of this conversational history. 2:07 And the conversational history, of course, includes the initial user prompt, 2:10 the fact that the request is a function call, and so on. 2:13 And lastly, the LLM, knowing what had happened earlier, 2:17 that the user asks a question, requests a function call, 2:19 and then also that I call the function and return 8 a.m. 2:23 Finally, the LLM can look at all this and generate the final response, 2:26 which is, it is 8 a.m. 2:28 So to be clear, in order to call a function, 2:31 the LLM doesn\u0026#39;t call the function directly. 2:34 It instead outputs something in a specific format like this 2:38 that tells me that I need to call the function for the LLM 2:41 and then tell the LLM what was the output of the function I requested. 2:45 In this example, we had given the LLM only a single function, 2:49 but you can imagine if we gave it three or four functions, 2:52 we could tell it to output functions in all caps, 2:55 then the name of the function it wants called, 2:57 and maybe even some arguments of these functions. 3:00 In fact, now let\u0026#39;s take a look at a slightly more complex example 3:03 where the getCurrentTime function accepts an argument for the time zone 3:08 at which you want the current time. 3:10 For this second example, I\u0026#39;ve written a function 3:14 that gets the current time in a specified time zone, 3:16 where here the time zone is the input argument 3:19 to the getCurrentTime function. 3:22 So to let the LLM use this tool to answer questions 3:25 like maybe, what time is it in New Zealand? 3:27 Because my answer is there, so before I call her up, 3:29 I do look up what time it is in New Zealand. 3:31 To let the LLM use this tool, you might modify the system prompt 3:35 to say you can use the getCurrentTime tool for a specific time zone. 3:39 To use it, I\u0026#39;ll put the following, getCurrentTime, 3:41 and then, you know, include the time zone. 3:43 And this is an abbreviated prompt. 3:45 In practice, you might put more details than this into the prompt 3:48 to tell it what is the function, how to use it, and so on. 3:50 In this example, the LLM will then realize 3:53 it needs to fetch the time in New Zealand, 3:56 and so it will generate output like this, 3:58 function: getCurrentTime Pacific/Auckland. 4:02 This is the New Zealand time zone 4:04 because Auckland is a major city in New Zealand. 4:06 Then I have to write code to search for whether or not 4:10 this function all caps appeared in the LLM output, 4:13 and if so, then I need to pull out the function to call. 4:17 Lastly, I will then call getCurrentTime 4:19 with the specified arguments, which is generated by the LLM, 4:22 which is Pacific/Auckland, and maybe returns is 4 a.m. 4:25 Then as usual, I feed this to the LLM and the LLM outputs. 4:28 It is 4 a.m. in New Zealand. 4:30 To summarize, here\u0026#39;s the process for getting LLM to use tools. 4:34 First, you have to provide the tool to the LLM, 4:37 implement the function, and then tell the LLM that it is available. 4:40 When the LLM decides to call a tool, 4:42 it then generates a specific output that lets you know 4:45 that you need to call the function for the LLM. 4:48 Then you call the function, get its output, 4:51 take the output of the function you just called, 4:53 and give that output back to the LLM, 4:55 and the LLM then uses that to go on to whatever it decides to do next, 4:59 which in our examples in this video was to just generate the final output, 5:03 but sometimes it may even decide that the next step 5:05 is to go call yet another tool, and the process continues. 5:09 Now, it turns out that this all-caps function syntax is a little bit clunky. 5:13 This is what we used to do before LLMs were trained natively 5:17 or to know by themselves how to request that tools be called. 5:21 With modern LLMs, you don\u0026#39;t need to tell it to output all-caps function, 5:25 then search for all-caps function, and so on. 5:27 Instead, LLMs are trained to use a specific syntax 5:31 to request very clearly when it wants a tool called. 5:34 In the next video, I want to share with you 5:36 what the modern syntax actually looks like 5:38 for letting LLMs request to have tools be called. 5:42 Let\u0026#39;s go on to the next video. 3.3 Tool syntax 0:01 Let\u0026#39;s take a look at how to write code to have your LLM get tools called. 0:04 Here\u0026#39;s our old getCurrentTime function without the time zone argument. 0:09 Let me show you how to use the AI Suite open source library in order to have your LLM call 0:14 tools. By the way, technically, as you saw from the last video, the LLM doesn\u0026#39;t call the tool. 0:20 The LLM just requests that you call the tool. But among developers building agentic workflows, 0:25 many of us will occasionally just say the LLM calls the tool, even though it\u0026#39;s not technically 0:30 what happens, but because it\u0026#39;s just a shorter way to say it. 0:34 This syntax here is very similar to the OpenAI syntax for calling these LLMs, except that here, 0:41 I\u0026#39;m using the AI Suite library, which is an open source package that some friends and I had worked 0:46 on that makes it easy to call multiple LLM providers. So the code syntax, and if this 0:53 looks like a lot to you, don\u0026#39;t worry about it. You\u0026#39;ll see more of this in the code labs. 0:57 But very briefly, this is very similar to the OpenAI syntax, where you say response equals 1:02 client check, completions create, then select the model, which in this case, we\u0026#39;ll use the 1:07 OpenAI model GPT-4o, messages equals messages, assuming you\u0026#39;ve put into an array here the 1:13 messages you want to pass the LLM, and it will say tools equals, then a list of the tools you want 1:19 the LLM to have access to. And in this case, there\u0026#39;s just one tool, which is get current time, 1:23 and then don\u0026#39;t worry too much about the max turns parameter. This is included because 1:28 after a tool call returns, the LLM might decide to call another tool, and after that tool call 1:33 returns, the LLM might decide to call yet another tool. So max turns is just a ceiling on how many 1:38 times you want the LLM to request one tool after another before you stop to just break out of a 1:44 possible infinite loop. In practice, you almost never hit this limit unless your code is doing 1:49 something unusually ambitious. So I wouldn\u0026#39;t worry about the max turns parameter. I usually just set it to 1:54 five, but in practice, it doesn\u0026#39;t matter that much. And it turns out that with AISuite, the function 2:00 get current time is automatically described to the LLM in an appropriate way to enable the LLM to 2:06 know when to call it. So rather than you needing to manually write a long prompt to tell the LLM, 2:12 once get current time, this syntax in AISuite does that automatically. And to make it seem not 2:18 too mysterious, the way it does that, it actually looks at the dot string associated with get current 2:23 time with this comments in get current time in order to figure out how to describe this function 2:30 to the LLM. So to illustrate how this works, here\u0026#39;s the function again, and here\u0026#39;s the 2:36 snippet of code using AISuite to call the LLM. Behind the scenes, what this will do is create a 2:43 JSON schema that describes the function in detail. And this over here on the right is what is actually 2:50 passed to the LLM. And specifically, it will pull the name of the function, which is get current time, 2:55 and then also a description of the function, which is pulled out from the doc string to tell the LLM 3:01 what this function does, which lets it decide when to call it. There\u0026#39;s some APIs which require that 3:06 you manually construct this JSON schema and then pass this JSON schema to the LLM, but the AISuite 3:13 package does this automatically for you. To go through a slightly more complex example, if you 3:18 have this more complex get current time tool that also has an input time zone parameter, then AISuite 3:25 will create this more complex JSON schema where, as before, it pulls out the name of the function, 3:30 which is get current time, pulls out the description from the doc string, and then also identifies 3:35 what are the parameters and describes them to the LLM based on the documentation here shown on the 3:41 left, so that when it\u0026#39;s generating the function arguments to call the tool, it knows that it 3:47 should be something like America/New York or Pacific/Auckland or some other time zone. 3:53 And so if you execute this code snippet here on the lower left, it will use the OpenAI 3:59 GPT-4o model, see if the LLM wants the function called, and if so, it\u0026#39;ll call the function, 4:04 get the output from the function, feed that back to the LLM, and do that up to a maximum of five 4:10 turns and then return the response. Note that if the LLM requests to call the get current time 4:17 function, AISuite or this client, it will call the get current time for you, so you don\u0026#39;t need 4:23 to explicitly do it yourself. All that is done in this single function call that you have to write. 4:29 Just note that there are some other implementations of LLM interfaces where you have to do that step 4:36 manually, but with this particular package, this is all wrapped into this client chat completions 4:41 create function call. So you now know how to get an LLM to call functions, and I hope that you enjoy 4:49 playing with this in the labs, and it\u0026#39;s actually really amazing when you provide a few functions 4:54 to LLM and LLM decides to go and take action in the world, go and get more information 4:59 to fulfill your requests. If you haven\u0026#39;t played with this before, I think you\u0026#39;ll find this to be 5:04 really cool. It turns out that of all the tools you can give an LLM, there\u0026#39;s one that\u0026#39;s a bit special, 5:10 which is a code execution tool. It turns out to be really powerful. If you can tell an LLM, 5:16 you can write code, and I will have a tool to execute that code for you, because code can do 5:21 a lot of things, and we give an LLM the flexibility to write code and have code executed. That turns 5:28 out to be an incredibly powerful tool to give to LLMs. So code execution is special. Let\u0026#39;s go 5:35 on to the next video to talk about the code execution tool for LLMs. 3.4 Code execution 0:05 In a few agentic applications I\u0026#39;ve worked on, I gave the LLM the option to write code to then 0:06 carry out the task I wanted it to. And I\u0026#39;ve been a few times now, I\u0026#39;ve been really surprised and 0:10 delighted by the cleverness of the code solutions it generated in order to solve various tasks for 0:17 me. So if you haven\u0026#39;t used code execution much, I think you might be surprised and delighted at 0:23 what this will let your LLM applications do. Let\u0026#39;s take a look. Let\u0026#39;s take an example of 0:29 building an application that can input math word problems and solve them for you. So you might 0:36 create tools that add numbers, subtract numbers, multiply numbers, and divide numbers. And if 0:40 someone says, please add 13.2 plus 18.9, then it triggers the add tool and then it gets you the 0:46 right answer. But what if someone now types in, what is the square root of two? Well, one thing 0:50 you could do is write a new tool for a square root, but then maybe some new thing is needed to 0:56 carry out exponentiation. And in fact, if you look at the number of buttons on your modern 1:02 scientific calculator, are you going to create a separate tool for every one of these buttons and 1:06 the many more things that we would want to do in math calculation? So instead of trying to implement 1:12 one tool after another, a different approach is to let it write and execute code. To tell the LLM 1:19 to write code, you might write a prompt like this. Write code to solve the user\u0026#39;s query. Return your 1:24 answer as Python code delimited with execute Python and closing execute Python tags. So given a query 1:31 like what is the square root of two, the LLM might generate outputs like this. You can then use 1:37 pattern matching, for example, a regular expression to look for the start and end execute Python tags 1:44 and extract the code in between. So here you get these two lines of code shown in the green box, 1:50 and you can then execute this code for the LLM and get the output, in this case, 1.4142 and so on. 1:57 Lastly, this numerical answer is then passed back to the LLM and it can write a nicely formatted 2:04 answer to the original question. There are a few different ways you can carry out the code 2:08 execution step for the LLM. One is to use Python\u0026#39;s exec function. This is a built-in Python function 2:15 which will execute whatever code you pass in. And this is very powerful for your LLM to really 2:21 write code and get you to execute that code, although there are some security implications 2:26 which we\u0026#39;ll see later in this video. And then there are also some tools that will let you run the code 2:31 in a safer sandbox environment. And of course, square root of two is a relatively simple example. 2:38 An LLM can also accurately write code to, for example, do interest calculations and solve much harder 2:45 math calculations than this. One refinement to this idea, which you sort of saw in our section 2:52 on reflection, is that if code execution fails, so if for some reason the LLM had generated code 2:58 that wasn\u0026#39;t quite correct, then passing that error message back to the LLM to let it reflect and 3:04 maybe revise this code and try another one or two times. That can sometimes also allow it to get a 3:10 more accurate answer. Now, running arbitrary code that an LLM generates does have a small chance of 3:17 causing something bad to happen. Recently, one of my team members was using a highly agentic coder 3:24 and it actually chose to remove star.py within a project directory. So this is actually a real 3:30 example. And eventually that agentic coder did apologize. It said, yes, that\u0026#39;s actually right, 3:35 that was an incredibly stupid mistake. I guess I was glad that this agentic coder was really sorry, 3:40 but I already deleted a bunch of Python files. Unfortunately, the team member had it backed 3:44 up on GitHub repo, so there was no real harm done, but it would have been not great if this 3:50 arbitrary code, which made the mistake of deleting a bunch of files, had been executed 3:55 without the backup. So the best practice for code execution is to run it inside a sandbox 4:00 environment. In practice, the risk for any single line of code is not that high. So if I\u0026#39;m being 4:07 candid, many developers will execute code from the LLM without too much checking. But if you want to 4:12 be a bit safer, then the best practice is to create a sandbox so that if an LLM generates bad 4:19 code, there\u0026#39;s a lower risk of data loss or leakage of sensitive data and so on. So sandbox 4:26 environments like Docker or E2B as a lightweight sandbox environment can reduce the risk of 4:33 arbitrary codes being executed in a way that damages your system or your environment. 4:39 It turns out that code execution is so important that a lot of trainers of LLMs actually do special 4:46 work to make sure that code execution works well on their applications. But I hope that as you add 4:52 this as one more tool for you to potentially offer to LLMs or let you make your applications 4:58 much more powerful. So far and what we\u0026#39;ve discussed, you have to create tools and make them 5:05 available one at a time to your LLM. It turns out that many different teams are building similar 5:11 tools and having to do all this work of building functions and making them available to the OMs. 5:18 But there is recently a new standard called MCP, Model Context Protocol, that\u0026#39;s making it much 5:24 easier for developers to get access to a huge set of tools for LLMs to use. This is an important 5:31 protocol that more and more teams are using to develop LLM based applications. Let\u0026#39;s go learn 5:37 about MCP in the next video. 3.5 MCP 0:00 MCP, the Model Context Protocol, was a standard proposed by Anthropic but now adopted by many 0:07 other companies and by many developers as a way to give an LLM access to more context and to 0:13 more tools. There are a lot of developers developing around the MCP ecosystem and so 0:18 learning about this will give you a lot more access to resources for your applications. 0:24 Let\u0026#39;s take a look. This is the pain points that MCP attempts to solve. If one developer is writing 0:31 an application that wants to integrate with data from Slack and Google Drive and GitHub or access 0:37 data from a Postgres database, then they might have to write code to wrap around Slack APIs 0:42 to have functions to provide to the application, write code to wrap around Google Drive APIs to 0:47 parse the application, and similarly for these other tools or data sources. Then what has been 0:54 happening in the developer community is if a different team is building a different application, 0:59 then they too will integrate by themselves with Slack and Google Drive and GitHub and so on. 1:04 So many developers were all building custom wrappers around these types of data sources. 1:10 And so if there are M applications being developed and there are N tools out there, 1:16 the total amount of work done by the community was M times N. What MCP did was propose a standard 1:24 for applications to get access to tools and data sources so that the total work that needs to be 1:29 done by the community is now M plus N rather than M times N. The initial design of MCP focused a lot 1:38 on how to give more context to an LLM or how to fetch data. So a lot of the initial tools were 1:44 ones that would just fetch data. And if you read the MCP documentation, that refers to these as 1:51 resources. But MCP gives access to both data as well as the more general functions that an 1:57 application may want to call. And it turns out that there are many MCP clients. These are the 2:04 applications that want access to tools or to data as well as service, which are often the software 2:11 wrappers that then give access to data in Slack or GitHub or Google Drive or allows you to take 2:16 actions at these different types of resources. So today there\u0026#39;s a rapidly growing list of MCP 2:23 clients that consume the tools or the resources as well as MCP service that provide the tools and 2:28 the resources. And I hope that you find it useful to build your own MCP client. Your application 2:35 maybe one day will be an MCP client. And if you want to provide resources to other developers, 2:40 maybe you can build your own MCP server someday. Let me show you a quick example of using an MCP 2:46 client. This is a cloud desktop app and it has been connected to a GitHub MCP server. So when 2:54 I enter this query, summarize the readme.md from the GitHub repo at this URL, this is actually 3:00 an AI suite repo. Then this application, which is an MCP client, uses the GitHub MCP server with 3:07 the request, please get the file readme.md from the repo AI suite from this repo. And then it 3:14 gets this response, which is pretty long. All this is then fed back to the LLMs context and the LLM 3:21 then generates the summary of the markdown file. Now let me enter another request, which is let me 3:27 enter what are the latest pull requests. This in turn causes the LLMs to use the MCP server to make 3:36 a different request, to list the pull request. This is another tool provided by GitHub\u0026#39;s MCP 3:42 server. And so it makes this request with repo AI suite, sort, going to update it, list 20, and so on. 3:50 And then it gives this response, which is fed back to the LLM and the LLM then writes this nice 3:55 text summary of the latest pull request for this repo. MCP is an important standard. If you want to 4:01 learn more about it, DeepLearning.ai also has a short course that goes much deeper into just the MCP 4:08 protocol that you can check out after finishing the course, if you\u0026#39;re interested. I hope this 4:13 video gives you a brief overview of why it\u0026#39;s useful and also why many developers are now building to 4:20 this standard. This brings us to the last video on tool use. And I hope that by giving your own access 4:27 to tools, you build and build agentic applications that are much more powerful. In the next module, 4:34 we\u0026#39;ll talk about evaluations and error analysis. It turns out that one of the things I\u0026#39;ve seen 4:41 that distinguishes people that can execute agentic workflows really well versus teams that are not 4:47 as efficient at it is your ability to drive a disciplined evaluation process. In the next 4:55 set of videos, which I think is maybe the most important module of this entire course, 5:00 I hope to share with you some of the best practices of how to use evals to drive 5:05 development of agentic workflows. Look forward to seeing you in the next module. ","description":"```","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 3: Tool Use 3.1 What are tools? 0:00 In this module, you learn about tool use by LLMs, and that means letting your LLM decide when it might want to request to have a function called to take some action, or gather some information, or do something else. 0:12 Just as we as humans can do a lot more with tools than we can with just our bare hands, LLMs too can also do a lot more with access to tools. 0:23 But rather than using hammers and spanners and pliers, when we give tools, that is, functions, for an LLM to request a call, that\u0026#39;s what lets it do a lot more. 0:33 Let\u0026#39;s take a look. 0:34 If you were to ask an LLM that\u0026#39;s been trained maybe many months ago, what time is it right now? 0:40 Well, that trained model does not know exactly what time it is, and so hopefully it responds, sorry, I do not have access to the current time. 0:48 But if you were to write a function and give the LLM access to this function, then that lets it respond with a more useful answer. 0:56 When we let LLMs call functions, or more precisely, let an LLM request to call functions, that\u0026#39;s what we mean by tool use, and the tools are just functions that we provide to the LLM that it can request to call. 1:08 In detail, this is how tool use works. 1:11 In this example, I\u0026#39;m going to give the getCurrentTime function that I showed on the previous slide to the LLM. 1:18 When you then prompt it, what time is it, the LLM can decide to call the getCurrentTime function. 1:23 That will return the current time, which is then fed back to the LLM in the conversational history, and finally the LLM can output is, say, 3.20pm. 1:33 So the sequence of steps is, there\u0026#39;s the input prompt. 1:36 The LLM, in this case, looks at the set of tools, which is just one tool in this example, but looks at the set of tools available, and it will decide in this example to call the tool. 1:47 The tool is a function that then returns a value, that …"},{"title":"「Scripts」Module 4: Practical Tips for Building Agentic AI","url":"/courses/andrew-ng-agentic-ai/lecture/lec-04/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 4: Practical Tips for Building Agentic AI 4.1 Evaluations(evals) 0:05 In this module, I\u0026#39;d like to share with you practical tips for building agentic AI workflows. 0:04 I hope that these tips will enable you to be much more effective than the typical developer 0:10 at building these types of systems. I find that when developing an agentic AI system, 0:16 it\u0026#39;s difficult to know in advance where it will work and where it won\u0026#39;t work so well, 0:21 and thus where you should focus your effort. So very common advice is to try to build even a 0:27 quick and dirty system to start, so you can then try it out and look at it to see where it may not 0:34 yet be working as well as you wish, to then have much more focused efforts to develop it even 0:40 further. In contrast, I find that it\u0026#39;s sometimes less useful to sit around for too many weeks 0:46 theorizing and hypothesizing how to build it. It\u0026#39;s often better to just build a quick system in a 0:53 safe, reasonable way that doesn\u0026#39;t leak data, kind of do it in a responsible way, but just build 0:58 something quickly so you can look at it and then use that initial prototype to prioritize and try 1:03 further development. Let\u0026#39;s start with an example of what might happen after you\u0026#39;ve built a prototype. 1:10 I want to use as our first example the invoice processing workflow that you\u0026#39;ve seen previously, 1:16 with the task to extract four required fields and then to save it to a database record. After having 1:22 built such a system, one thing you might do is find a handful of invoices, maybe 10 or 20 invoices, 1:28 and go through them and just take a look at their output and see what went well and if there were 1:33 any mistakes. So let\u0026#39;s say you look through 20 invoices, you find that invoice 1 is fine, the 1:38 output looks correct. For invoice 2, maybe it confused the date of the invoice, that is when 1:43 was the invoice issued, with the due date of the invoice, and in this task we want to extract the 1:48 due date so we can issue payments on time. So then I might note down in a document or in a 1:53 spreadsheet that for invoice 2, the dates were mixed up. Maybe invoice 3 was fine, invoice 4 was 1:58 fine, and so on. But as I go through this example, I find that there are quite a lot of examples where I had 2:03 mixed up the dates. So it is based on going through a number of examples like this, that in this case 2:10 you might conclude that one common error mode is that it is struggling with the dates. In that case, 2:17 one thing you might consider would be to of course figure out how to improve your system to make it 2:23 extract due dates better, but also maybe write an eval to measure the accuracy with which it is 2:29 extracting due dates. In comparison, if you had found that it was extracting the biller address 2:35 incorrectly, who knows, maybe you have billers with unusual sounding names and so maybe it 2:40 struggles with billers, or especially if you have international billers whose names may not even all 2:45 be English letters, then you might instead focus on building an eval for the biller address. So one 2:51 of the reasons why building a quick and dirty system and looking at the output is so helpful 2:56 is it even helps you decide what do you want to put the most effort into evaluating. Now if you\u0026#39;ve 3:03 decided that you want to modify your system to improve the accuracy with which it is extracting 3:09 the due date of the invoice, then to track progress it might be a good idea to create an 3:14 evaluation or an eval to measure the accuracy of date extraction. There are probably multiple ways 3:20 one might go about this, but let me share with you how I might go about this. To create a test set or 3:25 an evaluation set, I might find 10 to 20 invoices and manually write down what is the due date. So 3:33 maybe one invoice has a due date of August 20th, 2025, and I write it down as a standard year, month, 3:39 date format. And then to make it easy to evaluate in code later, I would probably write the prompt 3:46 to the LLM to tell it to always format the due date in this year, month, date format. And with that, 3:51 I can then write code to extract out the one date that the LLM has output, which is the due date, 3:56 because that\u0026#39;s the one day we care about. So this is a regular expression, pattern matching, you know, 4:01 four numbers of the year, two for the month, two for the date, and extract that out. And then I can 4:06 just write code to test if the extract date is equal to the actual date, that is the ground 4:11 truth annotation I had written down. So with an eval set of, say, 20 or so invoices, I build and 4:18 make changes to see if the percentage of time that it gets the extracted date correct is hopefully 4:24 going up as I tweak my prompts or tweak other parts of my system. So just to summarize what 4:29 we\u0026#39;ve seen so far, we build a system, then look at outputs to discover where it may be behaving in 4:35 an unsatisfactory way, such as due dates are wrong. Then to drive improvements to this important 4:40 output, put in place a small eval with, say, just 20 examples to help us track progress. 4:46 And this lets me go back to two prompts, try different algorithms, and so on to see if I can 4:50 move up this metric of due date accuracy. So this is what improving an Agentic AI workflow will often 4:57 feel like. Look at the output, see what\u0026#39;s wrong, then if you know how to fix it, just fix it. But 5:01 if you need a longer process of improving it, then put in place an eval and use that to drive 5:05 further development. One other thing to consider is if after working for a while, if you think 5:10 those 20 examples you had initially aren\u0026#39;t good enough, maybe they don\u0026#39;t cover all the cases you 5:15 want, or maybe 20 examples is just too few, then you can always add to the eval set over time to 5:19 make sure it better reflects your personal judgments on whether or not the system\u0026#39;s performance is 5:25 sufficiently satisfactory. This is just one example. For the second example, let\u0026#39;s look at 5:30 building a marketing copy assistant for writing captions for Instagram, where to keep things 5:35 succinct, let\u0026#39;s say our marketing team tells us that they want captions that are at most 10 words 5:40 long. So we would have an image of a product, say a pair of sunglasses that we want to market, 5:45 and then have a user query, like please write a caption to sell these sunglasses, and then have a 5:52 LLM, or large multimodal model, analyze the image and the query and generate a description of the 5:58 sunglasses. And there are lots of different ways that a marketing copy assistance may go wrong, 6:03 but let\u0026#39;s say that you look at the output and you find that the copy or the text generated mostly 6:08 sounds okay, but maybe it\u0026#39;s just sometimes too long. So for the sunglasses input, generate 17 6:13 words, if you have a coffee machine, it\u0026#39;s okay, stylish is okay, blue shirt, 14 words, blender, 6:18 11 words. So it looks like in this example, the LLM is having a hard time adhering to the length 6:24 guideline. So again, there are lots of things that could have gone wrong with a marketing copy 6:28 assistant. But if you find that it\u0026#39;s struggling with the length of the output, they might build 6:33 an eval to track this so that you can make improvements and make sure it\u0026#39;s getting better 6:39 at adhering to the length guideline. So to create an eval, to measure the text length, what you 6:44 might do is create a set of test stars, so mark a pair of sunglasses, a coffee machine, and so on, 6:49 and maybe create 10 to 20 examples. Then you would run each of them through your system and write 6:56 code to measure the word count of the output. So this is Python code to measure the word count of a 7:02 piece of text. Then lastly, you would compare the length of the generated text to the 10 word target 7:10 limit. So if word count is equal to 10, now I\u0026#39;m correct, plus equals one. One difference between 7:15 this and the previous invoice processing example is that there is no per example ground truth. 7:22 The target is just 10, same for every single example. Whereas in contrast, for the invoice 7:26 processing example, we had to generate a custom target label that is the correct due date of the 7:32 invoice, and we\u0026#39;re testing the outputs against that per example ground truth. I know I used a 7:38 very simple workflow for generating these captions, but these types of evals can be applied to much 7:43 more complex generation workflows as well. Let me touch on one final example in which we\u0026#39;ll revisit 7:49 the research agents we\u0026#39;ve been looking at. If you look at the output of the research agents on 7:55 different input prompts, let\u0026#39;s say that when you ask it to write an article on recent breakthroughs 8:01 in black hole science, you find that it missed some high profile result and a loss of news coverage. 8:07 So this is an unsatisfactory result. Or if you asked it to research renting versus buying a 8:11 home in Seattle, well, it seems to do a good job. Or robotics for harvesting fruits. Well, 8:16 it didn\u0026#39;t mention a leading equipment company. So based on this evaluation, it looks like 8:22 sometimes it misses a really important point that a human expert writer would have captured. So then 8:29 I would create an eval to measure how often it captures the most important points. For example, 8:34 you might come up with a number of example prompts on black holes, robotic harvesting, 8:40 and so on. And for each one, come up with, let\u0026#39;s say, three to five gold standard discussion points 8:45 for each of these topics. Notice that here we do have a per example annotation because the 8:51 gold standard talking points, that is the most important talking points, they are different for 8:56 each of these examples. With these ground truth annotations, you might then use an LLMs judge to 9:01 count how many of the gold standard talking points were mentioned. And so an example prompt might be 9:07 to say, determine how many of the five gold standard talking points are present in the 9:12 provided essay. You have the optional prompts, the essay text, gold standard points, and so on, 9:16 and have it return a JSON object with two Gs that scores how many of the points, zero to five, 9:21 to the score, as well as an explanation. And this allows you to get a score for each prompt in your 9:28 evaluation set. In this example, I\u0026#39;m using LLM-as-a-judge to count how many of the talking points 9:35 were mentioned because there\u0026#39;s so many different ways to talk about these talking points, and so a 9:40 regular expression or a code for simple pattern matching might not work that well, which is why 9:46 you might use an LLM-as-a-judge and treat this as a slightly more subjective evaluation for whether 9:52 or not, say, event horizons were adequately mentioned. So this is your third example of how 9:57 you might build evals. In order to think about how to build evals for your application, the evals 10:04 you build will often have to reflect whatever you see or you\u0026#39;re worried about going wrong in your 10:10 application. And it turns out that broadly, there are two axes of evaluation. On the top axis is the 10:18 way you evaluate the output. In some cases, you evaluate it by writing code with objective evals, 10:26 and sometimes you use an LLM-as-a-judge for more subjective evals. On the other axis is whether 10:34 you have a per-example ground truth or not. So for checking invoice date extraction, we were writing 10:43 code to evaluate if we got the actual date, and that had a per-example ground truth because each 10:49 invoice has a different actual date. But in the example where we checked marketing copy length, 10:55 every example had a length limit of 10, and so there was no per-example ground truth for that 11:02 problem. In contrast, for counting gold standard talking points, there was a per-example ground 11:07 truth because each article had different important talking points. But we used an LLM-as-a-judge to 11:13 read the essay to see if those topics were adequately mentioned because there\u0026#39;s so many 11:17 different ways to mention the talking points. And the last of the four quadrants would be LLM-as-a-judge 11:23 with no per-example ground truth. And one place where we saw that was if you are grading 11:30 charts with a rubric. This is when we\u0026#39;re looking at visualizing the coffee machine sales, and if 11:35 you ask it to create a chart according to a rubric, such as whether it\u0026#39;s clear access labels and so on, 11:40 there is the same rubric for every chart, and that would be using an LLM-as-a-judge but without a 11:46 per-example ground truth. So I find this two-by-two grid as maybe a useful way to think about the 11:51 different types of evals you might construct for your application. And by the way, these are 11:56 sometimes also called end-to-end evals because one end is the input end, which is the user query 12:01 prompt, and the other end is the final output. And so all of these are evals for the entire end-to-end 12:08 system\u0026#39;s performance. So just to wrap up this video, I\u0026#39;d like to share a few final tips for 12:13 designing end-to-end evals. First, quick and dirty evals is fine to get started. I feel like I see 12:20 quite a lot of teams that are almost paralyzed because they think building evals is this 12:25 massive multi-week effort, and so they take longer than would be ideal to get started. But I think 12:32 just as you iterate on an agentic workflow and make it better over time, you should plan to 12:37 iterate on your evals as well. So if you put in place 10, 15, 20 examples as your first cut at 12:44 evals and write some code or try prompting an LLM-as-a-judge, just do something to start to get some 12:49 metrics that can complement the human eye at looking at the output, and then there\u0026#39;s a blend 12:54 of the two that can drive your decision making. And as the evals become more sophisticated over 12:58 time, you can then shift more and more of your trust to the metric-based evals rather than 13:03 needing to read over hundreds of outputs every time you tweak a prompt somewhere. And as you 13:08 go through this process, you\u0026#39;ll likely find ways to keep on improving your evals as well. So if you 13:15 had 20 examples to start, you may then run into places where your evals fail to capture your 13:22 judgment about what system is better. So maybe you update the system and you look at it and you feel 13:28 like this has got to work much better, but your eval fails to show the new system achieving a 13:34 higher score. If that\u0026#39;s the case, that\u0026#39;s often an opportunity to go maybe collect a larger eval set 13:40 or change the way you evaluate the output to make it correspond better to your judgment as to what 13:45 system is actually working better. And so your evals will get better over time. And lastly, 13:50 in terms of using evals to gain inspiration as to what to work on next, a lot of agentic workflows 13:56 are being used to automate tasks that, say, humans can do. And so I find for such applications, 14:02 I\u0026#39;ll look for places where the performance is worse than that of an expert human, and that 14:06 often gives me inspiration for where to focus my efforts or what are the types of examples that I 14:11 maybe get my agentic workflow to work better than it is currently. So I hope that after you\u0026#39;ve built 14:18 that quick and dirty system, you think about when it would make sense to start putting in some evals 14:23 to track the potentially problematic aspects of the system, and that that will then help you 14:28 drive improvements in the system. In addition to helping you drive improvements, it turns out that 14:34 there\u0026#39;s a method of evals that helps you hone in of your entire agentic system. What are the 14:40 components most worth focusing your attention on? Because agentic systems often have many pieces. 14:47 So which piece is going to be most productive for you to spend time working to improve? It turns 14:53 out being able to do this well is a really important skill for driving efficient development 14:58 of agentic workflows. In the next video, I\u0026#39;d like to deep dive into this topic. So let\u0026#39;s go on to 15:03 the next video. 4.2 Error analysis and prioritizing next steps 0:07 Let\u0026#39;s say you\u0026#39;ve built an agentic workflow and if it\u0026#39;s not yet working as well as you wish, 0:04 and this happens to me all the time by the way, I\u0026#39;ll often build a quick and dirty system and 0:09 it doesn\u0026#39;t do as well as I wish it would, the question is where do you focus your efforts 0:14 to make it better? Turns out agentic workflows have many different components and working on 0:19 some of the components could be much more fruitful than working on some other components. So your 0:25 skill at choosing where to focus your efforts makes a huge difference in the speed with which 0:30 you can make improvements to your system. And I found that one of the biggest predictors for how 0:35 efficient and how good a team is, is whether or not they\u0026#39;re able to drive a disciplined error 0:41 analysis process to tell you where to focus your efforts. So this is an important skill. Let\u0026#39;s take 0:47 a look at how to carry out error analysis. In the research agent example, we had carried out an error 0:54 analysis in the previous video and we saw that it was often missing key points and a human expert 1:00 would have made in writing essays on certain topics. So now you\u0026#39;ve spotted this problem that 1:06 is sometimes missing key points, how do you know what to work on? It turns out that of the many 1:12 different steps in this workflow, almost any of them could have contributed to this problem of 1:17 missing key points. For example, maybe the first LLM was generating search terms that weren\u0026#39;t great, so 1:23 it was just searching for the wrong things and did not discover the right articles. Or maybe use a 1:28 web search engine that just wasn\u0026#39;t very good. There are multiple web search engines out there, in fact 1:33 actually quite a few that I tend to use for my own base applications and some are better than others. 1:39 Or maybe web search was just fine but when we gave the list of web search results in LLM, maybe it 1:44 didn\u0026#39;t do a good job choosing the best handful to download. Maybe web fetch has fewer problems in 1:51 this case, assuming you can fetch web pages accurately. But after dumping the web pages in LLM, 1:56 maybe the LLM is ignoring some of the points in the documents we had fetched. So it turns out that 2:02 there are teams that sometimes look at this and go by gut to pick one of these components to work on 2:09 and sometimes that works and sometimes that leads to many months of work with very little progress 2:13 in the overall performance of the system. So rather than going by gut to decide which of these 2:20 many components to work on, I think it\u0026#39;s much better to carry out an error analysis to better 2:26 understand each step in the workflow. And in particular, I\u0026#39;ll often examine the traces and that 2:32 means the intermediate output after each step in order to understand which component\u0026#39;s performance 2:38 is subpar, meaning say much worse than what a human expert would do, because that points to where 2:45 there may be room for security improvement. Let\u0026#39;s look at an example. If we ask the research agent 2:50 to write an essay about recent news in black hole science, maybe the output search terms like these, 2:56 search for black hole theories Einstein, Event Horizon Telescope Radio, and so on. And I would 3:01 then have a human expert look at these and see are these reasonable web search terms for writing 3:07 about recent discoveries in black hole science. And maybe in this case an expert says these web 3:13 searches look okay, they\u0026#39;re pretty similar to what I would do as a human. Then I look at the outputs of 3:20 the web search and look at the URLs returned. So web search would return many different web pages 3:27 and maybe one web page returns is that an elementary school student claims to track a 3:32 30-year-old black hole mystery from Astro Kid News. And this doesn\u0026#39;t look like the most rigorous 3:38 peer-reviewed article. And maybe examining all of the articles that web search returns causes you to 3:44 conclude that it\u0026#39;s returning too many blog or popular press types of articles and not enough 3:50 scientific articles to write a research report of the quality that you are looking for. It\u0026#39;d be good 3:56 to just look through the outputs of the other steps as well. Maybe the LLM finds the best five 4:00 sources you can, you end up with Astro Kid News, SpaceBot 2000, Space Fun News and so on. And it is 4:06 by looking at these intermediate outputs that you can then try to get a sense of the quality of the 4:12 output of each of these steps. To introduce some terminology, the overall set of outputs of all of 4:18 the intermediate steps is often called the trace of a run of this agent. And then some terminology 4:24 you see in other sources as well is the output of a single step is sometimes called a span. 4:30 This is terminology from the computer observability literature where people try to 4:35 figure out what computers are doing. And in this course, I use the word trace quite a bit. I\u0026#39;ll use 4:41 the word span a little bit less, but you may see both of these terms on the internet. So by reading 4:46 the traces, you start to get an informal sense of where might be the most problematic components. 4:52 In order to do this in a more systematic way, it turns out to be useful to focus your attention 4:57 on the cases that the system is doing poorly on. Maybe you write some essays just fine and the 5:02 output is completely satisfactory. So I would put those aside and try to come up with a set of 5:06 examples where for whatever reason, the final output of your research agent is not quite 5:11 satisfactory and just focus on those examples. So this is one of the reasons we call error analysis 5:17 because we want to focus on the cases where the system made an error and we want to go through 5:22 to figure out which components were most responsible for the error in the research agent 5:28 output. In order to make this more rigorous, rather than reading and getting an informal sense, 5:33 you might actually build up a spreadsheet to more explicitly count up where the errors are. And by 5:40 error, I mean when a step outputs something that performs significantly worse than maybe what a 5:47 human expert would have given a similar input as that component. So I\u0026#39;ll often do this myself in a 5:52 spreadsheet. So I might build a spreadsheet like this. And so for the first query, I\u0026#39;ll look at 5:57 recent developments in black hole science. And I see that the search results has too many blog 6:01 posts, popular press articles, not enough scientific papers. And then based on this, 6:07 it is true that the five best sources aren\u0026#39;t great. But here I won\u0026#39;t say that the five best sources 6:12 did a bad job because if the inputs to LLM for selecting the five best sources were all 6:18 non-rigorous articles, then I can\u0026#39;t blame this picking the five best sources for not picking 6:23 better articles because it did the best it could have or as what did nearly as well as any human 6:28 might have given the same selection to choose from. And then you might go through this for 6:32 different prompts. Renting versus buying in Seattle. Maybe it missed a well-known blog. 6:37 Robotics for harvesting fruit. Maybe in this case, we look at it and say, 6:41 oh, the search terms are too generic and the search results also weren\u0026#39;t good and so on. 6:45 And then based on this, I would count up in my spreadsheet how often I observe errors in the 6:52 different components. So in this example, I\u0026#39;m dissatisfied with the search terms 5% of the time, 6:57 but I\u0026#39;m dissatisfied with the search results 45% of the time. And if I actually see this, 7:01 I might just take a careful look at the search terms to make sure that the search terms really 7:05 were okay and that poor choice of search terms were not what led to poor search results. But 7:10 if I really think the search terms are fine, but the search results are not, then I would take a 7:14 careful look at the web search engine I\u0026#39;m using and if there are any parameters I can tune to make 7:18 it bring back more relevant or higher quality results. There\u0026#39;s this type of analysis that 7:23 tells me in this example that maybe I really should focus my attention on fixing the search 7:28 results and not on the other components of this agentic workflow. So to wrap up this video, 7:34 I find that it\u0026#39;s useful to develop a habit of looking at traces. After you build an agentic 7:40 workflow, go ahead and look at the intermediate outputs to get a feel for what it is actually 7:44 doing at every step so that you can better understand if different steps are performing 7:48 better or worse. And a more systematic error analysis, maybe done with a spreadsheet, 7:54 can let you gather statistics or count up which component performs poorly most frequently. And 7:59 so by looking at what components are doing poorly, as well as where I have ideas for 8:05 efficiently improving different components, then that will let you prioritize what component to 8:10 work on. So maybe a component is problematic, but I don\u0026#39;t have any ideas for improving it, 8:15 so that would suggest maybe not prioritizing that as high. But if there is a component that is 8:20 generating a lot of errors, and if I have ideas how to improve that, then that would be a good 8:25 reason to prioritize working on that component. And I just want to emphasize that error analysis 8:31 is a very helpful output for you to decide where to focus your efforts, because in any complex 8:37 system, there are just so many things you could work on. It\u0026#39;s too easy to pick something to work 8:42 on and work on it for weeks or even months, only to discover later that that did not result in 8:47 improved performance in your overall system. And so using error analysis to decide where to focus 8:52 your effort turns out to be incredibly useful for improving your efficiency. In this video, 8:58 we went over error analysis with the research agent example, but I think error analysis is 9:04 such an important topic, I want to go over some additional examples with you. 9:08 So let\u0026#39;s go on to the next video, where we\u0026#39;ll look at more examples of error analysis. 4.3 More error analysis examples 0:05 I found that for many developers, it\u0026#39;s only by seeing multiple examples that you can then 0:05 get practice and hone your intuitions about how to carry out error analysis. 0:09 So let\u0026#39;s take a look at two more examples, and we\u0026#39;ll look at invoice processing 0:14 and responding to customer emails. 0:16 Here\u0026#39;s the workflow that we had for invoice processing, where we had a clear process to 0:21 follow an agentic workflow of identifying the four required fields and then recording 0:27 them in a database. 0:28 In the example from the first video of this module, we said that the system was often 0:32 making a mistake in the due date of the invoice. 0:36 So we can carry out error analysis to try to figure out which of the components it may 0:40 have been due to. 0:41 So for example, did the PDF to text make a mistake, or did the LLM extract the wrong date 0:47 out of whatever was output from the PDF to text component? 0:51 To carry out an error analysis, I would try to find a number of examples where the data 0:56 extracted is incorrect. 0:58 So same as the last video, it\u0026#39;s useful to focus on the examples where the performance 1:02 is subpar to try to figure out what went wrong with those examples. 1:05 So ignore the examples that got the date right, but try to find somewhere between 10 and 100 1:10 invoices where it got the date wrong. 1:12 And then I would look through to try to figure out was the cause of the problem that PDF 1:18 to text got the date wrong, or was it that the LLM, given the PDF to text output, pulled 1:24 out the wrong date. 1:25 And so you might build up a little spreadsheet like this and go through 20 invoices and just 1:30 count up how often did PDF to text extract the dates or the text incorrectly so that 1:35 even a human couldn\u0026#39;t tell what is the due date versus the PDF to text look good enough, 1:40 but the LLM, when asked to pull the dates, somehow pulled out the wrong date, like maybe 1:44 identifying the invoice date rather than the due date of the invoice. 1:48 So in this example, it looks like the LLM data extraction was responsible for a lot 1:52 more errors. 1:53 So this tells me that maybe I should focus my efforts on the LLM data extraction component 1:57 rather than on PDF to text. 1:59 And this is important because if not for this error analysis, I can imagine some teams spending 2:05 weeks or months trying to tune the PDF to text only to discover after that time that 2:10 it did not make much of an impact to the final system\u0026#39;s performance. 2:14 Oh, and by the way, these percentages here at the bottom can add up not to 100% because 2:20 these errors are not mutually exclusive. 2:22 To look at one last example, let\u0026#39;s go back to the agentic workflow for responding to 2:27 customer emails, where the LLM, given a customer email like this, asking for an order, would 2:34 pull up the order details, fetch the information from the database, then draft a response for 2:39 a human to review. 2:40 So again, I would find a number of examples where, for whatever reason, the final output 2:46 is unsatisfactory and then try to figure out what had gone wrong. 2:50 And so some things that could go wrong. 2:52 Maybe the LLM had written an incorrect database query. 2:56 So when the query was sent to the database, it just did not successfully pull up the customer 3:01 info. 3:02 Or maybe the database has corrupted data. 3:05 So even though the LLM wrote a completely appropriate database query, maybe in SQL or some other 3:10 query language, the database did not have the correct information. 3:13 Or maybe given the correct information about the customer order, the LLM wrote an email 3:17 that was somehow not quite right. 3:20 So again, I would look through a handful of emails where the final output was unsatisfactory 3:25 and try to figure out what had gone wrong. 3:26 So maybe in email one, we find that the LLM had asked for the wrong table in the query, 3:31 just asked for the wrong data in the way it created the database. 3:34 In email two, maybe I find that the database actually has an error. 3:38 And maybe given that input, the LLM somehow wrote a subalternate email as well, and so 3:44 on. 3:44 And in this example, after going through many emails, maybe I find that the most common 3:50 error is in the way the LLM is writing a database query, say a SQL query, in order to fetch 3:57 the relevant information. 3:58 Whereas the database is mostly correct, although there\u0026#39;s a little bit of data errors there. 4:02 And the way the LLM writes the email also has some errors. 4:05 Maybe it doesn\u0026#39;t quite where they write 30% of the time. 4:08 And this tells me that it\u0026#39;d be most worthwhile maybe for me to improve the way the LLM is 4:13 writing queries. 4:14 Second most important would be maybe improve the prompting for how I write the final email. 4:20 That an analysis like this can tell you that 75% of the errors, maybe the system gets lots 4:25 of things right, but of all the things it gets not quite right, 75% of the problems 4:29 is from the database query. 4:31 This is incredibly helpful information to tell you where to focus your efforts. 4:36 When I\u0026#39;m developing Agentic AI workflows, I\u0026#39;ll often use this type of error analysis 4:40 to tell me where to focus my attention in terms of what to work on next. 4:45 When you\u0026#39;ve made that determination, it turns out that to complement the end-to-end 4:49 evals that we spoke about earlier in this module, it\u0026#39;s often useful to evaluate not 4:54 just the entire end-to-end system, but also individual components, because that can make 4:59 you more efficient in how you improve the one component that, say, error analysis has 5:05 caused you to decide to focus your attention on. 5:08 So let\u0026#39;s go on to the next video to learn about component-level evals. 4.4 Component-level evaluations 0:04 Let\u0026#39;s take a look at how to build and use component-level evals. 0:04 In our example of a research agent, we said that the research agent was sometimes missing 0:09 key points. But if the problem was web search, if every time we change the web search engine, 0:15 we need to rerun the entire workflow, that can give us a good metric for performance, 0:20 but that type of eval is expensive. Moreover, this is a pretty complicated workflow, 0:26 so even if web search made things a little bit better, maybe noise introduced by the randomness 0:31 of other components would make it harder to see little improvements to the web search quality. 0:38 So as an alternative to only using end-to-end evals, what I would do is consider building an 0:43 eval just to measure the quality of the web search component. For example, to measure the 0:48 quality of the web search results, you might create a list of gold standard web resources. 0:53 So for a handful of queries, have an expert say, these are the most authoritative sources that if 0:58 someone was searching the internet, they really should find these web pages or any of these web 1:03 pages would be good. And then you can write code to capture how many of the web search outputs 1:09 correspond to the gold standard web resources. The standard metrics from information retrieval, 1:15 the F1 score, don\u0026#39;t worry about the details if you don\u0026#39;t know what that means, but there are 1:18 standard metrics that allow you to measure of a list of web pages returned by web search, 1:23 how much does that overlap with what an expert determined are the gold standard web resources. 1:29 With this, you\u0026#39;re now armed with a way to evaluate just the quality of the web search component. 1:34 And so as you vary the parameters or hyperparameters of how you care about web search, 1:40 such as if you swap in and out different web search engines, so maybe try Google and Bing 1:45 and Dr. Go and Tivoli and U.com and others, or as you vary the number of results or as you vary 1:50 the date range that you ask the web search engines to search over, this can very quickly let you 1:54 judge if the quality of the web search component is going up and does make more incremental 2:01 improvements. And then of course, before you call the job done, it would be good to run an 2:05 end-to-end eval to make sure that after tuning your web search system for a while that you are 2:10 improving the overall system performance. But during that process of tuning these hyperparameters 2:16 one at a time, you could do so much more efficiently by evaluating just one component 2:20 rather than needing to rerun end-to-end evals every single time. So component level evals can 2:27 provide a clearer signal for specific errors. It actually lets you know if you\u0026#39;re improving 2:32 the web search component or whatever component you\u0026#39;re working on and avoid the noise in the 2:37 complexity of the overall end-to-end system. And if you\u0026#39;re working on a project where you have 2:43 different teams focused on different components, it can also be more efficient for one team to just 2:48 have his own very clear metric to optimize without needing to worry about all of the other components. 2:53 And so this lets the team work on a smaller, more targeted problem faster. So when you\u0026#39;ve decided to 3:00 work on improving a component, consider if it\u0026#39;s worth putting in place a component-wise eval and 3:05 if that will let you go faster on improving the performance of that component. Now the one thing 3:11 you may be wondering is, if you decided to improve a component, how do you actually go about making 3:16 that one component work better? Let\u0026#39;s take a look at some examples of that in the next video. 4.5 How to address problems you identify 0:06 An agentic workflow may comprise many different types of components, and so your tools for 0:05 improving different components will be pretty different. But I\u0026#39;d like to share with you some 0:09 general patterns I\u0026#39;ve seen. Some components in your agentic workflow will be non-LLM-based, 0:15 so it may be something like a web search engine or a text retrieval component, 0:20 if that\u0026#39;s part of your RAG or Retrieval Augmented Generating System, something for code execution, 0:24 or maybe with a separately trained machine learning model, maybe for speech recognition 0:28 or detecting people in pictures, and so on. So sometimes these non-LLM-based components will 0:34 have parameters or hyperparameters that you can tune. So for web search, you can tune things like 0:39 the number of results or maybe the date range that you ask the web search engine to consider. 0:44 For a RAG text retrieval component, you might change the similarity threshold that determines 0:49 what pieces of text it considers similar, or the chunk size. Often RAG systems will take text and 0:55 chop it up into smaller chunks for matching, so the main hyperparameters you could use. Or for 1:00 people detection, you might change the detection threshold, so how sensitive it is and how likely 1:05 it is to declare this found a person, and this will trade off the false positives and false 1:08 negatives. If they follow all the details of the hyperparameters I just discussed, don\u0026#39;t worry 1:12 about it. The details aren\u0026#39;t that important, but often the components were parameters that 1:16 you can tune. And then of course, you can also try to replace the component. I do this a lot 1:21 in my agentic workflows, where I\u0026#39;ll swap in different RAG search engines or swap in different 1:26 RAG providers and so on, just to see if some other provider might work better. Because of the 1:32 diversity of non-LLM-based components, I think the techniques for how to improve it will be 1:37 more diverse and dependent on exactly what that component is doing. For an LLM-based component, 1:43 here are some options you might consider. One would be to try to improve your prompts. So maybe 1:49 try to add more explicit instructions. Or if you know what few-shot prompting is, that refers to 1:55 adding one or more concrete examples of an example of an input and a desired output. And so few-shot 2:01 prompting, which you can learn about from some deep learning short courses as well, is a technique 2:06 that can give your LLM some examples to hopefully help it get better performing outputs written. Or 2:12 you can also try a different LLM. So with AI Suite or other tools, it could be pretty easy to try 2:19 multiple LLMs and then you can use evals to pick the best model for your application. Sometimes, 2:25 if a single step is too complex for one LLM to do, you can consider if you want to decompose 2:30 the task into smaller steps. Or maybe decompose it into a generation step and then a reflection 2:35 step. But more generally, if you have instructions that are very complex all within one step, maybe a 2:40 single LLM has a hard time following all those instructions. And you can break the task down 2:45 to smaller steps that may be easier for, say, two or three calls in a row to carry out accurately. 2:51 And lastly, something to try when the other methods aren\u0026#39;t working well enough is to consider 2:56 fine-tuning a model. This tends to be quite a bit more complex than the other options, so it can be 3:02 quite a bit more expensive as well in terms of developer time to implement. But if you have some 3:06 data that you can use to fine-tune an LLM on, that could give you much better performance than 3:14 prompting alone. So I tend not to fine-tune a model until I\u0026#39;ve really exhausted the other 3:19 options, because fine-tuning tends to be quite complex. But for applications where after trying 3:25 everything else, if I\u0026#39;m still at, say, 90% performance or 95% performance, and I really 3:30 need to eke out those last few percentage points of improvement, then sometimes fine-tuning my own 3:35 custom model is a great technique to use. I tend to do this only on the more mature applications 3:41 because of how costly it is. It turns out that when you\u0026#39;re trying to choose an LLM to use, 3:47 one thing that\u0026#39;s very hopeful for you as a developer is if you have good intuitions about 3:52 how intelligent or how capable different large language models are. One thing you can do is just 3:56 try a lot of models and see what works best. But I find that as I work with different models, 4:00 I start to hone intuitions about which models work best for what types of tasks. And when you hone 4:06 those intuitions, you can be more efficient as well in writing good prompts for the model as 4:10 well as choosing good models for your tasks. So I\u0026#39;d like to share with you some thoughts on how to 4:16 hone your intuition on what models will work well for your application. Let\u0026#39;s illustrate this with 4:22 an example of using an LLM to follow instructions to remove or to redact PII or personally 4:29 identifiable information. So you\u0026#39;re now to remove private sensitive information. For example, 4:34 if you are using an LLM to summarize customer calls, then maybe one summary is on July 14th, 4:41 2023, Jessica Alvarez with a social security number, a certain address, a business support 4:46 ticket, and so on. So this piece of text has a lot of sensitive, personally identifiable 4:52 information. Now, let\u0026#39;s say we want to remove all PII from such summaries because we want to use the 4:58 data for downstream statistical analysis of what customers are calling about. And to protect customer 5:03 information, we want to strip out that PII before we do that downstream statistical analysis. So you 5:08 might prompt an LLM with instructions to identify all cases of PII in the text below and then return 5:15 the redacted text with redacted colon and so on. It turns out that the larger frontier models tend 5:23 to be much better at following instructions, whereas the smaller models tend to be pretty 5:29 good at answering simple factual questions, but are just not as good at following instructions. 5:34 If you run this prompt on the smaller model, the OpenWay Llama 3.1 model with 8 billion parameters, 5:40 then it may generate an output like this. It says the identified PII is social security number and 5:45 address, and then it redacts it as follows and so on. And it actually makes a few errors. It didn\u0026#39;t 5:50 follow the instructions properly. It showed the list, then redacted the text, then returned another 5:56 list, which it wasn\u0026#39;t supposed to. And in this list of PII, it missed the name. And then I think 6:02 it also didn\u0026#39;t redact partly the address. So details aren\u0026#39;t important, but it didn\u0026#39;t follow these instructions 6:07 perfectly, and maybe it missed a little bit of PII. In contrast, if you use a more intelligent model, 6:12 one that\u0026#39;s better at following instructions, you may get a better result like this, where it\u0026#39;s 6:17 actually correctly listed all the PII and correctly redacted all of the PII. And so I find that as 6:24 different LLM providers specialize on different tasks, different models really are better for 6:30 different tasks. Some are better at coding, some are better at following instructions, some are better 6:34 at certain niche types of facts. And if you can hold your intuition for what models are more or 6:40 less intelligent, and what type of instructions they\u0026#39;re more or less able to follow, then you\u0026#39;ll 6:44 be able to make better decisions as to what models to use. So to share a couple tips on how to do this, 6:50 I encourage you to play with different models often. So whenever I do a new model release, 6:55 I\u0026#39;ll often go try it out and try out different queries on it, both closed-weight proprietary 7:00 models as well as open-weight models. And I find that sometimes having a personal set of evals might 7:06 also be helpful, where there\u0026#39;s a set of things you ask a lot of different models that might help you 7:10 calibrate how well they do on different types of tasks. One other thing that I do a lot that I hope 7:16 will be useful to you is I spend a lot of time reading other people\u0026#39;s prompts. So sometimes 7:22 people will publish their prompts on the internet, and I\u0026#39;ll often go and read them to understand 7:27 what best practices in prompting look like. Or I\u0026#39;ll often chat to my friends at various companies, 7:32 including some of the frontier model companies, and share my prompts with them, take a look at 7:36 how they prompt. And sometimes I\u0026#39;ll also go to open-source packages written by people I really 7:42 respect and download the open-source package and dig through that open-source package to find the 7:48 prompts the authors have written in order to read it, in order to hold my intuition about how to 7:53 write good prompts. This is one technique that I encourage you to consider, is by reading lots of 7:58 other people\u0026#39;s prompts that will help you get better at writing prompts yourself. And I certainly 8:04 do this a lot, and I encourage you to do so too. And this will hone your intuition about what types 8:09 of instructions models are good at following, and when to say certain things to different models. 8:14 In addition to playing with models and reading other people\u0026#39;s prompts, if you try out lots of 8:19 different models in your agentic workflows, that also lets you hone your intuition. So you see 8:24 which models work best for which types of tasks, and either looking at traces to get an informal 8:30 sense, or looking at either component-wise or end-to-end evals can help you assess how well 8:36 different models are working for different parts of your workflow. And then you start to hone 8:40 intuitions about not just performance, but maybe also price and speed trade-offs for the use of 8:46 different models. And one of the reasons I tend to develop my agentic workflows with AI Suite is 8:51 because it then makes it easy to quickly swap out and try out different models. And this makes me 8:56 more efficient in terms of trying out and assessing which models work best for my workflow. So we\u0026#39;ve 9:02 talked a lot about how to improve the performance of different components to hopefully improve the 9:08 overall performance of your end-to-end system. In addition to improving the quality of the output, 9:15 one other thing you might want to do in your workflows is to optimize the latency as well as 9:21 cost. I find that for a lot of teams, when you start developing, usually the number one thing 9:25 to worry about is just are the outputs sufficiently high quality. But then when the system is working 9:31 well and you put in the production, then there\u0026#39;s often value to make it run faster as well as run 9:37 at lower cost as well. So in the next video, let\u0026#39;s take a look at some ideas for improving 9:42 cost and latency for agentic workflows. 4.6 Latency, cost optimization 0:06 When building agentic workflows, I\u0026#39;ll often advise teams to focus on getting high-quality outputs 0:05 and to optimize cost and latency only later. It\u0026#39;s not that cost and latency don\u0026#39;t matter, 0:11 but I think getting the performance or the output quality to be high is usually the hardest part, 0:18 and then only when it\u0026#39;s really working, then maybe focus on the other things. 0:22 One thing that\u0026#39;s happened to me a few times was my team built an agentic workflow, 0:26 and we shipped it to users, and then we were fortunate enough to have so many users use it 0:31 that the cost actually became a problem, and then we had to, you know, stramble to bring the cost 0:36 back down. But that\u0026#39;s a good problem to have, so I tend to worry about cost, usually less. 0:42 Not that I ignore it completely, but it\u0026#39;s just lower down my list of things to worry about, 0:47 and so we have so many users that we really need to bring the cost down per user. 0:51 And then latency, I tend to worry a bit about it, but again, not as much as just making sure 0:57 the output quality is high. But when you do get there, it will be useful to have tools to optimize 1:03 latency and cost. Let\u0026#39;s take a look at some ideas on how to do that. 1:06 If you want to optimize the latency of an agentic workflow, one thing I will often do is then 1:13 benchmark or time the workflow. So in this research agent, it takes a number of steps, 1:20 and if I were to time each of the steps, maybe LLM takes 7 seconds to generate the search terms. 1:26 Web search takes 5 seconds, this takes 3 seconds, this takes 11 seconds, 1:30 and then writing the final essay takes 18 seconds on average. And it is then by looking at this 1:35 overall timeline that I can get a sense of which components have the most room for making faster. 1:43 In this example, there may be multiple things you could try. If you haven\u0026#39;t already taken 1:47 advantage of parallelism for some steps, like maybe web fetch, maybe it\u0026#39;s worth considering 1:53 doing some of these operations in parallel. Or if you find that some of the LLMs sets are 1:59 taking too long, so if this first step takes 7 seconds, this last LLMs set takes 18 seconds, 2:04 I might also consider trying a smaller, maybe slightly less intelligent model to see if it 2:09 still works well enough for that, or if I can find a faster LLM provider. There are lots of APIs online 2:14 for different LLMs interfaces, and some companies have specialized hardware to allow them to serve 2:20 certain LLMs much faster, so sometimes it\u0026#39;s worth trying different LLMs providers to see which ones 2:26 can return tokens the fastest. But at least doing this type of timing analysis can give you a sense 2:32 of which components to focus on in terms of reducing latency. In terms of optimizing costs, 2:39 a similar calculation where you calculate the cost of each step would also let you benchmark 2:43 and decide which steps to focus on. Many LLMs providers charge per token based on the input 2:49 and output length. Many API providers charge per API call, and the computational steps may have 2:55 different costs based on how you pay for server capacity and how much the service costs. And so 3:00 for a process like this, you might decide in this example that the tokens for this LLMs step on 3:06 average cost 0.04 cents, each web search API maybe costs 1.6 cents, tokens cost this much, 3:14 API call costs this much, PDF to text costs this much, tokens for the final SA generation cost this 3:19 much, and this would maybe again give you a sense of are there cheaper components you could use or 3:23 cheaper LLMs you could use to see where the biggest opportunity is for optimizing costs. And I found 3:29 that these benchmarking exercises can be very clarifying, and sometimes they\u0026#39;ll clearly tell 3:34 me that certain components are just not worth worrying about because they\u0026#39;re not that material 3:38 or contributor to either cost or to latency. So I find that when either cost or latency becomes 3:45 an issue, by simply measuring the cost and or latency of each step, that often gives you a 3:51 basis with which to decide which components to focus on optimizing. So we\u0026#39;re nearly at the end 3:58 of this module. I know we\u0026#39;ve covered a lot, but thank you for sticking with me. Let\u0026#39;s go on to 4:02 the final video of this module to wrap up. 4.7 Development process summary 0:06 We\u0026#39;ve gone through a lot of tips for driving a disciplined, efficient process for building 0:05 Agentic AI systems. I\u0026#39;d like to wrap up by sharing with you what it feels like to be going through 0:11 this process. When I\u0026#39;m building these workflows, I feel like there are two major activities I\u0026#39;m 0:16 often spending time on. One is building, so writing software, trying to write code to improve 0:21 my system. And the second, which sometimes doesn\u0026#39;t feel like progress, but I think is equally 0:27 important, is analysis to help me decide where to focus my build efforts next. And I often go 0:32 back and forth between building and analyzing, including things like error analysis. So for 0:38 example, when building a new agentic workflow, I\u0026#39;ll often start by quickly building an end-to-end 0:43 system, maybe even a quick and dirty implementation. And this lets me then start to examine the final 0:49 outputs of the end-to-end system, or also read through traces to get a sense of where it\u0026#39;s doing 0:54 well, where it\u0026#39;s doing poorly. Based on even just looking at traces, sometimes this will give me a 0:59 gut sense of which individual components I might want to improve. And so I might go tune some 1:05 individual components or keep tuning the overall end-to-end system. As my system starts to mature 1:11 a little bit more, then beyond just manually examining a few outputs and reading through 1:15 traces, I might start to build evals and have a small data set, maybe just 10-20 examples, 1:21 to compute metrics, at least on end-to-end performance. And this then further helps me 1:27 have a more refined perspective on how to improve the end-to-end system or how to improve individual 1:32 components. As it matures even further, my analysis then becomes maybe even more disciplined, where I 1:38 start to do error analysis and look through the components and try to count up how frequently 1:42 individual components led to subpar outputs. And this more rigorous analysis then lets me be even 1:49 more focused in deciding what components to work on next or inspire ideas for improving the overall 1:54 end-to-end system. And then eventually, when it\u0026#39;s even more mature to drive more efficient improvements 2:00 at the component level, that\u0026#39;s when I might also build component-level evals. And so the workflow 2:06 of building an agentic system often goes back and forth. It\u0026#39;s not a linear process. We sometimes 2:11 tune the end-to-end system, then do some error analysis, then improve a component for a bit, 2:15 then tune the component-level evals. And I tend to bounce back and forth between these two types 2:20 of techniques. And what I see less experienced teams often do is spend a lot of time building 2:27 and probably much less time analyzing with error analysis, building evals, and so on. That would be 2:32 ideal because this is analysis that helps you really focus where to spend your time building. 2:38 And just one more tip. There are actually quite a few tools out there to help with monitoring traces, 2:44 logging runtime, computing costs, and so on. And those tools can be helpful. I sometimes use a few 2:49 of them, and quite a few of DeepLearning.ai short course partners offer those tools, and they do 2:54 work well. I find that for agentic workflows I end up working on, most agentic workflows are pretty 3:00 custom. And so I end up building pretty custom evals myself because I want to capture the things 3:07 that work incorrectly with my system. So even though I do use some of those tools, I also end 3:12 up building a lot of custom evals that are well fit to my specific application and the issues I 3:18 see with it. So thanks for sticking with me this far to the end of the fourth of five modules. 3:25 If you\u0026#39;re able to implement even a fraction of the ideas from this module, I think you\u0026#39;ll be 3:32 well ahead of the vast majority of developers in terms of your sophistication at implementing 3:38 agentic workflows. Hope you found these materials useful, and I look forward to seeing you in the 3:43 final module. We\u0026#39;ll talk about some more advanced design patterns for building 3:48 highly autonomous agents. I\u0026#39;ll see you in the last module of this course.``` ","description":"```","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ Module 4: Practical Tips for Building Agentic AI 4.1 Evaluations(evals) 0:05 In this module, I\u0026#39;d like to share with you practical tips for building agentic AI workflows. 0:04 I hope that these tips will enable you to be much more effective than the typical developer 0:10 at building these types of systems. I find that when developing an agentic AI system, 0:16 it\u0026#39;s difficult to know in advance where it will work and where it won\u0026#39;t work so well, 0:21 and thus where you should focus your effort. So very common advice is to try to build even a 0:27 quick and dirty system to start, so you can then try it out and look at it to see where it may not 0:34 yet be working as well as you wish, to then have much more focused efforts to develop it even 0:40 further. In contrast, I find that it\u0026#39;s sometimes less useful to sit around for too many weeks 0:46 theorizing and hypothesizing how to build it. It\u0026#39;s often better to just build a quick system in a 0:53 safe, reasonable way that doesn\u0026#39;t leak data, kind of do it in a responsible way, but just build 0:58 something quickly so you can look at it and then use that initial prototype to prioritize and try 1:03 further development. Let\u0026#39;s start with an example of what might happen after you\u0026#39;ve built a prototype. 1:10 I want to use as our first example the invoice processing workflow that you\u0026#39;ve seen previously, 1:16 with the task to extract four required fields and then to save it to a database record. After having 1:22 built such a system, one thing you might do is find a handful of invoices, maybe 10 or 20 invoices, 1:28 and go through them and just take a look at their output and see what went well and if there were 1:33 any mistakes. So let\u0026#39;s say you look through 20 invoices, you find that invoice 1 is fine, the 1:38 output looks correct. For invoice 2, maybe it confused the date of the invoice, that is when 1:43 was the invoice issued, with …"},{"title":"「Scripts」Module 5: Patterns for Highly Autonomous Agents","url":"/courses/andrew-ng-agentic-ai/lecture/lec-05/","section":"courses","date":"2025-10-17","summary":"Module 5: Patterns for Highly Autonomous Agents 5.1 Planning workflows 0:03 Welcome to this final module where you learn about design patterns that lets you build 0:05 highly autonomous agents, where you don\u0026#39;t need to hard code in advance the sequence of steps to take, 0:11 but it can be more flexible and decide for itself what steps it wants to take to accomplish a task. 0:16 We\u0026#39;ll talk about the planning design pattern and then later in this module, 0:20 how to build multi-agent systems. Let\u0026#39;s dive in. 0:23 Suppose you run a sunglasses retail store and have information on what sunglasses are in your 0:30 inventory stored in a database. You might want a customer service agent to be able to answer 0:35 questions like, do you have any round sunglasses in stock? They\u0026#39;re under $100. This is a fairly 0:40 complex query because you have to look through the product descriptions to see what sunglasses 0:45 are round, then look at what is in stock, and then finally see what\u0026#39;s under $100 in order to tell the 0:52 customer, yes, we have classic sunglasses. How do you build an agent to answer a broad range of 0:59 customer queries like this and many others? In order to do so, we\u0026#39;re going to give LLM 1:05 a set of tools to let it get item descriptions, such as look up if different glasses are round, 1:11 check inventory, maybe process item returns, which is not needed for this query, 1:15 but we need it for other queries, get item price, check past transactions, process item sale, 1:22 and so on. In order to let an LLM figure out what\u0026#39;s the right sequence of tools to use to respond to 1:29 the customer request, you might then write a prompt like this. You have access to the following tools 1:35 and give it a description of each of the, say, six tools or even more tools that the LLM has, 1:40 and to then tell it to return a step-by-step plan to carry out the user\u0026#39;s request. In this case, 1:47 to answer this particular query, a reasonable plan that an LLM might output might be to first 1:53 use get item descriptions to check the different descriptions to find the round sunglasses, 1:59 and then use check inventory to see if they\u0026#39;re in stock, and use get item price to see if the 2:03 in-stock results are less than $100. After an LLM outputs this plan with three steps, what we can do 2:11 is then take the step one text, that is this text written here in red, and pass that to an LLM, 2:18 maybe with additional context about what are the tools with your user query, with additional 2:22 background context, and pass an LLM to carry out step one. And in this case, hopefully the LLM will 2:29 choose to call the get item descriptions to get the appropriate descriptions of items, 2:34 and the output of that first step can let it select which are the round sunglasses, 2:39 and that output of step one is then passed together with the step two instructions, that 2:45 would be these instructions that I have here in blue, to an LLM to then execute the second step 2:49 of the plan. Hopefully it will then take the two pairs of round sunglasses we found on the previous 2:55 slide and check the inventory, and the output of that second step is then used to another LLM call, 3:02 where you have the output of the second step as well as the instructions of what to do for step 3:06 three. Pass the LLM to have it get the item price, and finally this output is fed back to 3:12 the LLM one last time to generate the final answer for the user. In this slide, I\u0026#39;ve simplified a lot 3:17 of details a little bit. The actual plan typically written by the LLM is more detailed than these 3:22 simple one-line instructions, but the basic workflow is to have an LLM write out multiple 3:27 steps of a plan, and then task it to execute each step of the plan in turn with some appropriate 3:33 surrounding context about what is the task, what are the tools available, and so on. And the 3:38 exciting thing about using an LLM to plan this way is that we did not have to decide in advance 3:45 what is the sequence in which to call tools in order to answer a fairly complex customer request. 3:52 If a customer were to make a different request, such as I would like to return the gold frame 3:58 glasses that I had purchased but not the metal frame ones, then you can imagine an LLM similarly 4:05 being able to come up with a different plan to figure out based on what they had purchased previously, 4:09 which glasses they had bought based on get item descriptions, where the gold frame ones they want 4:14 to return, and then maybe call process item return. So with an agent that can plan like this, it can 4:19 carry out a much wider range of tasks that can require calling many different tools in many 4:25 different orders. One more example of planning, let\u0026#39;s take a look at an email assistant. If you 4:30 want to be to tell your assistant, so please reply to that email invitation from Bob in New York, 4:34 tell him to attend and archive his email. Then an email assistant may be given tools like this to 4:40 search email, move an email, delete an email, and send an email. And you might write an assistant 4:44 prompt saying you have access to the following tools, and again please return the step-by-step plan. 4:48 In this case, maybe the LLM will say the steps for this are to use search email to find the email from 4:54 Bob that mentioned dinner and New York, and then generate and send an email to confirm attendance, 4:59 and then lastly move that email to the archive folder. Given this plan, which looks a reasonable 5:04 one, you would then again task an LLM step-by-step to carry out this plan. So the text from the first 5:11 step, shown here in red, will be fed to the LLM with additional background context, and hopefully 5:16 it\u0026#39;ll trigger search email. Then the output of that can be given to an LLM again with the step 5:22 two instructions to send an appropriate response. And then finally, assuming the email was sent 5:27 successfully, you can take that output and have the LLM execute the third step of moving the email 5:33 from Bob into the archive folder. The planning design pattern is already used successfully 5:39 in many highly agentic coding systems, where if you ask it to write a piece of software to build 5:45 some fairly complex application, it might actually come up with a plan to build this component, build 5:51 this component, to almost form a checklist, and then do those steps one at a time to build a 5:56 decently complex piece of software. For many other applications, the use of planning is still 6:02 maybe more experimental. It\u0026#39;s not in very widespread use. And one of the challenges of 6:07 planning is it makes the system sometimes a little bit hard to control, because you as a developer, 6:13 you don\u0026#39;t really know at runtime what plan it will come up with. And so I think outside highly 6:19 agentic coding systems, where it actually works really well, adoption of planning is still growing 6:24 in other sectors. But this is exciting technology, and I think it will keep getting better and we\u0026#39;ll 6:29 see it in more and more applications. The cool thing about building agents that can plan for 6:34 themselves is you don\u0026#39;t need to hard code in advance the exact sequence of steps an LLM may 6:39 take to carry out a complex task. Now, I know that in this video, I\u0026#39;ve gone over the planning process 6:45 at a fairly high level, with it all putting a list of steps and then tasking an LLM to carry 6:51 out the steps of the plan one step at a time. But how does this actually work? In the next video, 6:57 we\u0026#39;ll take a deeper dive to look further into the guts of what these plans actually look like, 7:03 and how the strings together to have an LLM plan and execute the plan for you. 7:07 Let\u0026#39;s take a look at that in the next video. 5.2 Creating and executing LLM plans 0:04 In this video, we\u0026#39;ll look in detail at how to prompt an LLM to generate a plan, 0:04 and how to read, interpret, and execute that plan. Let\u0026#39;s dive in. 0:08 This is a plan that you saw in the previous video for the customer service agents, 0:13 and I have presented this plan at a high level using simple text descriptions. 0:17 Let\u0026#39;s take a look at how you can get an LLM to write very clear plans that go a little bit 0:23 beyond these simple high-level text descriptions. It turns out that many developers will ask an LLM 0:30 to format the plan it once executed in JSON format, because this allows downstream code 0:36 to parse what exactly are the steps of the plan in relatively clear and unambiguous ways, 0:42 and all of the leading LLMs are pretty good at generating JSON outputs at this point. 0:46 So the system prompt might say something like this. You have access to the following tools, 0:51 and then create a step-by-step plan in JSON format, and you might describe the JSON format 0:56 in enough detail with the goal of getting it to output a plan like that shown here on the right. 1:02 So in this JSON output, it creates a list where the first list item has clear keys and values 1:09 that say step one of the plan has the following description, and it should use the following tool 1:15 with the following arguments parsed to that tool. Then after that, step two of the plan 1:20 is to carry out this task, and then use this tool, and so on. So this JSON format, as opposed to 1:26 writing the plan in English, allows downstream code to more clearly parse out exactly what are 1:32 the steps of the plan so that it can be reliably executed one step at a time. Instead of JSON, 1:38 I also see some developers use XML, where you can use XML delimiters. You use XML tags 1:45 to clearly specify what are the steps of the plan and what step number it is. 1:50 Some developers, I feel like fewer developers, will use markdown, which is just sometimes 1:55 slightly more ambiguous in terms of how we parse it, and I think plain text is maybe the least 2:00 reliable of these options. But I think either JSON, which I\u0026#39;m showing here, or XML would be 2:05 good options for how to ask the LLM to format a plan unambiguously. So that\u0026#39;s it. By opening 2:12 plans in JSON, you can then parse it and have downstream workflows execute different steps of 2:18 the plan more systematically. Now, in terms of getting LLMs to plan, it turns out there\u0026#39;s one 2:24 other really neat idea that lets an LLM output very complex plans and get them executed reliably, 2:31 and that\u0026#39;s to let them write code and to have code express the plan. 2:35 Let\u0026#39;s take a look at this in the next video. 5.3 Planning with code execution 0:03 Planning with code execution is the idea that, instead of asking an LLM to output a plan in, 0:06 say, JSON format to execute one step at a time, why not have the LLM just try to write code and 0:12 that code can capture multiple steps of the plan, like call this function, then call this function, 0:17 then call this function, and by executing code generated by the LLM, we can actually carry out 0:22 fairly complex plans. Let\u0026#39;s take a look at when you might want to use this technique. 0:27 Let\u0026#39;s say you want to build a system to answer questions about coffee machine sales based on 0:33 a spreadsheet with data like this of previous sales. You might have an LLM with a set of tools 0:38 like these to get column max, to look at a certain column and get the maximum value, 0:44 so there\u0026#39;s a whole answer, what\u0026#39;s the most expensive coffee, or get column mean, filter 0:49 rows, get column min, get column median, sum rows, and so on. So these are examples of a range of 0:55 tools you might give an LLM to process this spreadsheet or these rows and columns of data 1:00 in different ways. Now, if a user were to ask which month had the highest sales of hot chocolate, 1:06 it turns out that you can answer this query using these tools, but it\u0026#39;s pretty complicated. 1:11 You\u0026#39;d have to use filter rows to extract transactions in January for hot chocolate, 1:16 then do stats on that, and then repeat for February, figure out stats on that, 1:20 then repeat for March, repeat for April, repeat for May, all the way through December, 1:24 and then take the max, and so you can actually string it together with a pretty complicated 1:28 process using these tools, but it\u0026#39;s not such a great solution. But worse, whether someone to 1:35 ask how many unique transactions were there last week, well, these tools are insufficient to get 1:39 that answer, so you may end up creating a new tool, get unique entries, or you may run into 1:45 another query, what were the amounts of last five transactions, then you have to create yet another 1:49 tool to get the data to answer that query. And in practice, I\u0026#39;ve seen teams, when they run across 1:56 more and more queries, end up creating more and more and more and more tools to try to give the 2:01 other enough tools to cover all the range of things someone may ask about a dataset like this. 2:06 So this approach is brittle, inefficient, and I\u0026#39;ve seen teams continuously dealing with edge cases 2:13 and trying to create more tools, but it turns out there is a better way, which is if you were 2:18 to prompt LLM to say, please write code to solve the user\u0026#39;s query and return your answer as Python 2:24 code, maybe delimited with these beginning and ending execute Python XML tags, then LLM can just 2:31 write code to load the spreadsheet into a data processing library, here it\u0026#39;s using the pandas 2:38 library, and then here it actually is coming up with a plan. The plan is, after loading the CSV, 2:44 first it has to ensure the date column is parsed a certain way, then sort by the date, select the 2:49 last five transactions, show just the price column, and so on. But these are the steps one, two, three, 2:55 and four, and five, say, of the plan. Because a programming language like Python, and in this 3:01 example, also with the pandas data processing library imported, because this has many built-in 3:07 functions, hundreds or even thousands of functions, and moreover, these are functions that the LLM has 3:14 seen a lot of data on how to call when. By letting your LLM write code, it can choose from 3:21 these hundreds or thousands of relevant functions that it\u0026#39;s already seen a lot of data on when to 3:26 use, so this lets it string together different choices of functions to call from this very large 3:33 library in order to come up with a plan for answering a fairly complex query like this. 3:39 Just one more example. If someone were to ask, how many unique transactions last week? 3:44 Well, you can come up with a plan to read the CSV file, parse the date column, define the time 3:50 window, filter rows, drop duplicate rows, and count. The details of this aren\u0026#39;t important, but hopefully 3:55 what you can see is, if you read the comments here, the LLM is roughly coming up with a four-step plan 4:02 and is expressing each of the steps in code that you can then just execute, and this will get the 4:08 user their answer. So for applications where the task can plausibly be done by writing code, letting 4:15 an LLM express its plan in software code that you can just execute for the LLM can be a very powerful 4:22 way to let it write rich plans. And of course, the caveat that I mentioned in the module on tool use 4:29 to consider if you need to find a safe execution environment like a sandbox to run the code, that 4:35 also applies. Although I know that even though it\u0026#39;s probably not the best practice, I also know a lot of 4:40 developers that don\u0026#39;t use a sandbox. Lastly, it turns out that planning with code works well. 4:46 From this diagram adapted from a research paper by Xinyao Wang and others, you can see that for many 4:53 different models for the tasks that they examined, code as action in which the LLM is invited to write 5:00 code and take actions through code, that is superior to having it write JSON and then translate 5:07 JSON into action or text. And you also see a trend that writing code outperforms having the LLM write 5:15 a plan in JSON, and writing a plan in JSON is also a bit better than writing a plan in just plain text. 5:21 Now, of course, there are applications where you might want to give your custom tools to an LLM 5:26 to use, and so writing code isn\u0026#39;t for every single application. But when it does apply, it can be a 5:32 very powerful way for an LLM to express a plan. So that wraps up the section on planning. Today, one of 5:39 the most powerful uses of Agentic AI that plans is highly agentic software coders. It turns out that 5:47 if you ask one of the highly agentic software coding assistance tools to write a complex piece 5:52 of software for you, it may come up with a detailed plan to build this component of software first, 5:58 then build a second component, build a third, maybe even plan to test out the components as 6:02 going along. And then it forms a checklist that then goes through to execute one step at a time. 6:08 And so it actually works really well for building increasingly complex pieces of software. For other 6:14 applications, I think the use of planning is still growing and developing. One of the disadvantages 6:19 of planning is that because the developer doesn\u0026#39;t tell the system what exactly to do, it\u0026#39;s a little 6:25 bit harder to control it, and you don\u0026#39;t really know in advance what will happen at runtime. But giving 6:30 up some of this control, it does significantly increase the range of things that the model may 6:35 decide to try out. So this important technology is kind of cutting edge, doesn\u0026#39;t feel completely 6:42 mature outside of maybe agentic coding where it works well, although I\u0026#39;m sure there\u0026#39;s still a lot 6:47 of room to grow. But hopefully you enjoy using it in some of your applications someday. 6:54 That wraps up planning. There\u0026#39;s one last design pattern I hope to share with you in this module, 6:59 which is how to build multi-agent systems. We have not just one agent, but many of them 7:05 working in collaboration to complete the task for you. Let\u0026#39;s take a look at that in the next video. 5.4 Multi-agent workflows 0:00 We\u0026#39;ve talked a lot about how to build a single agent to complete tasks for you. 0:04 In a multi-agent or multi-agentic workflow, we instead have a collection of multiple agents 0:09 collaborate to do things for you. 0:12 When some people hear for the first time about multi-agent systems, they wonder, why do I 0:16 need multiple agents? 0:18 It\u0026#39;s just the same LLM that I\u0026#39;m prompting over and over, or just one computer. 0:22 Why do I need multiple agents? 0:25 I find that one useful analogy is, even though I may do things on a single computer, we do 0:31 decompose work in a single computer into maybe multiple processes or multiple threads. 0:37 And as a developer, thinking, even though it\u0026#39;s one CPU on a computer, say, thinking 0:42 about how to take work and decompose it into multiple processes and multi-computer programs 0:47 to run, that makes it easier for me as a developer to write code. 0:52 And in a similar way too, if you have a complex task to carry out, sometimes, instead of thinking 0:59 about how to hire one person to do it for you, you might think about hiring a team of 1:04 a few people to do different pieces of the task for you. 1:08 And so in practice, I found that for many developers of agentic systems, having this 1:12 mental framework of not asking, what\u0026#39;s the one person I might hire to do something, but 1:17 instead, would it make sense to hire people with three or four different roles to do this 1:22 overall task for me, that helps give another way to take a complex thing and decompose 1:28 it into sub-tasks and to build for those individual sub-tasks one at a time. 1:33 Let\u0026#39;s take a look at some examples of how this works. 1:36 Take the task of creating marketing assets, say you want to market sunglasses. 1:40 Can you come up with a marketing brochure for that? 1:43 You might need a researcher on your team to look at trends on sunglasses and what competitors 1:48 are offering. 1:49 You might also have a graphic designer on your team to render charts or nice-looking 1:54 graphics of your sunglasses. 1:56 And then also a writer to take the research, take the graphic assets and put it all together 2:00 into a nice-looking brochure. 2:02 Or to write a research article, you might want a researcher to do online research, a 2:06 statistician to calculate statistics, a lead writer, and then an editor to come up with 2:10 a polished report. 2:11 Or to prepare a legal case, real law firms will often have associates, paralegals, maybe 2:16 an investigator. 2:18 And we naturally, because of the way human teams do work, can think of different ways 2:24 that complex tasks can be broken down into different individuals with different roles. 2:31 So these are examples of when a complex task were already naturally decomposed into sub-tasks 2:38 that different people with different skills can carry out. 2:41 Take the example of creating marketing assets. 2:44 Look into detail into what a researcher, graphic designer, and writer might do. 2:49 A researcher might have the task of analyzing market trends and researching competitors. 2:55 And when designing the research agents, one question to keep in mind is what are the tools 3:01 that the researcher may need in order to come up with a research report on market trends 3:06 and what competitors are doing. 3:07 So one natural tool that an agentic researcher might need to use would be web search. 3:14 Because as a human researcher, asked to do these tasks might need to search online in 3:18 order to come up with their report. 3:20 Or for a graphic designer agent, they might be tasked with creating visualizations and 3:25 artwork. 3:26 And so what are the tools that an agentic software graphic designer might need? 3:31 Well, they may need image generation and manipulation APIs. 3:36 Or maybe, similar to what you saw with the coffee machine example, maybe it needs code 3:41 execution to generate charts. 3:44 And lastly, the writer has transformed the research into report text and marketing copy. 3:49 And in this case, they don\u0026#39;t need any tools other than what an LLM can already do to generate 3:54 text. 3:55 In this and the next video, I\u0026#39;m going to use these purple boxes to denote an agent. 4:00 And the way you build individual agents is by prompting an LLM to play the role of a 4:06 researcher or a graphic designer or a writer, depending on which agent it is part of. 4:11 So for example, for the research agents, you might prompt it to say, you are a research 4:15 agent, expert at analyzing market trends and competitors, carry out online research to 4:22 analyze market trends for the sunglasses product and give a summary as well of what competitors 4:26 are doing. 4:27 So that would allow you to build a researcher agent. 4:30 And similarly, by prompting an LLM to act as a graphic designer with the appropriate 4:35 tools and to act as a writer, that\u0026#39;s how you can build a graphic designer as well as 4:41 a writer agent. 4:43 Having built these three agents, one way to have them work together to generate your final 4:49 reports would be to use a simple linear audit workflow or a linear plan in this case. 4:56 So if you want to create a summer marketing campaign for sunglasses, you might give that 5:00 prompt to the research agents. 5:02 The research agent then writes a report that says, here are the current sunglasses trends 5:06 and competitive offerings. 5:08 This research report can then be fed to the graphic designer that looks at the data the 5:13 research has found and creates a few data visualizations and artwork options. 5:17 All these assets can then be passed to the writer that then takes the research and the 5:23 graphic output and writes the final marketing brochure. 5:27 The advantage of building a multi-agent workflow in this case is when designing a researcher 5:32 or graphic designer or writer, you can focus on one thing at a time. 5:36 So I can spend some time building maybe the best graphic designer agents I can, while 5:41 maybe my collaborators are building research agents and writer agents. 5:45 And in the end, we string it all together to come up with this multi-agent system. 5:50 And in some cases, I\u0026#39;m seeing developers start to reuse some agents as well. 5:56 So having built a graphic designer for marketing brochures, maybe I\u0026#39;ll think about if I can 6:01 build a more general graphic designer that can help me write marketing brochures as well 6:05 as social media posts, as well as help me illustrate online webpages. 6:10 So by coming up with what are the agents you might hire to do a task, and this will sometimes 6:16 correspond to who are the types of human employees you might hire to do a task. 6:22 You can come up with a workflow like this with maybe even building agents that you could 6:27 choose to reuse in other applications as well. 6:30 Now, what you see here is a linear plan where one agent, the researcher does his work, then 6:36 the graphic designer, and then the writer. 6:38 With agents, you can also, as an alternative to a linear plan, you can also have agents 6:44 interact with each other in more complex ways. 6:47 Let me illustrate with an example of planning using multiple agents. 6:51 So previously, you saw how we may give an LLM a set of tools that we can call to carry 6:56 out different tasks. 6:57 In what I want to show you, we will instead give an LLM the option to call on different 7:03 agents to ask the different agents to help complete different tasks. 7:07 So in detail, you might write a prompt like you\u0026#39;re a marketing manager, have the following 7:11 team of agents to work with, and then give a description of the agents. 7:14 And this is very much similar to what we\u0026#39;re doing with planning and using tools, except 7:19 the tools, the green boxes, are replaced with agents, these purple boxes that the LLM can 7:24 call on. 7:25 You can also ask it to return a step-by-step plan to carry out the user\u0026#39;s request. 7:28 And in this case, the LLM may ask the researcher to research current sunglasses trends and 7:33 then report back. 7:34 Then it will ask the graphic designer to create the images and then report back, then ask 7:38 the writer to create a report, and then maybe the LLM will choose to review or reflect on 7:42 and improve the report one final time. 7:45 In executing this plan, you would then take the step one text of the researcher, carry 7:50 out research, then pass that to the graphic designer, pass it to the writer, and then 7:55 maybe do one final reflection step, and then you\u0026#39;d be done. 7:59 One interesting view of this workflow is as if you have these three agents up here, but 8:04 this LLM on the left is actually like a fourth agent that\u0026#39;s a marketing manager, that is 8:10 a manager of a marketing team, that is setting direction and then delegating tasks to the 8:15 researcher, the graphic designer, and the writer agents. 8:18 So this becomes actually a collection of four agents for a marketing manager agent coordinating 8:22 the work of the researcher, the graphic designer, and the writer. 8:26 In this video, you saw two communication patterns. 8:30 One was a linear one where your agents took actions one at a time until you got to the 8:35 end. 8:36 And the second had a marketing manager coordinating the activity of a few other agents. 8:41 It turns out that one of the key design decisions you may end up having to make when building 8:46 multi-agentic systems is what is the communication pattern between your different agents? 8:51 This is an area of hard research and there are multiple patterns emerging, but in the 8:56 next video, I want to show you what are some of the most common communication patterns 8:59 for getting your agents to work with each other. 9:02 Let\u0026#39;s go see that in the next video. 5.5 Communication patterns for multi-agent systems 0:01 When you have a team of people working together, the patterns by which they communicate can be 0:06 quite complex. And in fact, designing an organizational chart is actually pretty 0:10 complex to try to figure out what\u0026#39;s the best way for people to communicate, to collaborate. 0:16 It turns out, designing communication patterns for multi-agent systems is also quite complex. 0:22 But let me show you some of the most common design patterns I see used by different teams today. 0:26 In a marketing team with a linear plan, where first a researcher worked, then a graphic designer, 0:31 then a writer, the communication pattern was linear. The researcher would communicate with 0:36 the graphic designer, and in both the research and the graphic designer, maybe pass the outputs to 0:40 the writer. And so there\u0026#39;s a very linear communication pattern. This is one of the 0:46 two most common communication plans that I see being used today. The second of the two most 0:51 common communication plans would be similar to what you saw in this example, with planning using 0:58 multiple agents, where there is a manager that communicates with a number of team members and 1:05 coordinates their work. So in this example, the marketing manager decides to call on the researcher 1:10 to do some work. Then if you think of the marketing manager as getting the report back, and then 1:15 sending it to the graphic designer, getting a report back, and then sending it to the writer, this would be a 1:20 hierarchical communication pattern. If you\u0026#39;re actually implementing a hierarchical communication 1:25 pattern, it\u0026#39;ll probably be simpler to have the researcher pass the report back to the marketing 1:29 manager, rather than the researcher pass the results directly to the graphic designer and to 1:34 the writer. But so this type of hierarchy is also a pretty common way to plan the communication 1:40 patterns, where you have one manager coordinating the work of a number of other agents. And just to 1:45 share with you some more advanced and less frequently used, but nonetheless sometimes used in 1:50 practice communication patterns, one would be a deeper hierarchy, where same as before, if you have 1:56 a marketing manager send tasks to the researcher, graphic designer, writer, but maybe the researcher has 2:01 themselves two other agents that they call on, such as a web researcher and a fact checker. Maybe the 2:07 graphic designer just works by themselves, whereas the writer has an initial style writer and a citation 2:13 checker. So this would be a hierarchical organization of agents, in which some agents 2:19 might themselves call other sub-agents. And I also see this used in some applications, but this is 2:25 much more complex than a one-level hierarchy, so used less often today. And then one final pattern 2:31 that is quite challenging to execute, but I see a few experimental projects use it, is the all-to-all 2:38 communication pattern. So in this pattern, anyone is allowed to talk to anyone else at any time. And 2:44 the way you implement this is you prompt all four of your agents, in this case, to tell them that 2:50 there are three other agents they could decide to call on. And whenever one of your agents decides 2:55 to send a message to another agent, that message gets added to the receiver agent\u0026#39;s contacts. And 3:02 then a receiver agent can think for a while and decide when to get back to that first agent. And 3:07 so if you can all collaborate in a crowd and talk to each other for a while until, say, each of them 3:13 declares that it is done with this task, and then it starts talking. And maybe when everyone thinks 3:18 it\u0026#39;s done, or maybe when the writer concludes it\u0026#39;s good enough, that\u0026#39;s when you generate the final 3:22 output. In practice, I find the results of all-to-all communication patterns a bit hard to predict. 3:27 So some applications don\u0026#39;t need high control. You can run it and see what you get. If the marketing 3:32 brochure isn\u0026#39;t good, maybe that\u0026#39;s okay. You just run it again and see if you get a different result. 3:37 But I think for applications where you\u0026#39;re willing to tolerate a little bit of chaos and 3:41 unpredictability, I do see some developers using this communication pattern. So that, I hope, 3:47 conveys some of the richness of multi-agent systems. Today, there are quite a lot of software 3:54 frameworks as well that support easily building multi-agent systems. And they also make implementing 4:00 some of these communication patterns relatively easy. So maybe if you use your own multi-agent system, 4:06 you\u0026#39;ll find some of these frameworks hopeful for exploring these different 4:09 communication patterns as well. And so that now brings us to the final video 4:17 of this module and of this course. Let\u0026#39;s go on to the final video to wrap up. 5.6 Conclusion 0:04 Welcome to the final video of this course. It feels like we\u0026#39;ve been through a lot together, 0:04 just you and me, and we\u0026#39;ve gone through a lot of topics in Agentic AI. Let\u0026#39;s take a look. 0:10 In the first module, we talked about what are the applications you can build with Agentic AI 0:15 that just were not possible before. And we started then to look at key design patterns, 0:22 including the reflection design pattern, which is a simple way to sometimes give your application a 0:27 nice performance boost, and then tool use or function calling, which expands what your LLM 0:33 application can do, with code execution being one important case of that. And then we spent a lot of 0:38 time talking about evaluations, as well as error analysis, and how to drive a disciplined process 0:44 of building, as well as analyzing, to be efficient in how you keep on improving the performance of 0:50 your agentic AI system. This fourth module is some of the material that I think you will find 0:56 most useful as you keep building Agentic AI systems, I hope, for a long time. And then in 1:01 this module, we talk about planning and multi-agent systems that can let you build much more powerful, 1:07 although sometimes harder to control, and harder to predict in advanced types of systems. So with 1:13 the skills you learn from this course, I think you now know how to build a lot of cool, exciting 1:19 Agentic AI applications. When my team, or I see other teams as well, interview people for jobs, 1:25 I find interviews often try to assess whether or not candidates have pretty much the skills you\u0026#39;re 1:31 learning about in this course. And so I hope that this course will also open up new professional 1:36 opportunities for you, and you just will do more. Whether you\u0026#39;re doing these things for fun, or for 1:42 professional practical settings, I think you\u0026#39;ll enjoy this new set of things you can now build. 1:50 So, just to wrap up, I want to thank you again for spending all this time with me, 1:55 and I hope you will take these skills, use them responsibly, and just go build cool stuff. ","description":"```","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":"Module 5: Patterns for Highly Autonomous Agents 5.1 Planning workflows 0:03 Welcome to this final module where you learn about design patterns that lets you build 0:05 highly autonomous agents, where you don\u0026#39;t need to hard code in advance the sequence of steps to take, 0:11 but it can be more flexible and decide for itself what steps it wants to take to accomplish a task. 0:16 We\u0026#39;ll talk about the planning design pattern and then later in this module, 0:20 how to build multi-agent systems. Let\u0026#39;s dive in. 0:23 Suppose you run a sunglasses retail store and have information on what sunglasses are in your 0:30 inventory stored in a database. You might want a customer service agent to be able to answer 0:35 questions like, do you have any round sunglasses in stock? They\u0026#39;re under $100. This is a fairly 0:40 complex query because you have to look through the product descriptions to see what sunglasses 0:45 are round, then look at what is in stock, and then finally see what\u0026#39;s under $100 in order to tell the 0:52 customer, yes, we have classic sunglasses. How do you build an agent to answer a broad range of 0:59 customer queries like this and many others? In order to do so, we\u0026#39;re going to give LLM 1:05 a set of tools to let it get item descriptions, such as look up if different glasses are round, 1:11 check inventory, maybe process item returns, which is not needed for this query, 1:15 but we need it for other queries, get item price, check past transactions, process item sale, 1:22 and so on. In order to let an LLM figure out what\u0026#39;s the right sequence of tools to use to respond to 1:29 the customer request, you might then write a prompt like this. You have access to the following tools 1:35 and give it a description of each of the, say, six tools or even more tools that the LLM has, 1:40 and to then tell it to return a step-by-step plan to carry out the user\u0026#39;s request. In this case, 1:47 to answer this particular query, a reasonable plan that …"},{"title":"「吴恩达 Agentic AI 课程实录」模块1：Agentic AI简介","url":"/courses/andrew-ng-agentic-ai/lecnotes-zh/lec-01/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 模块1：Agentic AI简介「Andrew Ng：Agentic AI」 1.0 简介 欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使用 Agentic AI 构建的真正有价值和有用的应用数量也增长得非常迅速，即使没有炒作那么快。在本课程中，我想向您展示构建 Agentic AI 应用的最佳实践。这将在您现在可以构建什么方面，为您开启许多新的机会。\n如今，agentic 工作流正被用于构建客户支持代理等应用，或进行深度研究以帮助撰写富有洞察力的研究报告，或处理棘手的法律文件，或查看患者输入信息并提出可能的医学诊断。在我带的许多团队中，我们构建的很多项目如果没有 agentic 工作流是根本不可能完成的。因此，知道如何用它们来构建应用是当今 AI 领域最重要和最有价值的技能之一。\n","description":"欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 模块1：Agentic AI简介「Andrew Ng：Agentic AI」 1.0 简介 欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使用 Agentic AI 构建的真正有价值和有用的应用数量也增长得非常迅速，即使没有炒作那么快。在本课程中，我想向您展示构建 Agentic AI 应用的最佳实践。这将在您现在可以构建什么方面，为您开启许多新的机会。\n如今，agentic 工作流正被用于构建客户支持代理等应用，或进行深度研究以帮助撰写富有洞察力的研究报告，或处理棘手的法律文件，或查看患者输入信息并提出可能的医学诊断。在我带的许多团队中，我们构建的很多项目如果没有 agentic 工作流是根本不可能完成的。因此，知道如何用它们来构建应用是当今 AI 领域最重要和最有价值的技能之一。\n我发现，真正懂得如何构建 agentic 工作流的人与那些效率较低的人之间，最大的区别之一是能否推动一个规范的开发流程，特别是专注于评估和错误分析的流程。在本课程中，我将告诉您这意味着什么，并向您展示如何才能真正擅长构建这些 agentic 工作流。能够做到这一点是当今 AI 领域最重要的技能之一，它将为您开启更多的机会，无论是工作机会，还是亲手打造出色软件的机会。\n那么，让我们进入下一个视频，更深入地探讨什么是 agentic 工作流。\n1.1 什么是 Agentic AI 那么，什么是 Agentic AI？为什么 Agentic AI 工作流如此强大？让我们来看一看。\n如今，我们许多人使用大型语言模型（LLM）的方式是提示它，比如说，为我们写一篇关于某个主题 X 的文章。我认为这类似于去找一个人，或者在这种情况下，去找一个 AI，请它为我打出一篇文章，要求它从第一个词写到最后一个词，一气呵成，并且永远不能使用退格键。事实证明，我们人类并不能通过这种被迫以完全线性顺序写作的方式来完成我们最好的作品，AI 模型也是如此。但尽管受到这种写作方式的限制，我们的大语言模型表现得出奇地好。\n相比之下，使用 agentic 工作流，过程可能是这样的：你可能会让它首先写一个关于某个主题的文章大纲，然后问它是否需要进行任何网络研究。在进行了一些网络研究并可能下载了一些网页之后，再让它撰写初稿，然后阅读初稿，看看哪些部分需要修改或做更多研究，接着修改草稿，如此循环。这种工作流更类似于先进行一些思考和研究，然后进行一些修改，再进行更多的思考，等等。通过这种迭代过程，事实证明，agentic 工作流可能需要更长的时间，但它能交付出质量好得多的工作成果。\n所以，一个 Agentic AI 工作流是一个基于 LLM 的应用执行多个步骤来完成一项任务的过程。在这个例子中，你可能会使用一个 LLM 来撰写文章大纲，然后你可能会使用一个 LLM 来决定在网络搜索引擎中输入什么搜索词，或者说，用什么搜索词来调用网络搜索 API，以获取相关的网页。基于此，你可以将下载的网页输入到一个 LLM 中，让它撰写初稿，然后可能使用另一个 LLM 进行反思，并决定哪些地方需要更多修改。根据你设计这个工作流的方式，也许你甚至可以加入一个“人类在环”的步骤，让 LLM 可以选择请求人类审查某些关键事实。在此基础上，它可能会修改草稿，这个过程会产生一个好得多的工作输出。\n你在本课程中学到的关键技能之一，就是如何将像写文章这样的复杂任务分解成更小的步骤，让 agentic 工作流一次执行一步，从而获得你想要的工作输出。事实证明，知道如何将任务分解为步骤，以及如何构建组件来很好地执行单个步骤，是一项棘手但重要的技能，它将决定你为各种激动人心的应用构建 agentic 工作流的能力。\n在本课程中，我们将使用一个贯穿始终的例子，也是你将与我一起构建的东西——一个研究代理（research agent）。下面是它看起来的样子。你可以输入一个研究主题，比如“我如何建立一个新的火箭公司来与 SpaceX 竞争”。我个人不想与 SpaceX 竞争，但如果你想，你可以试着让研究代理帮助你做背景研究。\n这个代理首先会规划要使用哪些研究方法，包括调用网络搜索引擎下载一些网页，然后综合和排序研究发现， …"},{"title":"「吴恩达 Agentic AI 课程实录」模块2：反思设计模式","url":"/courses/andrew-ng-agentic-ai/lecnotes-zh/lec-02/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ 模块2：反思设计模式「Andrew Ng：Agentic AI」 2.1 通过反思改进任务输出 反思设计模式是我在许多应用中使用过的一种方法，而且它的实现惊人地简单。让我们来看一看。\n正如人类有时会反思自己的成果并找到改进的方法一样，大型语言模型（LLM）也可以。例如，我可能会写这样一封电子邮件，如果我打字很快，我可能会写出一份不太好的初稿。如果我通读一遍，我可能会说：“嗯，‘下个月’这个说法对于 Tommy 什么时候有空吃晚饭来说不够清楚”，而且我还有一个拼写错误，也忘了署名。这会让我修改草稿，说得更具体一些：“嘿，Tommy，你 5 号到 7 号有空吃晚饭吗？”\n类似的过程也能让 LLM 改进它们的输出。你可以提示一个 LLM 写出邮件的初稿，得到邮件版本1（email v1）后，你可以把它传递给也许是同一个模型，同一个大型语言模型，但使用不同的提示，告诉它进行反思并写一个改进的第二稿，从而得到最终的输出，即邮件版本2（email v2）。在这里，我只是硬编码了这个工作流：提示 LLM 一次，然后再次提示它进行反思和改进，从而得到邮件 v2。\n事实证明，类似的过程可以用来改进其他类型的输出。例如，如果你让一个 LLM 编写代码，你可能会提示它为某个任务编写代码，它可能会给你代码的 v1 版本。然后，你可以将它传递给同一个或另一个 LLM，要求它检查错误并编写一个改进的代码第二稿。不同的 LLM 有不同的优势，所以我有时会为撰写初稿和进行反思改进选择不同的模型。例如，事实证明，推理模型（有时也称为思考模型）非常擅长发现错误，所以我有时会通过直接生成来编写代码的初稿，但随后使用一个推理模型来检查错误。\n现在，事实证明，如果你能获得外部反馈，即来自 LLM 外部的新信息，而不仅仅是让 LLM 反思代码，那么反思会变得更加强大。在代码的例子中，你可以做的一件事就是直接执行代码，看看代码做了什么。通过检查输出，包括代码的任何错误信息，这对于 LLM 反思并找到改进其代码的方法来说，是极其有用的信息。\n所以在这个例子中，LLM 生成了代码的初稿，但当我运行它时，它产生了一个语法错误。当你将这个代码输出和错误日志传回给 LLM，并要求它根据反馈进行反思并撰写新稿时，这给了它大量非常有用的信息，从而能够得出一个好得多的代码版本2。\n所以，反思设计模式并非魔术。它不会让 LLM 每次都百分之百地做对所有事情，但它通常能给性能带来适度的提升。但需要记住的一个设计考虑是，当有新的、额外的外部信息可以被纳入反思过程时，反思会变得更加强大。所以在那个例子中，如果你能运行代码，并将代码输出或错误信息作为反思步骤的额外输入，那确实能让 LLM 更深入地反思，并找出可能出了什么问题（如果有的话），从而产生一个比没有这些可供吸收的外部信息时好得多的代码第二版。所以要记住一点，每当反思有机会获得额外信息时，它都会变得更加强大。\n现在，让我们进入下一个视频，我想与你分享一个更系统的比较，关于使用反思与直接生成（我们有时称之为零样本提示）的对比。让我们进入下一个视频。\n2.2 为什么不直接生成？ 让我们来看一看，为什么我们可能更倾向于使用反思工作流，而不是仅仅提示一次 LLM，让它直接生成答案然后就此结束。\n","description":"反思设计模式是我在许多应用中使用过的一种方法，而且它的实现惊人地简单。让我们来看一看。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ 模块2：反思设计模式「Andrew Ng：Agentic AI」 2.1 通过反思改进任务输出 反思设计模式是我在许多应用中使用过的一种方法，而且它的实现惊人地简单。让我们来看一看。\n正如人类有时会反思自己的成果并找到改进的方法一样，大型语言模型（LLM）也可以。例如，我可能会写这样一封电子邮件，如果我打字很快，我可能会写出一份不太好的初稿。如果我通读一遍，我可能会说：“嗯，‘下个月’这个说法对于 Tommy 什么时候有空吃晚饭来说不够清楚”，而且我还有一个拼写错误，也忘了署名。这会让我修改草稿，说得更具体一些：“嘿，Tommy，你 5 号到 7 号有空吃晚饭吗？”\n类似的过程也能让 LLM 改进它们的输出。你可以提示一个 LLM 写出邮件的初稿，得到邮件版本1（email v1）后，你可以把它传递给也许是同一个模型，同一个大型语言模型，但使用不同的提示，告诉它进行反思并写一个改进的第二稿，从而得到最终的输出，即邮件版本2（email v2）。在这里，我只是硬编码了这个工作流：提示 LLM 一次，然后再次提示它进行反思和改进，从而得到邮件 v2。\n事实证明，类似的过程可以用来改进其他类型的输出。例如，如果你让一个 LLM 编写代码，你可能会提示它为某个任务编写代码，它可能会给你代码的 v1 版本。然后，你可以将它传递给同一个或另一个 LLM，要求它检查错误并编写一个改进的代码第二稿。不同的 LLM 有不同的优势，所以我有时会为撰写初稿和进行反思改进选择不同的模型。例如，事实证明，推理模型（有时也称为思考模型）非常擅长发现错误，所以我有时会通过直接生成来编写代码的初稿，但随后使用一个推理模型来检查错误。\n现在，事实证明，如果你能获得外部反馈，即来自 LLM 外部的新信息，而不仅仅是让 LLM 反思代码，那么反思会变得更加强大。在代码的例子中，你可以做的一件事就是直接执行代码，看看代码做了什么。通过检查输出，包括代码的任何错误信息，这对于 LLM 反思并找到改进其代码的方法来说，是极其有用的信息。\n所以在这个例子中，LLM 生成了代码的初稿，但当我运行它时，它产生了一个语法错误。当你将这个代码输出和错误日志传回给 LLM，并要求它根据反馈进行反思并撰写新稿时，这给了它大量非常有用的信息，从而能够得出一个好得多的代码版本2。\n所以，反思设计模式并非魔术。它不会让 LLM 每次都百分之百地做对所有事情，但它通常能给性能带来适度的提升。但需要记住的一个设计考虑是，当有新的、额外的外部信息可以被纳入反思过程时，反思会变得更加强大。所以在那个例子中，如果你能运行代码，并将代码输出或错误信息作为反思步骤的额外输入，那确实能让 LLM 更深入地反思，并找出可能出了什么问题（如果有的话），从而产生一个比没有这些可供吸收的外部信息时好得多的代码第二版。所以要记住一点，每当反思有机会获得额外信息时，它都会变得更加强大。\n现在，让我们进入下一个视频，我想与你分享一个更系统的比较，关于使用反思与直接生成（我们有时称之为零样本提示）的对比。让我们进入下一个视频。\n2.2 为什么不直接生成？ 让我们来看一看，为什么我们可能更倾向于使用反思工作流，而不是仅仅提示一次 LLM，让它直接生成答案然后就此结束。\n使用直接生成时，你只需用一条指令来提示 LLM，然后让它生成一个答案。所以你可以要求一个 LLM 写一篇关于黑洞的文章，然后让它直接生成文本；或者让它编写计算复利的 Python 函数，然后让它直接写出代码。你在这里看到的提示示例也被称为零样本提示（zero-shot prompting）。\n让我解释一下“零样本”是什么意思。与零样本提示相对的一种相关方法是，在你的提示中包含一个或多个你希望输出看起来像的例子。这被称为单样本提示（one-shot prompting），如果你在提示中包含了一个期望的输入输出对的例子；或者双样本或少样本提示（two-shot or few-shot prompting），取决于你在提示中包含了多少这样的例子。因此，零样本提示指的是如果你包含了零个例子，也就是不包含任何你想要的期望输出的例子。但如果你还不熟悉这些术语，也别担心。重要的是，在你看到的这些例子中，你只是提示 LLM 一次性直接生成答案，我也称之为零样本提示，因为我们包含了零个例子。\n事实证明，多项研究表明，在各种任务上，反思都能提升直接生成的性能。这张图改编自 Madaan 等人的研究论文，它展示了一系列不同的任务，在有和没有反思的情况下，使用不同模型实现的结果。解读这张图的方法是看这些相邻的浅色条和深色条对，其中浅色条显示的是零样本提示，深色条显示的是同一模型但带 …"},{"title":"「吴恩达 Agentic AI 课程实录」模块3：工具使用","url":"/courses/andrew-ng-agentic-ai/lecnotes-zh/lec-03/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 模块3：工具使用「Andrew Ng：Agentic AI」 3.1 什么是工具？ 在本模块中，你将学习大型语言模型（LLM）的工具使用，这意味着让你的 LLM 决定何时可能需要请求调用一个函数来执行某些操作、收集一些信息或做其他事情。就像我们人类使用工具能比徒手做更多的事情一样，LLM 在获得工具后也能做更多的事情。不过，我们给 LLM 的工具不是锤子、扳手和钳子，而是函数，让它能够请求调用，从而完成更多任务。让我们来看一看。\n如果你问一个可能在数月前训练好的 LLM：“现在几点了？” 那个训练好的模型并不知道确切的时间，所以它很可能会回答：“抱歉，我无法获取当前时间。” 但是，如果你编写一个函数并让 LLM 能够访问这个函数，那么它就能给出一个更有用的答案。\n当我们让 LLM 调用函数，或者更准确地说，让 LLM 请求调用函数时，这就是我们所说的“工具使用”，而工具就是我们提供给 LLM、可供其请求调用的函数。\n具体来说，工具使用是这样工作的。在这个例子中，我将把上一张幻灯片中展示的 getCurrentTime 函数提供给 LLM。当你接着提示它“现在几点了？”时，LLM 可以决定调用 getCurrentTime 函数。该函数将返回当前时间，这个时间随后会作为对话历史的一部分反馈给 LLM，最后 LLM 就可以输出，比如说，“现在是下午3点20分”。\n","description":"在本模块中，你将学习大型语言模型（LLM）的工具使用，这意味着让你的 LLM 决定何时可能需要请求调用一个函数来执行某些操作、收集一些信息或做其他事情。就像我们人类使用工具能比徒手做更多的事情一样，LLM 在获得工具后也能做更多的事情。不过，我们给 LLM 的工具不是锤子、扳手和钳子，而是函数，让它能够请求调用，从而完","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 模块3：工具使用「Andrew Ng：Agentic AI」 3.1 什么是工具？ 在本模块中，你将学习大型语言模型（LLM）的工具使用，这意味着让你的 LLM 决定何时可能需要请求调用一个函数来执行某些操作、收集一些信息或做其他事情。就像我们人类使用工具能比徒手做更多的事情一样，LLM 在获得工具后也能做更多的事情。不过，我们给 LLM 的工具不是锤子、扳手和钳子，而是函数，让它能够请求调用，从而完成更多任务。让我们来看一看。\n如果你问一个可能在数月前训练好的 LLM：“现在几点了？” 那个训练好的模型并不知道确切的时间，所以它很可能会回答：“抱歉，我无法获取当前时间。” 但是，如果你编写一个函数并让 LLM 能够访问这个函数，那么它就能给出一个更有用的答案。\n当我们让 LLM 调用函数，或者更准确地说，让 LLM 请求调用函数时，这就是我们所说的“工具使用”，而工具就是我们提供给 LLM、可供其请求调用的函数。\n具体来说，工具使用是这样工作的。在这个例子中，我将把上一张幻灯片中展示的 getCurrentTime 函数提供给 LLM。当你接着提示它“现在几点了？”时，LLM 可以决定调用 getCurrentTime 函数。该函数将返回当前时间，这个时间随后会作为对话历史的一部分反馈给 LLM，最后 LLM 就可以输出，比如说，“现在是下午3点20分”。\n所以，步骤顺序是：首先有输入提示。在这种情况下，LLM 查看可用的工具集（本例中只有一个工具），并决定调用该工具。这个工具是一个函数，它会返回一个值，该值被反馈给 LLM，然后 LLM 最终生成它的输出。\n现在，工具使用的一个重要方面是，我们可以让 LLM 自行决定是否使用任何工具。所以在同样的设置下，如果我问它：“绿茶里有多少咖啡因？” LLM 不需要知道当前时间来回答这个问题，所以它可以直接生成答案：“绿茶通常含有这么多咖啡因”，并且它在这样做的时候没有调用 getCurrentTime 函数。\n在我的幻灯片中，我将使用 LLM 上方带有虚线框的这个标记，来表示我们正在向 LLM 提供一组工具，供其在认为合适时选择使用。这与你在之前视频中看到的一些例子相反，在那些例子中，我作为开发者硬编码了，例如，在研究代理的某个特定点上总是进行网络搜索。相比之下，getCurrentTime 函数的调用并不是硬编码的，而是由 LLM 自行决定是否要请求调用 getCurrentTime 函数。再次强调，我们将使用这个虚线框标记来表示我们何时向 LLM 提供一个或多个工具，由 LLM 来决定它想调用哪些工具（如果有的话）。\n这里还有一些工具使用可能帮助基于 LLM 的应用生成更好答案的例子：\n网络搜索：如果你问它：“你能找一些在加州山景城附近的意大利餐厅吗？” 如果它有一个网络搜索工具，那么 LLM 可能会选择调用一个网络搜索引擎来查询“加州山景城附近的餐厅”，并使用获取到的结果来生成输出。 数据库查询：如果你经营一家零售店，并且希望能够回答像“给我看看购买了白色太阳镜的顾客”这样的问题，如果你的 LLM 被赋予了访问查询数据库工具的权限，那么它可能会在销售表中查找哪些条目是销售了一副白色太阳镜的，然后用这个信息来生成输出。 计算：最后，如果你想进行利率计算：“如果我存入500美元，利率为5%，10年后我会得到多少钱？” 如果你恰好有一个利率计算工具，那么它就可以调用利率计算函数来计算出结果。或者，事实证明，你稍后会看到的一种方法是，让 LLM 编写代码，比如写一个这样的数学表达式，然后对它求值，这将是另一种让 LLM 计算出正确答案的方式。 因此，作为开发者，你需要思考你希望应用真正做什么样的事情，然后创建所需的函数或工具，并将它们提供给 LLM，让它能够使用适当的工具来完成任务，比如餐厅推荐器、零售问答系统或金融助手可能需要做的事情。所以，根据你的应用，你可能需要实现并向你的 LLM 提供不同的工具。\n到目前为止，我们看过的大多数例子都只向 LLM 提供了一个工具或一个函数。但在许多用例中，你会希望向 LLM 提供多个工具或多个函数，供其选择调用哪一个（或不调用）。例如，如果你正在构建一个日历助手代理，你可能希望它能够满足这样的请求：“请在我的日历上找一个周四的空闲时段，并与 Alice 安排一个约会。”\n在这个例子中，我们可能会向 LLM 提供一个用于“安排约会”（即发送日历邀请）的工具或函数，一个用于“检查日历”以查看我何时有 …"},{"title":"「吴恩达 Agentic AI 课程实录」模块4：构建 Agentic AI 的实用技巧","url":"/courses/andrew-ng-agentic-ai/lecnotes-zh/lec-04/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 模块4：构建 Agentic AI 的实用技巧「Andrew Ng：Agentic AI」 4.1 评估（Evals） 在本模块中，我想与你分享一些构建 Agentic AI 工作流的实用技巧。我希望这些技巧能让你在构建这类系统时，比普通开发者效率高得多。\n我发现，在开发一个 Agentic AI 系统时，很难预先知道它在哪里能行得通，又在哪里效果不佳，因此也很难知道应该将精力集中在哪里。所以一个非常普遍的建议是，先尝试构建一个哪怕是粗糙的系统，这样你就可以试用它，观察它，看看在哪些地方它可能还没有达到你期望的效果，从而能够更有针对性地进行进一步的开发。相比之下，我发现花好几周时间坐着理论化和假设如何构建它，有时效果反而不佳。通常更好的做法是，以一种安全、合理、不泄露数据的方式，负责任地快速构建一些东西，这样你就可以观察它，然后利用这个初始原型来确定优先级并尝试进一步的开发。\n让我们从一个例子开始，看看在你构建了一个原型之后可能会发生什么。\n示例一：发票处理 我想用我们之前见过的发票处理工作流作为第一个例子，其任务是提取四个必需的字段，然后将它们保存到数据库记录中。在构建了这样一个系统之后，你可能会做的一件事是，找几张发票，也许是10或20张，然后过一遍，看看它们的输出，看看哪些处理得好，是否有任何错误。\n假设你检查了20张发票，你发现第一张发票没问题，输出看起来是正确的。对于第二张发票，它可能混淆了发票日期（即发票开具的日期）和发票的到期日。在这个任务中，我们想要提取的是到期日，这样我们才能按时付款。于是，我可能会在一个文档或电子表格中记下，对于第二张发票，日期搞混了。也许第三、第四张发票都没问题，依此类推。但当我过完这些例子后，我发现有很多例子都混淆了日期。\n正是基于对这样一些例子的检查，在这种情况下，你可能会得出结论：一个常见的错误模式是，它在处理日期方面有困难。那样的话，你可能会考虑的一件事，当然是找出如何改进你的系统，让它能更好地提取到期日，但同时，也许也可以编写一个评估（eval）来衡量它提取到期日的准确性。相比之下，如果你发现它错误地提取了开票方地址——谁知道呢，也许你的开票方有不寻常的名字，所以它可能在处理开票方上遇到困难，特别是如果你有国际开票方，他们的名字甚至可能不全是英文字母——那么你可能会转而专注于为开票方地址构建一个评估。\n所以，为什么构建一个粗糙的系统并查看其输出如此有帮助，原因之一是，它甚至能帮助你决定，你最想投入精力去评估的是什么。\n现在，如果你已经决定要修改你的系统，以提高它提取发票到期日的准确性，那么为了跟踪进展，创建一个评估来衡量日期提取的准确性可能是个好主意。实现这一点可能有多种方法，但我来分享一下我可能会怎么做。\n为了创建一个测试集或评估集，我可能会找10到20张发票，并手动写下它们的到期日。所以，也许一张发票的到期日是2025年8月20日，我把它写成标准的“年-月-日”格式。然后，为了便于稍后在代码中进行评估，我可能会在给 LLM 的提示中告诉它，总是将到期日格式化为这种“年-月-日”的格式。这样，我就可以编写代码来提取 LLM 输出的那个日期，也就是到期日，因为那是我们关心的唯一日期。这是一个正则表达式，用于模式匹配，你知道的，四位数的年份，两位数的月份，两位数的日期，然后把它提取出来。接着我就可以直接编写代码来测试提取出的日期是否等于实际日期，也就是我写下的标准答案。\n","description":"在本模块中，我想与你分享一些构建 Agentic AI 工作流的实用技巧。我希望这些技巧能让你在构建这类系统时，比普通开发者效率高得多。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 模块4：构建 Agentic AI 的实用技巧「Andrew Ng：Agentic AI」 4.1 评估（Evals） 在本模块中，我想与你分享一些构建 Agentic AI 工作流的实用技巧。我希望这些技巧能让你在构建这类系统时，比普通开发者效率高得多。\n我发现，在开发一个 Agentic AI 系统时，很难预先知道它在哪里能行得通，又在哪里效果不佳，因此也很难知道应该将精力集中在哪里。所以一个非常普遍的建议是，先尝试构建一个哪怕是粗糙的系统，这样你就可以试用它，观察它，看看在哪些地方它可能还没有达到你期望的效果，从而能够更有针对性地进行进一步的开发。相比之下，我发现花好几周时间坐着理论化和假设如何构建它，有时效果反而不佳。通常更好的做法是，以一种安全、合理、不泄露数据的方式，负责任地快速构建一些东西，这样你就可以观察它，然后利用这个初始原型来确定优先级并尝试进一步的开发。\n让我们从一个例子开始，看看在你构建了一个原型之后可能会发生什么。\n示例一：发票处理 我想用我们之前见过的发票处理工作流作为第一个例子，其任务是提取四个必需的字段，然后将它们保存到数据库记录中。在构建了这样一个系统之后，你可能会做的一件事是，找几张发票，也许是10或20张，然后过一遍，看看它们的输出，看看哪些处理得好，是否有任何错误。\n假设你检查了20张发票，你发现第一张发票没问题，输出看起来是正确的。对于第二张发票，它可能混淆了发票日期（即发票开具的日期）和发票的到期日。在这个任务中，我们想要提取的是到期日，这样我们才能按时付款。于是，我可能会在一个文档或电子表格中记下，对于第二张发票，日期搞混了。也许第三、第四张发票都没问题，依此类推。但当我过完这些例子后，我发现有很多例子都混淆了日期。\n正是基于对这样一些例子的检查，在这种情况下，你可能会得出结论：一个常见的错误模式是，它在处理日期方面有困难。那样的话，你可能会考虑的一件事，当然是找出如何改进你的系统，让它能更好地提取到期日，但同时，也许也可以编写一个评估（eval）来衡量它提取到期日的准确性。相比之下，如果你发现它错误地提取了开票方地址——谁知道呢，也许你的开票方有不寻常的名字，所以它可能在处理开票方上遇到困难，特别是如果你有国际开票方，他们的名字甚至可能不全是英文字母——那么你可能会转而专注于为开票方地址构建一个评估。\n所以，为什么构建一个粗糙的系统并查看其输出如此有帮助，原因之一是，它甚至能帮助你决定，你最想投入精力去评估的是什么。\n现在，如果你已经决定要修改你的系统，以提高它提取发票到期日的准确性，那么为了跟踪进展，创建一个评估来衡量日期提取的准确性可能是个好主意。实现这一点可能有多种方法，但我来分享一下我可能会怎么做。\n为了创建一个测试集或评估集，我可能会找10到20张发票，并手动写下它们的到期日。所以，也许一张发票的到期日是2025年8月20日，我把它写成标准的“年-月-日”格式。然后，为了便于稍后在代码中进行评估，我可能会在给 LLM 的提示中告诉它，总是将到期日格式化为这种“年-月-日”的格式。这样，我就可以编写代码来提取 LLM 输出的那个日期，也就是到期日，因为那是我们关心的唯一日期。这是一个正则表达式，用于模式匹配，你知道的，四位数的年份，两位数的月份，两位数的日期，然后把它提取出来。接着我就可以直接编写代码来测试提取出的日期是否等于实际日期，也就是我写下的标准答案。\n所以，有了一个包含大约20张发票的评估集，我就可以进行构建和修改，看看随着我调整提示或系统的其他部分，它正确提取日期的百分比是否在希望中上升。\n总结一下我们到目前为止看到的内容：我们构建一个系统，然后查看输出以发现它可能表现不佳的地方，比如到期日错误。然后，为了推动对这个重要输出的改进，我们建立一个小型的评估，比如说只有20个例子，来帮助我们跟踪进展。这让我可以回头去调整提示，尝试不同的算法等等，看看我是否能提升“到期日准确性”这个指标。这就是改进一个 Agentic AI 工作流通常的感觉：查看输出，看哪里错了，如果你知道怎么修复，就直接修复它。但如果你需要一个更长的改进过程，那就建立一个评估，并用它来推动进一步的开发。\n另外需要考虑的一件事是，如果工作了一段时间后，你认为最初的那20个例子不够好，也许它们没有覆盖你想要的所有情况，或者20个例子实在太少，那么你可以随时向评估集中添加更多的例子，以确保它能更好地反映你个人对于系统性能是否足够满意的判断。\n示例二：营 …"},{"title":"「吴恩达 Agentic AI 课程实录」模块5：高度自主代理的设计模式","url":"/courses/andrew-ng-agentic-ai/lecnotes-zh/lec-05/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 模块5：高度自主代理的设计模式「Andrew Ng：Agentic AI」 5.1 规划工作流 欢迎来到最后一个模块，在这里你将学习一些设计模式，让你能够构建高度自主的代理。在使用这些模式时，你无需预先硬编码要采取的步骤顺序，代理可以更灵活地自行决定要采取哪些步骤来完成任务。我们将讨论规划设计模式，以及在本模块后面部分，如何构建多代理系统。让我们开始吧。\n示例一：零售客户服务 假设你经营一家太阳镜零售店，并且你的库存中有哪些太阳镜的信息都存储在数据库中。你可能希望有一个客户服务代理能够回答像“你们有库存的圆形太阳镜吗？价格在100美元以下”这样的问题。这是一个相当复杂的查询，因为你必须查看产品描述，看哪些太阳镜是圆形的，然后查看哪些有库存，最后再看哪些价格低于100美元，才能告诉顾客：“是的，我们有经典款太阳镜。” 你如何构建一个能回答像这样以及许多其他各种客户查询的代理呢？\n为了做到这一点，我们将给 LLM 一套工具，让它能够：\n获取商品描述（比如查找不同的眼镜是否是圆形的） 检查库存 处理商品退货（这个查询不需要，但其他查询可能需要） 获取商品价格 检查过去的交易记录 处理商品销售等等。 为了让 LLM 弄清楚响应客户请求应该使用什么正确的工具顺序，你可能会写一个这样的提示：“你可以使用以下工具\u0026hellip;”，然后给它每个工具（比如说六个或更多工具）的描述，接着告诉它“返回一个执行用户请求的逐步计划”。\n在这种情况下，为了回答这个特定的查询，一个 LLM 可能输出的合理计划可能是：\n首先，使用 get_item_descriptions 检查不同的描述以找到圆形太阳镜。 然后，使用 check_inventory 查看它们是否有库存。 再使用 get_item_price 查看有库存的结果是否低于100美元。 在 LLM 输出了这个包含三个步骤的计划之后，我们可以将第一步的文本（即这里用红色写的文本）传递给一个 LLM，可能还会附加上下文，比如有哪些工具、你的用户查询是什么、以及其他背景信息，然后让 LLM 执行第一步。在这种情况下，希望 LLM 会选择调用 get_item_descriptions 来获取相应的商品描述，该步骤的输出能让它选出哪些是圆形太阳镜。\n","description":"欢迎来到最后一个模块，在这里你将学习一些设计模式，让你能够构建高度自主的代理。在使用这些模式时，你无需预先硬编码要采取的步骤顺序，代理可以更灵活地自行决定要采取哪些步骤来完成任务。我们将讨论规划设计模式，以及在本模块后面部分，如何构建多代理系统。让我们开始吧。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 模块5：高度自主代理的设计模式「Andrew Ng：Agentic AI」 5.1 规划工作流 欢迎来到最后一个模块，在这里你将学习一些设计模式，让你能够构建高度自主的代理。在使用这些模式时，你无需预先硬编码要采取的步骤顺序，代理可以更灵活地自行决定要采取哪些步骤来完成任务。我们将讨论规划设计模式，以及在本模块后面部分，如何构建多代理系统。让我们开始吧。\n示例一：零售客户服务 假设你经营一家太阳镜零售店，并且你的库存中有哪些太阳镜的信息都存储在数据库中。你可能希望有一个客户服务代理能够回答像“你们有库存的圆形太阳镜吗？价格在100美元以下”这样的问题。这是一个相当复杂的查询，因为你必须查看产品描述，看哪些太阳镜是圆形的，然后查看哪些有库存，最后再看哪些价格低于100美元，才能告诉顾客：“是的，我们有经典款太阳镜。” 你如何构建一个能回答像这样以及许多其他各种客户查询的代理呢？\n为了做到这一点，我们将给 LLM 一套工具，让它能够：\n获取商品描述（比如查找不同的眼镜是否是圆形的） 检查库存 处理商品退货（这个查询不需要，但其他查询可能需要） 获取商品价格 检查过去的交易记录 处理商品销售等等。 为了让 LLM 弄清楚响应客户请求应该使用什么正确的工具顺序，你可能会写一个这样的提示：“你可以使用以下工具\u0026hellip;”，然后给它每个工具（比如说六个或更多工具）的描述，接着告诉它“返回一个执行用户请求的逐步计划”。\n在这种情况下，为了回答这个特定的查询，一个 LLM 可能输出的合理计划可能是：\n首先，使用 get_item_descriptions 检查不同的描述以找到圆形太阳镜。 然后，使用 check_inventory 查看它们是否有库存。 再使用 get_item_price 查看有库存的结果是否低于100美元。 在 LLM 输出了这个包含三个步骤的计划之后，我们可以将第一步的文本（即这里用红色写的文本）传递给一个 LLM，可能还会附加上下文，比如有哪些工具、你的用户查询是什么、以及其他背景信息，然后让 LLM 执行第一步。在这种情况下，希望 LLM 会选择调用 get_item_descriptions 来获取相应的商品描述，该步骤的输出能让它选出哪些是圆形太阳镜。\n然后，第一步的输出会连同第二步的指令（即我这里用蓝色标出的指令）一起传递给一个 LLM，以执行计划的第二步。希望它会接着处理我们在上一张幻灯片中找到的两副圆形太阳镜并检查库存。\n第二步的输出随后会用于另一次 LLM 调用，其中包含了第二步的输出以及第三步要做什么的指令。将这些传递给 LLM，让它获取商品价格，最后这个输出会最后一次反馈给 LLM，以生成给用户的最终答案。\n在这张幻灯片中，我稍微简化了很多细节。LLM 实际编写的计划通常比这些简单的一行指令更详细，但基本的工作流程是，让一个 LLM 写出一个包含多个步骤的计划，然后让它在适当的上下文（比如任务是什么、有哪些可用工具等）中，依次执行计划的每一步。\n使用 LLM 以这种方式进行规划的激动人心之处在于，我们不必预先决定调用工具的顺序，就能回答一个相当复杂的客户请求。如果客户提出一个不同的请求，比如“我想退回我购买的金色镜框眼镜，而不是金属镜框的”，那么你可以想象一个 LLM 同样能够想出一个不同的计划，根据他们之前购买的记录，通过 get_item_descriptions 找出他们买了哪些眼镜，哪些是他们想退回的金色镜框眼镜，然后可能调用 process_item_return。所以，有了一个能像这样进行规划的代理，它就能执行更广泛的任务，这些任务可能需要以许多不同的顺序调用许多不同的工具。\n示例二：邮件助手 再看一个规划的例子，让我们来看一个邮件助手。如果你想告诉你的助手：“请回复纽约的 Bob 发来的那封邮件邀请，告诉他我会参加，并把他的邮件归档。” 那么，一个邮件助手可能会被赋予像这样的工具：搜索邮件、移动邮件、删除邮件和发送邮件。你可能会写一个助手提示，说：“你可以使用以下工具，\u0026hellip;请返回逐步的计划。” 在这种情况下，LLM 可能会说，完成这个任务的步骤是：\n使用 search_email 找到 Bob 发来的那封提到“晚餐”和“纽约”的邮件。 然后生成并发送一封邮件以确认参加。 最后将那封邮件移动到归档文件夹。 鉴于这个计划看起来是合理的，你接下来会再次让一个 LLM 按部就班地执行这个计划。所以，第一步的文本（这里用红色显示） …"},{"title":"「吴恩达Agentic AI 模块1」4个颠覆性观点","url":"/courses/andrew-ng-agentic-ai/blog/lec-01/","section":"courses","date":"2025-10-17","summary":" 吴恩达在他的新课《Agentic AI》中指出，应当让AI模仿人类的思考和工作流程：先构思，再研究，然后起草，最后反复修改。这就是“Agentic工作流”的核心思想，它正在开启一个全新的AI应用时代。\n","description":"你是否曾有过这样的经历：为了完成一份报告，你给大语言模型（LLM）一个提示词，比如“帮我写一篇关于黑洞的论文”，然后得到了一篇看起来还不错，但深度和洞察力都略显平庸的文章？这是一种非常普遍的AI使用方式，但它远未发挥出AI的全部潜力。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" 吴恩达在他的新课《Agentic AI》中指出，应当让AI模仿人类的思考和工作流程：先构思，再研究，然后起草，最后反复修改。这就是“Agentic工作流”的核心思想，它正在开启一个全新的AI应用时代。\nhttps://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 「吴恩达Agentic AI 模块1」4个颠覆性观点 引言：超越一次性问答 你是否曾有过这样的经历：为了完成一份报告，你给大语言模型（LLM）一个提示词，比如“帮我写一篇关于黑洞的论文”，然后得到了一篇看起来还不错，但深度和洞察力都略显平庸的文章？这是一种非常普遍的AI使用方式，但它远未发挥出AI的全部潜力。\n吴恩达（Andrew Ng）在他的新课《Agentic AI》中提出了一个绝佳的比喻：这种“一键生成”模式，就像强迫一个人（或AI）从第一个字写到最后一个字，中间不许停顿思考，甚至不许按删除键。我们都知道，人类不是这样工作的，AI也不应该如此。真正强大的方法，是让AI模仿人类的思考和工作流程：先构思，再研究，然后起草，最后反复修改。这就是“Agentic工作流”的核心思想，它正在开启一个全新的AI应用时代。\n观点一：AI不再是“一键生成”，而是学会了“思考、研究、再修改” 过去，我们与AI的互动模式主要是“单次提示-单次生成”。我们提出一个问题，AI给出一个答案。但Agentic工作流彻底改变了这一点，它将一个复杂的任务分解成一系列更小的、可执行的步骤，形成一个迭代循环。\n以上文提到的论文写作为例，一个Agentic工作流可能会这样执行：\n构思大纲：首先，让LLM生成一个论文的结构大纲。 网络研究：根据大纲，LLM决定需要进行哪些网络搜索，并调用搜索API获取相关资料。 起草初稿：整合研究资料，撰写第一版草稿。 审阅反思：LLM（或另一个专门的“审阅”AI）阅读初稿，判断哪些部分需要修改、补充或进一步研究。 修改完善：根据审阅意见，对草稿进行修改，直至完成。 这个过程不再是僵化的线性生成，而是更接近人类专家完成复杂任务时的真实状态：思考、行动、反思、再行动。吴恩达指出，虽然这个过程可能花费更长的时间，但它最终能产出“好得多（much better）”的工作成果。\n正如吴恩达所说：“事实证明，无论是我们人类，还是AI模型，都无法在被强制以这种完全线性的顺序写作时，拿出自己最好的作品。”\n从“一次性生成”到“多步流程”的转变意义重大。它意味着AI不再只是一个快速的答案生成器，而是一个能够执行复杂流程、具备初步“思考”和“工作”能力的伙伴。\n观点二：提升AI能力的最大杠杆，不是新模型，而是新方法 在AI领域，我们常常认为，获得更好性能的唯一途径就是等待下一个更强大的模型（比如从GPT-3.5升级到GPT-4）。但吴恩达课程中的一个数据颠覆了这个认知：实现一个Agentic工作流所带来的性能提升，甚至可能超过模型本身的代际飞跃。\n课程中引用了一个名为Human Eval的编码能力基准测试数据：\nGPT-3.5（非Agentic模式）：准确率为40%。 GPT-4（非Agentic模式）：准确率跃升至67%，提升巨大。 关键发现：在GPT-3.5上应用Agentic工作流（例如增加代码反思和修正步骤），其性能提升的幅度，甚至超过了从GPT-3.5直接升级到GPT-4所带来的性能提升。 这个结果的含义极为深刻：\n“从一代模型到下一代模型的提升是巨大的，但这种提升仍然比不上在上一代模型上实施一个Agentic工作流所带来的差异。”\n这对所有AI开发者和企业来说都是一个振奋人心的消息。它意味着我们不必总是依赖于最新、最昂贵的大模型。通过巧妙地设计工作流，我们就能用现有的工具实现顶尖的性能。这不仅极大地降低了高水平AI应用的开发门槛，更意味着未来的竞争优势将越来越多地来自于流程设计的智慧，而不仅仅是计算资源的规模。\n观点三：当下最实用的AI Agent，可能不是最“自主”的那个 媒体和科幻作品常常将AI Agent描绘成高度自主、接近人类的“数字生命”。这种想象固然引人入胜，但吴恩达提醒我们，在现实世界中，最有价值的应用往往出现在自主性的另一端。\n他提出了一个“自主性光谱”的概念，从“低自主性”到“高自主性”分布。\n低自主性系统：其工作流程的每一步都由开发者预先设定好。例如，一个处理发票的Agent，其流程可能是固定的：识别PDF -\u0026gt; 提取关键字段（付款方、金额、日期） -\u0026gt; 存入数据库。 高自主性系统：Agent能够根据目标自主决定采取哪些步骤。例如，一个高级客服Agent需 …"},{"title":"「吴恩达Agentic AI 模块1」智能体AI工作流学习指南","url":"/courses/andrew-ng-agentic-ai/guides/lec-01/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 「吴恩达Agentic AI 模块1」：智能体AI工作流学习指南 本指南旨在评估和深化对吴恩达（Andrew Ng）关于智能体AI工作流课程核心概念的理解。它包括一个测验、一份答案解析、一组论文问题和一个关键术语词汇表，所有内容均基于提供的课程材料。\n测验： 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n什么是智能体AI工作流（Agentic AI Workflow）？它与传统的单次提示方法有何不同？ 吴恩达为什么创造并推广“agentic”（智能体的）这个词，而不是简单地使用“agent”（智能体）？ 智能体系统中的“自主性”（autonomy）谱系是如何划分的？请描述其两端。 根据课程内容，使用智能体工作流相比非智能体方法，在性能提升方面有何显著优势？请引用编码基准测试的例子。 除了提升性能，智能体工作流还提供了哪两个主要好处？ 什么是“任务分解”（Task Decomposition）？为什么它在构建智能体工作流中至关重要？ 在构建智能体工作流时，开发人员可以使用哪些核心“构建模块”（building blocks）？ 请简要解释“反思”（Reflection）这一智能体设计模式及其工作原理。 什么是“工具使用”（Tool Use）设计模式？请举例说明。 评估（evals）在开发智能体工作流中扮演什么角色？请描述两种主要的评估方法。 答案解析 什么是智能体AI工作流（Agentic AI Workflow）？它与传统的单次提示方法有何不同？ 智能体AI工作流是一个基于LLM（大语言模型）的应用程序执行多个步骤来完成任务的过程。与传统的单次提示（直接生成最终结果）不同，智能体工作流是一个迭代过程，可能包括规划、研究、草拟和修订，从而产生更高质量的成果。 吴恩达为什么创造并推广“agentic”（智能体的）这个词，而不是简单地使用“agent”（智能体）？ 吴恩达创造“agentic”这个形容词是为了避免关于什么是“真正的智能体”的二元论争议。他认为系统可以在不同程度上表现出智能体特性，使用“agentic”可以承认这种程度上的差异，让社区专注于构建系统，而不是争论定义。 智能体系统中的“自主性”（autonomy）谱系是如何划分的？请描述其两端。 智能体系统的自主性谱系从“低自主性”到“高自主性”不等。低自主性系统通常遵循由程序员预先确定的、确定性的步骤序列；而高自主性系统则能自主做出许多决策，包括决定执行任务的步骤顺序，甚至可能创建新工具。 根据课程内容，使用智能体工作流相比非智能体方法，在性能提升方面有何显著优势？请引用编码基准测试的例子。 智能体工作流能够显著提升性能，其提升幅度甚至可能超过模型本身的代际升级。在Human Eval编码基准测试中，将GPT-3.5与智能体工作流结合使用，其性能提升幅度超过了从GPT-3.5升级到GPT-4所带来的性能提升。 除了提升性能，智能体工作流还提供了哪两个主要好处？ 除了性能提升，智能体工作流还提供了另外两个好处：并行化（parallelism）和模块化（modularity）。并行化允许系统同时执行多个任务（如并行下载多个网页），从而比人类更快地完成某些工作；模块化则允许开发者轻松地添加、更新或替换工作流中的组件（如更换搜索引擎或LLM模型）。 什么是“任务分解”（Task Decomposition）？为什么它在构建智能体工作流中至关重要？ 任务分解是将一个复杂的任务或流程分解成一系列离散、更小的步骤的过程。这在构建智能体工作流中至关重要，因为它能将一个宏大目标转化为可由LLM或软件工具执行的具体、可管理的操作序列，从而实现整个工作流。 在构建智能体工作流时，开发人员可以使用哪些核心“构建模块”（building blocks）？ 开发人员可以使用的核心构建模块包括AI模型（如大语言模型或多模态模型）和软件工具。软件工具涵盖了多种功能，例如用于信息检索的API（如网络搜索、数据库查询）、用于执行任务的代码执行工具，以及与其他生产力应用（如邮件、日历）交互的接口。 请简要解释“反思”（Reflection）这一智能体设计模式及其工作原理。 “反思”是一种设计模式，即让LLM检查其自身的输出，并根据反馈进行迭代改进。例如，一个LLM生成代码后，可以提示它（或另一个“批评家”LLM）检查代码的正确性并提出批评，然后根据这些批评生成一个修正后的、更好的版本。 什么是“工具使用”（Tool Use）设计模式？请举例说明。 “工具使用”是一种设计模式，即赋予LLM调用外部函数或API（即“工具”）的能力来完成任务。例如，当被问及一个需要实时信息的问题时，LLM可以调用一个网络搜索工具来查找最新数据；当需要精确计算时，它可以调用一个代码执行工具来编写并运行代码得出答案。 评估（evals）在开发智能体工作流中扮演什么角色？请描述两种主要的评估方法。 评估在开发智能体工作流中扮演着至关重要的角色，它用于衡量系统性能、发现问题并推动迭代改进。两种主要的评估方法是：针对客观标准的评估，可以通过编写代码来检查（如检查输出中是否提及竞争对手）；以及针对主观标准的评估，通常使用“LLM作为裁判”（LLM as a judge）的方法，让另一个LLM为输出质量打分。 论文问题 请针对以下问题进行深入思考和阐述，无需提供答案。\n","description":"本指南旨在评估和深化对吴恩达（Andrew Ng）关于智能体AI工作流课程核心概念的理解。它包括一个测验、一份答案解析、一组论文问题和一个关键术语词汇表，所有内容均基于提供的课程材料。","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 「吴恩达Agentic AI 模块1」：智能体AI工作流学习指南 本指南旨在评估和深化对吴恩达（Andrew Ng）关于智能体AI工作流课程核心概念的理解。它包括一个测验、一份答案解析、一组论文问题和一个关键术语词汇表，所有内容均基于提供的课程材料。\n测验： 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n什么是智能体AI工作流（Agentic AI Workflow）？它与传统的单次提示方法有何不同？ 吴恩达为什么创造并推广“agentic”（智能体的）这个词，而不是简单地使用“agent”（智能体）？ 智能体系统中的“自主性”（autonomy）谱系是如何划分的？请描述其两端。 根据课程内容，使用智能体工作流相比非智能体方法，在性能提升方面有何显著优势？请引用编码基准测试的例子。 除了提升性能，智能体工作流还提供了哪两个主要好处？ 什么是“任务分解”（Task Decomposition）？为什么它在构建智能体工作流中至关重要？ 在构建智能体工作流时，开发人员可以使用哪些核心“构建模块”（building blocks）？ 请简要解释“反思”（Reflection）这一智能体设计模式及其工作原理。 什么是“工具使用”（Tool Use）设计模式？请举例说明。 评估（evals）在开发智能体工作流中扮演什么角色？请描述两种主要的评估方法。 答案解析 什么是智能体AI工作流（Agentic AI Workflow）？它与传统的单次提示方法有何不同？ 智能体AI工作流是一个基于LLM（大语言模型）的应用程序执行多个步骤来完成任务的过程。与传统的单次提示（直接生成最终结果）不同，智能体工作流是一个迭代过程，可能包括规划、研究、草拟和修订，从而产生更高质量的成果。 吴恩达为什么创造并推广“agentic”（智能体的）这个词，而不是简单地使用“agent”（智能体）？ 吴恩达创造“agentic”这个形容词是为了避免关于什么是“真正的智能体”的二元论争议。他认为系统可以在不同程度上表现出智能体特性，使用“agentic”可以承认这种程度上的差异，让社区专注于构建系统，而不是争论定义。 智能体系统中的“自主性”（autonomy）谱系是如何划分的？请描述其两端。 智能体系统的自主性谱系从“低自主性”到“高自主性”不等。低自主性系统通常遵循由程序员预先确定的、确定性的步骤序列；而高自主性系统则能自主做出许多决策，包括决定执行任务的步骤顺序，甚至可能创建新工具。 根据课程内容，使用智能体工作流相比非智能体方法，在性能提升方面有何显著优势？请引用编码基准测试的例子。 智能体工作流能够显著提升性能，其提升幅度甚至可能超过模型本身的代际升级。在Human Eval编码基准测试中，将GPT-3.5与智能体工作流结合使用，其性能提升幅度超过了从GPT-3.5升级到GPT-4所带来的性能提升。 除了提升性能，智能体工作流还提供了哪两个主要好处？ 除了性能提升，智能体工作流还提供了另外两个好处：并行化（parallelism）和模块化（modularity）。并行化允许系统同时执行多个任务（如并行下载多个网页），从而比人类更快地完成某些工作；模块化则允许开发者轻松地添加、更新或替换工作流中的组件（如更换搜索引擎或LLM模型）。 什么是“任务分解”（Task Decomposition）？为什么它在构建智能体工作流中至关重要？ 任务分解是将一个复杂的任务或流程分解成一系列离散、更小的步骤的过程。这在构建智能体工作流中至关重要，因为它能将一个宏大目标转化为可由LLM或软件工具执行的具体、可管理的操作序列，从而实现整个工作流。 在构建智能体工作流时，开发人员可以使用哪些核心“构建模块”（building blocks）？ 开发人员可以使用的核心构建模块包括AI模型（如大语言模型或多模态模型）和软件工具。软件工具涵盖了多种功能，例如用于信息检索的API（如网络搜索、数据库查询）、用于执行任务的代码执行工具，以及与其他生产力应用（如邮件、日历）交互的接口。 请简要解释“反思”（Reflection）这一智能体设计模式及其工作原理。 “反思”是一种设计模式，即让LLM检查其自身的输出，并根据反馈进行迭代改进。例如，一个LLM生成代码后，可以提示它（或另一个“批评家”LLM）检查代码的正确性并提出批评，然后根据这些批评生成一个修正后的、更好的版本。 什么是“工具使用”（Tool Use）设计模式？请 …"},{"title":"「吴恩达Agentic AI 模块1简报」Agentic AI工作流","url":"/courses/andrew-ng-agentic-ai/insights/lec-01/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 「吴恩达Agentic AI 模块1简报」Agentic AI工作流 概要 本文档综合分析了吴恩达关于“代理式AI工作流”（Agentic AI Workflows）的核心理念与实践方法。代理式AI是一种强大的应用程序构建范式，它将复杂的任务分解为多个步骤，通过迭代、反思和工具使用来执行，从而获得远超传统单次提示方法的性能和成果。\n核心要点 代理式工作流的定义： 与一次性生成结果的模式不同，代理式工作流是一个多步骤过程，AI系统通过规划、执行动作、反思和修正来完成任务。这类似于人类处理复杂问题的方式，例如先写大纲、再做研究、然后起草、最后修改。 性能的巨大飞跃： 采用代理式工作流带来的性能提升，可能超过模型本身的代际升级。数据显示，为GPT-3.5模型应用代理式工作流后，其在编程基准测试中的表现甚至可以超越未使用该工作流的更强大的GPT-4模型。 自主性光谱： 代理式系统存在一个从“低自主性”到“高自主性”的光谱。低自主性系统（步骤由工程师预先设定）更易于控制、更可靠，已在商业中广泛应用。高自主性系统（由LLM自行决定步骤）则更具实验性，也更难预测。 成功的关键技能： 构建高效代理式工作流的两大核心技能是任务分解（将复杂任务拆解为LLM或工具可执行的小步骤）和严格的评估（“Evals”），通过系统化的错误分析和性能追踪来驱动迭代改进。 四大设计模式： 构建代理式工作流主要依赖四种关键的设计模式： 反思 (Reflection): 让LLM检查并批判自身的输出，从而进行迭代改进。 工具使用 (Tool Use): 赋予LLM调用外部函数（如网络搜索、代码执行、数据库查询）的能力。 规划 (Planning): LLM自主决定完成任务所需的步骤顺序。 多代理协作 (Multi-Agent Collaboration): 模拟一个团队，让多个具有不同角色的AI代理协同工作。 广泛的应用价值： 代理式工作流已成功应用于多种场景，包括深度研究报告撰写、客户支持、发票处理和复杂的法律文件分析。对于许多项目而言，没有代理式工作流，其实现将“不可能”。 1. 代理式AI工作流：定义与理念 1.1 核心概念：超越单次生成 传统的与大型语言模型（LLM）交互的方式是“直接生成”，即用户提供一个提示，LLM一次性地从头到尾生成完整的文本，这如同要求一个人“不使用退格键一次性写完一篇文章”。\n","description":"本文档综合分析了吴恩达关于“代理式AI工作流”（Agentic AI Workflows）的核心理念与实践方法。代理式AI是一种强大的应用程序构建范式，它将复杂的任务分解为多个步骤，通过迭代、反思和工具使用来执行，从而获得远超传统单次提示方法的性能和成果。","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/7c889fbd-c356-4e24-8871-9704a4188f5c 「吴恩达Agentic AI 模块1简报」Agentic AI工作流 概要 本文档综合分析了吴恩达关于“代理式AI工作流”（Agentic AI Workflows）的核心理念与实践方法。代理式AI是一种强大的应用程序构建范式，它将复杂的任务分解为多个步骤，通过迭代、反思和工具使用来执行，从而获得远超传统单次提示方法的性能和成果。\n核心要点 代理式工作流的定义： 与一次性生成结果的模式不同，代理式工作流是一个多步骤过程，AI系统通过规划、执行动作、反思和修正来完成任务。这类似于人类处理复杂问题的方式，例如先写大纲、再做研究、然后起草、最后修改。 性能的巨大飞跃： 采用代理式工作流带来的性能提升，可能超过模型本身的代际升级。数据显示，为GPT-3.5模型应用代理式工作流后，其在编程基准测试中的表现甚至可以超越未使用该工作流的更强大的GPT-4模型。 自主性光谱： 代理式系统存在一个从“低自主性”到“高自主性”的光谱。低自主性系统（步骤由工程师预先设定）更易于控制、更可靠，已在商业中广泛应用。高自主性系统（由LLM自行决定步骤）则更具实验性，也更难预测。 成功的关键技能： 构建高效代理式工作流的两大核心技能是任务分解（将复杂任务拆解为LLM或工具可执行的小步骤）和严格的评估（“Evals”），通过系统化的错误分析和性能追踪来驱动迭代改进。 四大设计模式： 构建代理式工作流主要依赖四种关键的设计模式： 反思 (Reflection): 让LLM检查并批判自身的输出，从而进行迭代改进。 工具使用 (Tool Use): 赋予LLM调用外部函数（如网络搜索、代码执行、数据库查询）的能力。 规划 (Planning): LLM自主决定完成任务所需的步骤顺序。 多代理协作 (Multi-Agent Collaboration): 模拟一个团队，让多个具有不同角色的AI代理协同工作。 广泛的应用价值： 代理式工作流已成功应用于多种场景，包括深度研究报告撰写、客户支持、发票处理和复杂的法律文件分析。对于许多项目而言，没有代理式工作流，其实现将“不可能”。 1. 代理式AI工作流：定义与理念 1.1 核心概念：超越单次生成 传统的与大型语言模型（LLM）交互的方式是“直接生成”，即用户提供一个提示，LLM一次性地从头到尾生成完整的文本，这如同要求一个人“不使用退格键一次性写完一篇文章”。\n代理式AI工作流则截然不同。它将一个复杂的任务分解为一系列的子任务，并以迭代的方式完成。\n典型流程（以撰写研究报告为例）： 生成大纲： 首先，让LLM为主题撰写一个初步大纲。 研究与信息收集： 接着，让LLM决定需要进行哪些网络搜索，并调用搜索API来获取相关网页内容。 起草初稿： 基于收集到的信息，LLM撰写第一版草稿。 反思与修订： LLM（或另一个AI代理）阅读初稿，识别需要修改或补充研究的部分。 人类介入（可选）： 在关键环节（如事实核查），可以设计让AI请求人类审核的步骤。 最终修订： 综合所有反馈，完成最终报告。 吴恩达指出，这种迭代过程虽然耗时更长，但最终产出的工作成果质量“要好得多”。\n1.2 “代理式 (Agentic)”术语的由来 吴恩达创造并推广“代理式 (agentic)”这个形容词，是为了避免AI社区中关于“什么是真正的代理 (agent)”的无谓争论。他认为，与其将系统划分为“是代理”或“不是代理”的二元对立，不如承认系统可以在不同程度上表现出“代理行为”。这一术语的引入，旨在将焦点从定义辩论转移到构建有价值的系统上。\n2. 自主性光谱：从预定步骤到动态决策 代理式AI系统并非只有一种形态，而是存在于一个从低到高的自主性光谱上。\n自主性程度 特征 优点 缺点 应用场景 低自主性 1. 步骤顺序由工程师预先硬编码。\n2. 工具调用是确定性的。\n3. LLM的主要作用是生成文本内容。 可控性强、结果可预测、可靠性高 灵活性差、无法处理未知流程 大多数当前商业应用，如发票处理、标准客户问询。 半自主性 1. LLM可以在一定范围内做出决策。\n2. 可以从预定义的工具集中选择并调用工具。 兼具一定的灵活性和可控性。 复杂性增加、灵活性差、无法处理未知流程 灵活性差、无法处理未知流程 高自主性 1. LLM自主决定完成任务的完整步骤顺序。\n2. 甚至可以自主编写新的函数或创建新工具。\n3. 流程是动态且不确定的。 极高的灵活性、能处理未知和复杂的任务 可控性差、结果难以预测、可靠性较低，更具实验 …"},{"title":"「吴恩达Agentic AI 模块2」反思设计模式, 让AI自我修正","url":"/courses/andrew-ng-agentic-ai/blog/lec-02/","section":"courses","date":"2025-10-17","summary":" 吴恩达提出“反思”（Reflection）设计模式，它能让 AI 像人类一样审视并修正自己的工作，从而大幅提升输出质量。这是一种你在许多高级 AI 应用中都能看到的强大工作流。\n","description":"你是否也曾有过这样的经历：满怀期待地向一个大型语言模型（LLM）提出请求，结果却只得到一份平庸，甚至有明显错误的初稿？无论是生成代码、撰写邮件还是分析数据，AI的第一反应往往不够完美。这时，我们通常会选择反复修改提示词，但这就像是在黑暗中摸索，效果时好时坏。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" 吴恩达提出“反思”（Reflection）设计模式，它能让 AI 像人类一样审视并修正自己的工作，从而大幅提升输出质量。这是一种你在许多高级 AI 应用中都能看到的强大工作流。\nhttps://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/3c2ef484-a8c3-443f-bb90-11b8ae8904d9 「吴恩达Agentic AI 模块2」反思设计模式, 让AI自我修正 你是否也曾有过这样的经历：满怀期待地向一个大型语言模型（LLM）提出请求，结果却只得到一份平庸，甚至有明显错误的初稿？无论是生成代码、撰写邮件还是分析数据，AI 的第一反应往往不够完美。这时，我们通常会选择反复修改提示词，但这就像是在黑暗中摸索，效果时好时坏。\n然而，吴恩达（Andrew Ng）揭示了一种出人意料的简单技巧，能系统性地解决这个问题。这个技巧被称为“反思”（Reflection）设计模式，它能让 AI 像人类一样审视并修正自己的工作，从而大幅提升输出质量。这是一种你在许多高级 AI 应用中都能看到的强大工作流。\n像人类一样，AI 也能“三思而后行” 反思模式的核心概念非常直观。吴恩达用了一个生动的比喻：想象一下你正在匆忙地写一封电子邮件。你的初稿可能存在一些问题：\n用词模糊：“下个月”具体是哪几天？ 笔误：出现了打字错误。 遗漏信息：忘记了署名。 在你点击“发送”之前，你会自然而然地重读一遍，发现这些问题，然后写出一个更清晰、更准确的第二版。\nAI 也能遵循完全相同的两步流程。你可以先让 LLM 生成一个初稿（v1），然后，用一个新的提示词，指示它对这份初稿进行“反思”和“批判”，从而生成一个经过改进的最终版本（v2）。最妙的是，这个过程可以用同一个 LLM 来完成。\n正如人类会审视并改进自己的作品，大语言模型同样能做到这一点。\n真正的“秘密武器”：外部反馈 虽然自我反思很有用，但真正让这个模式威力倍增的，是在反思步骤中引入新的外部信息。\n以代码生成为例，这个流程非常清晰：\n你让 LLM 编写一段代码（v1）。 你执行这段代码。 代码运行后，抛出了一个语法错误。 这个“语法错误信息”就是全新的、来自模型外部的反馈。 你将原始代码（v1）和这个错误信息一起提交给 LLM，让它进行反思。 有了这个具体的外部反馈，LLM 就能进行更深度的反思，并生成一个正确得多的第二版代码（v2）。吴恩达指出，当你感觉单纯的提示词工程（Prompt Engineering）已经遇到瓶颈、收效甚微时，引入带有外部反馈的反思模式，往往能将系统性能推向一个全新的高度。\n不仅仅是文本：AI 还能“审视”自己的视觉作品 反思模式最令人惊艳的应用之一，是它同样适用于多模态任务。想象一下，你要求一个 AI 根据咖啡销售数据生成一张图表。\n初始结果：AI 编写的代码生成了一张“堆叠条形图”。这种图表虽然技术上没错，但可读性很差，不够直观。 反思步骤：接下来，你将初始代码和它生成的那张图表图片同时提供给一个多模态 LLM。关键在于你的提示词——要求它 “扮演一位专业数据分析师的角色”，对图表进行批判性审视。 惊人效果：这个多模态 LLM 能够进行视觉推理。它能“看到”这张图表，判断出其可视化效果不佳，然后主动修改代码，生成一张更清晰、更美观的常规条形图。 这展示了 AI 不仅能反思文本，还能审视自己的视觉创作并加以改进。\n打造 AI “评审员”来评估质量（但方法要对） 对于像“哪张图表更好看”这样主观的问题，我们该如何评估 AI 的表现呢？一个常见的想法是让另一个 LLM 充当“评委”。但这里有一个不易察觉的陷阱。\n如果你直接问 LLM：“图片 A 和图片 B，哪一个更好？” 你可能会得到一个不可靠的答案。吴恩达指出了一个关键问题：位置偏见（position bias）。许多 LLM 在被要求做二选一时，会倾向于选择它们看到的第一个选项。\n更科学、更可靠的方法是：\n不要进行直接比较。 让 LLM 根据一个详细的评分标准（rubric）单个输出打分。 这个评分标准应包含一系列具体的、二元（是/否）的问题，例如：“图表是否有清晰的标题？”、“坐标轴标签是否存在？”。 吴恩达解释说，之所以这样做，是因为 LLM 在像 1-5 分这样的量表上校准得并不好，而将多个二元（0/1）评分项的总分相加，能够产生更一致的结果。\n组建一支“专家级” AI 团队 反思模式还有一个高级玩法：使用不同的 LLM 来执行生成和反思两个步骤。\n这个策略的逻辑很简单：不同的模型有不同的专长。这就像一个创意团队，一个人负责天马行空地产生想法（生成），另一个人则扮演一丝不苟的编辑角色（反思）。 …"},{"title":"「吴恩达Agentic AI 模块2」反思设计模式学习指南","url":"/courses/andrew-ng-agentic-ai/guides/lec-02/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/3c2ef484-a8c3-443f-bb90-11b8ae8904d9 「吴恩达Agentic AI 模块2」反思设计模式学习指南 测验 简答题 说明： 请根据提供的学习材料，用2-3句话简要回答以下问题。\n什么是反思设计模式（Reflection Design Pattern）？ 与直接生成相比，反思设计模式的主要优势是什么？ 在反思过程中，为什么“外部反馈”（External Feedback）如此重要？ 什么是“零样本提示”（Zero-Shot Prompting），它与“直接生成”有何关系？ 在为反思设计模式编写提示（prompt）时，有哪些关键技巧可以提高其有效性？ 多模态模型（Multimodal Model）如何在图表生成工作流程中应用反思模式？ 什么是“客观评估”（Objective Evals），请以数据库查询为例说明其应用。 在进行主观评估时，为什么使用“评分标准”（Rubric）通常比直接让大型语言模型（LLM）作为裁判进行比较更好？ 请列举一个使用软件工具为反思过程提供外部反馈的具体例子。 何时应该考虑在工作流程中加入反思，特别是当提示工程（prompt engineering）遇到瓶颈时？ 答案解析 什么是反思设计模式（Reflection Design Pattern）？ 反思设计模式是一种工作流程，它模仿人类反思和改进自己工作的方式。在这个模式中，一个大型语言模型（LLM）首先生成一个初步的输出（如草稿代码或邮件），然后由同一个或另一个LLM对该草稿进行审视、批判和改进，从而生成一个更高质量的最终版本。 与直接生成相比，反思设计模式的主要优势是什么？ 研究表明，在多种任务上，反思设计模式能够显著提升系统性能。与直接生成的一次性输出相比，反思通过增加一个审视和修正的步骤，可以发现并修复初稿中的错误、遗漏或不清晰之处，从而得到更准确、更完善的结果。 在反思过程中，为什么“外部反馈”（External Feedback）如此重要？ 外部反馈为LLM提供了来自模型外部的新信息，这使得反思过程更加强大和深入。例如，执行代码后得到的错误信息或通过网络搜索核查的事实，都为LLM提供了具体的、客观的修正依据，使其能够更有效地发现并解决问题，而不是仅仅基于已有的信息进行推断。 什么是“零样本提示”（Zero-Shot Prompting），它与“直接生成”有何关系？ 零样本提示是指在不向LLM提供任何输入-输出示例的情况下，仅通过指令来让其生成答案。这种方式通常被视为“直接生成”，因为它要求模型一步到位地直接产出最终结果，而没有像一样本（one-shot）或少样本（few-shot）提示那样提供参考范例。 在为反思设计模式编写提示（prompt）时，有哪些关键技巧可以提高其有效性？ 为了编写有效的反思提示，首先应明确指示模型需要“审阅”或“反思”初稿。其次，提供一套清晰的评估标准（criteria），例如检查域名是否易于发音、邮件语气是否恰当或代码是否清晰完整，这样可以更好地引导LLM关注你最关心的方面。 多模态模型（Multimodal Model）如何在图表生成工作流程中应用反思模式？ 在图表生成中，一个LLM首先根据数据生成初始代码（v1）和图表。然后，一个能够处理图像输入的多模态模型会同时接收这段代码和生成的图表图像，通过视觉推理（visual reasoning）来评判图表的清晰度和美观度，并提出改进建议，最终生成更新后的代码（v2）以绘制出更好的图表。 什么是“客观评估”（Objective Evals），请以数据库查询为例说明其应用。 客观评估是指当任务存在明确的正确答案时，用来衡量系统性能的方法。在数据库查询的例子中，开发者会创建一个包含多个问题和其“标准答案”（ground truth answers）的评估集。通过比较系统在有反思和无反思两种情况下给出正确答案的百分比（例如95% vs 87%），就可以客观地判断反思步骤是否有效提升了查询的准确性。 在进行主观评估时，为什么使用“评分标准”（Rubric）通常比直接让大型语言模型（LLM）作为裁判进行比较更好？ 直接让LLM比较两个主观输出（如两张图表）的效果不佳，因为它可能存在位置偏见（position bias），即倾向于选择第一个选项，并且其判断标准不稳定。而使用评分标准，通过让LLM根据一系列明确的、二元的标准（如有无标题、坐标轴标签是否清晰等）对单个输出进行打分，可以得到更一致、更可靠的评估结果。 请列举一个使用软件工具为反思过程提供外部反馈的具体例子。 一个例子是使用单词计数工具来辅助文案写作。如果LLM生成的博客文章草稿超过了字数限制，一个简单的代码工具可以精确计算出单词数量，并将这个“超出限制”的信息作为外部反馈提供给LLM，指导它在下一稿中缩减篇幅以满足要求。 何时应该考虑在工作流程中加入反思，特别是当提示工程（prompt engineering）遇到瓶颈时？ 当你通过不断调整和优化提示（即提示工程）发现性能提升逐渐趋于平缓，投入更多努力也难以获得显著改善时，就应该考虑加入反思。引入反思，特别是带有外部反馈的反思，可以将性能曲线从停滞的平台期提升到一个新的、更高的改进轨道上。 论述题 说明： 请思考以下问题，并准备以论文形式进行深入探讨。这些问题没有提供标准答案。\n","description":"说明： 请根据提供的学习材料，用2-3句话简要回答以下问题。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/3c2ef484-a8c3-443f-bb90-11b8ae8904d9 「吴恩达Agentic AI 模块2」反思设计模式学习指南 测验 简答题 说明： 请根据提供的学习材料，用2-3句话简要回答以下问题。\n什么是反思设计模式（Reflection Design Pattern）？ 与直接生成相比，反思设计模式的主要优势是什么？ 在反思过程中，为什么“外部反馈”（External Feedback）如此重要？ 什么是“零样本提示”（Zero-Shot Prompting），它与“直接生成”有何关系？ 在为反思设计模式编写提示（prompt）时，有哪些关键技巧可以提高其有效性？ 多模态模型（Multimodal Model）如何在图表生成工作流程中应用反思模式？ 什么是“客观评估”（Objective Evals），请以数据库查询为例说明其应用。 在进行主观评估时，为什么使用“评分标准”（Rubric）通常比直接让大型语言模型（LLM）作为裁判进行比较更好？ 请列举一个使用软件工具为反思过程提供外部反馈的具体例子。 何时应该考虑在工作流程中加入反思，特别是当提示工程（prompt engineering）遇到瓶颈时？ 答案解析 什么是反思设计模式（Reflection Design Pattern）？ 反思设计模式是一种工作流程，它模仿人类反思和改进自己工作的方式。在这个模式中，一个大型语言模型（LLM）首先生成一个初步的输出（如草稿代码或邮件），然后由同一个或另一个LLM对该草稿进行审视、批判和改进，从而生成一个更高质量的最终版本。 与直接生成相比，反思设计模式的主要优势是什么？ 研究表明，在多种任务上，反思设计模式能够显著提升系统性能。与直接生成的一次性输出相比，反思通过增加一个审视和修正的步骤，可以发现并修复初稿中的错误、遗漏或不清晰之处，从而得到更准确、更完善的结果。 在反思过程中，为什么“外部反馈”（External Feedback）如此重要？ 外部反馈为LLM提供了来自模型外部的新信息，这使得反思过程更加强大和深入。例如，执行代码后得到的错误信息或通过网络搜索核查的事实，都为LLM提供了具体的、客观的修正依据，使其能够更有效地发现并解决问题，而不是仅仅基于已有的信息进行推断。 什么是“零样本提示”（Zero-Shot Prompting），它与“直接生成”有何关系？ 零样本提示是指在不向LLM提供任何输入-输出示例的情况下，仅通过指令来让其生成答案。这种方式通常被视为“直接生成”，因为它要求模型一步到位地直接产出最终结果，而没有像一样本（one-shot）或少样本（few-shot）提示那样提供参考范例。 在为反思设计模式编写提示（prompt）时，有哪些关键技巧可以提高其有效性？ 为了编写有效的反思提示，首先应明确指示模型需要“审阅”或“反思”初稿。其次，提供一套清晰的评估标准（criteria），例如检查域名是否易于发音、邮件语气是否恰当或代码是否清晰完整，这样可以更好地引导LLM关注你最关心的方面。 多模态模型（Multimodal Model）如何在图表生成工作流程中应用反思模式？ 在图表生成中，一个LLM首先根据数据生成初始代码（v1）和图表。然后，一个能够处理图像输入的多模态模型会同时接收这段代码和生成的图表图像，通过视觉推理（visual reasoning）来评判图表的清晰度和美观度，并提出改进建议，最终生成更新后的代码（v2）以绘制出更好的图表。 什么是“客观评估”（Objective Evals），请以数据库查询为例说明其应用。 客观评估是指当任务存在明确的正确答案时，用来衡量系统性能的方法。在数据库查询的例子中，开发者会创建一个包含多个问题和其“标准答案”（ground truth answers）的评估集。通过比较系统在有反思和无反思两种情况下给出正确答案的百分比（例如95% vs 87%），就可以客观地判断反思步骤是否有效提升了查询的准确性。 在进行主观评估时，为什么使用“评分标准”（Rubric）通常比直接让大型语言模型（LLM）作为裁判进行比较更好？ 直接让LLM比较两个主观输出（如两张图表）的效果不佳，因为它可能存在位置偏见（position bias），即倾向于选择第一个选项，并且其判断标准不稳定。而使用评分标准，通过让LLM根据一系列明确的、二元的标准（如有无标题、坐标轴标签是否清晰等）对单个输出进行打分，可以得到更一致、更可靠的评估结果。 请列举一个使用软件工具为反思过程提供外部反馈的具体例子。 一 …"},{"title":"「吴恩达Agentic AI 模块3」代理式人工智能工具使用与MCP学习指南","url":"/courses/andrew-ng-agentic-ai/guides/lec-03/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 「吴恩达Agentic AI 模块3」代理式人工智能工具使用与MCP学习指南 测验 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n在大型语言模型（LLM）的背景下，“工具使用”指的是什么？为什么它很重要？ 当一个工具可用时，LLM是否总会选择使用它？请解释其决策过程。 根据“早期时代”的方法，描述LLM使用工具的完整流程。 在手动的工具创建方法中，LLM是直接调用函数吗？请解释其背后的机制。 像AISuite这样的现代库与旧的手动提示方法相比，是如何简化为LLM提供工具的过程的？ 在现代工具使用语法中，JSON模式（JSON schema）扮演什么角色？它通常包含哪些信息？ 为什么说代码执行是LLM一个特别强大的工具？ 与LLM代码执行相关的主要风险是什么？缓解这种风险的最佳实践是什么？ 模型上下文协议（MCP）旨在解决开发者社区面临的什么核心问题？ 什么是MCP客户端和MCP服务器？请举例说明它们如何交互。 测验答案 在大型语言模型（LLM）的背景下，“工具使用”指的是什么？为什么它很重要？ “工具使用”是指让LLM自行决定何时请求调用一个函数来执行特定动作、收集信息或完成其他任务。这非常重要，因为就像人类使用工具能完成更多事情一样，为LLM提供工具（即函数）能极大地扩展其能力，使其能够完成仅凭自身训练数据无法完成的任务，例如获取当前时间或查询数据库。 当一个工具可用时，LLM是否总会选择使用它？请解释其决策过程。 不，LLM并不会总是使用可用的工具。开发者可以将决策权留给LLM，让它根据具体的用户提示来判断是否需要调用工具。例如，如果向一个拥有“获取当前时间”工具的LLM询问绿茶中的咖啡因含量，它会直接生成答案，而不会调用该工具，因为它判断该工具与问题无关。 根据“早期时代”的方法，描述LLM使用工具的完整流程。 该流程包括几个步骤：首先，开发者通过提示告知LLM可用工具及其调用格式。当LLM决定使用工具时，它会生成一个特定格式的文本（如FUNCTION: getCurrentTime）作为请求。然后，开发者编写的代码会捕获并解析这个输出，实际执行相应的函数，并将函数返回的结果反馈给LLM，最后LLM利用这个新信息生成最终的答复。 在手动的工具创建方法中，LLM是直接调用函数吗？请解释其背后的机制。 不，LLM不直接调用函数。其机制是，LLM通过生成一种特定格式的文本输出来“请求”调用函数。开发者需要编写代码来监视LLM的输出，当检测到这个预定义的格式时，由开发者的代码来负责解析请求、执行相应的函数，并将结果返回给LLM。 像AISuite这样的现代库与旧的手动提示方法相比，是如何简化为LLM提供工具的过程的？ 现代库（如AISuite）能自动处理将函数描述给LLM的复杂过程。开发者无需手动编写冗长的提示来解释每个工具的用法，库会自动检查函数的文档字符串（docstring），并生成一个结构化的JSON模式。这个模式清晰地告诉LLM函数的名称、功能和参数，从而实现了工具的无缝集成。 在现代工具使用语法中，JSON模式（JSON schema）扮演什么角色？它通常包含哪些信息？ JSON模式扮演着向LLM详细描述可用工具的角色。它是一个结构化的数据格式，通常包含函数的名称（name）、功能的详细描述（description，通常从代码的文档字符串中提取）以及函数的参数列表（parameters），包括每个参数的类型和用途。这使得LLM能够准确理解何时以及如何调用该工具。 为什么说代码执行是LLM一个特别强大的工具？ 代码执行之所以强大，是因为它赋予了LLM极大的灵活性和解决复杂问题的能力。相比于为每个数学运算（加、减、平方根等）创建单独的工具，允许LLM编写并执行代码能让它处理几乎无限种类的计算和逻辑任务。这种通用性使得LLM能够用创新的代码解决方案来应对各种复杂的用户请求。 与LLM代码执行相关的主要风险是什么？缓解这种风险的最佳实践是什么？ 主要风险在于安全性，LLM可能生成任意甚至有害的代码，例如错误地删除文件（如remove star.py）。缓解此风险的最佳实践是在一个安全的沙盒环境（如Docker或E2B）中运行代码。沙盒环境可以隔离代码的执行，从而降低数据丢失、敏感数据泄露或对系统造成损害的风险。 模型上下文协议（MCP）旨在解决开发者社区面临的什么核心问题？ MCP旨在解决开发者在将各种工具和数据源（如Slack、GitHub、数据库）集成到其LLM应用中时普遍存在的重复劳动问题。在MCP出现之前，每个应用开发者都需要为同一个数据源编写自定义的封装代码，导致社区总工作量呈 M（应用数）乘以 N（工具数）的规模。MCP通过提供一个标准化的集成协议，将工作量减少到 M 加 N 的规模。 什么是MCP客户端和MCP服务器？请举例说明它们如何交互。 MCP客户端是需要访问工具或数据的应用程序，而MCP服务器是提供这些工具或数据访问的服务封装层。例如，一个云桌面应用（MCP客户端）在需要获取GitHub仓库信息时，会向GitHub MCP服务器发送请求。服务器接收请求（如“列出最新的拉取请求”），执行相应的操作，并将结果返回给客户端，客户端再将这些信息提供给LLM以生成最终的摘要。 论文题目建议 比较并对比“早期时代”手动启用工具使用的方法与使用AISuite等现代库的自动化方法。深入探讨开发者工作流程的演变、底层机制的差异以及对应用开发效率的影响。 结合工具使用的概念，讨论大型语言模型中的“代理式”（Agentic）行为。模型能够决定何时使用哪个工具（包括决定不使用任何工具）的能力，是如何促成其代理特质的？ 深入分析将“代码执行”作为LLM工具的独特优势和重大风险。开发者应如何平衡这一工具的强大功能与保障系统安全性和可靠性的需求？请从技术和实践角度进行论述。 阐述模型上下文协议（MCP）的原理及其在构建复杂、多工具LLM应用生态系统中的重要性。该协议如何改变应用开发者（客户端）和服务提供商（服务器）之间的互动模式和开发范式？ 设想你正在构建一个课程资料中描述的“日历助手代理”。请详细规划你的开发步骤，从定义必要的工具（如检查日历、创建约会），到实现与LLM的交互循环。在你的规划中，请引用全部五个相关模块（工具定义、创建、语法、代码执行、MCP）中的概念。 关键术语词汇表 术语 定义 工具使用 (Tool Use) 允许大型语言模型（LLM）自行决定何时请求调用一个函数，以执行特定动作、收集信息或完成其他任务的过程。 工具 (Tool) 提供给LLM的函数或代码，LLM可以请求调用这些函数来扩展其能力。 代理工作流 (Agentic Workflow) LLM作为一个代理，自主地规划步骤、决策并使用工具来完成复杂任务的一系列流程。 代码执行 (Code Execution) 一种特殊的工具，允许LLM编写代码并请求执行，用于解决数学计算、数据处理等复杂问题，具有高度的灵活性。 沙盒环境 (Sandbox Environment) 一个安全的、隔离的执行环境（如Docker），用于运行由LLM生成的代码，以防止潜在的有害操作对系统造成损害。 JSON模式 (JSON Schema) 一种基于JSON的结构化数据格式，用于向LLM详细描述一个工具（函数），包括其名称、功能描述和参数。 AISuite 一个开源库，它简化了向LLM提供工具的过程，能够自动从函数的文档字符串生成JSON模式，与OpenAI的语法非常相似。 模型上下文协议 (MCP) 由Anthropic提出并被广泛采用的一个标准协议，旨在简化应用程序（客户端）对外部工具和数据源（服务器）的访问，减少开发者的重复集成工作。 MCP客户端 (MCP Client) 遵循MCP标准的应用程序，它消费（使用）由MCP服务器提供的工具或数据资源。 MCP服务器 (MCP Server) 遵循MCP标准的服务，它将对数据源（如GitHub, Slack）的访问封装起来，向MCP客户端提供工具或数据资源。 ","description":"请用2-3句话回答以下每个问题，以检验您对核心概念的理解。","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 「吴恩达Agentic AI 模块3」代理式人工智能工具使用与MCP学习指南 测验 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n在大型语言模型（LLM）的背景下，“工具使用”指的是什么？为什么它很重要？ 当一个工具可用时，LLM是否总会选择使用它？请解释其决策过程。 根据“早期时代”的方法，描述LLM使用工具的完整流程。 在手动的工具创建方法中，LLM是直接调用函数吗？请解释其背后的机制。 像AISuite这样的现代库与旧的手动提示方法相比，是如何简化为LLM提供工具的过程的？ 在现代工具使用语法中，JSON模式（JSON schema）扮演什么角色？它通常包含哪些信息？ 为什么说代码执行是LLM一个特别强大的工具？ 与LLM代码执行相关的主要风险是什么？缓解这种风险的最佳实践是什么？ 模型上下文协议（MCP）旨在解决开发者社区面临的什么核心问题？ 什么是MCP客户端和MCP服务器？请举例说明它们如何交互。 测验答案 在大型语言模型（LLM）的背景下，“工具使用”指的是什么？为什么它很重要？ “工具使用”是指让LLM自行决定何时请求调用一个函数来执行特定动作、收集信息或完成其他任务。这非常重要，因为就像人类使用工具能完成更多事情一样，为LLM提供工具（即函数）能极大地扩展其能力，使其能够完成仅凭自身训练数据无法完成的任务，例如获取当前时间或查询数据库。 当一个工具可用时，LLM是否总会选择使用它？请解释其决策过程。 不，LLM并不会总是使用可用的工具。开发者可以将决策权留给LLM，让它根据具体的用户提示来判断是否需要调用工具。例如，如果向一个拥有“获取当前时间”工具的LLM询问绿茶中的咖啡因含量，它会直接生成答案，而不会调用该工具，因为它判断该工具与问题无关。 根据“早期时代”的方法，描述LLM使用工具的完整流程。 该流程包括几个步骤：首先，开发者通过提示告知LLM可用工具及其调用格式。当LLM决定使用工具时，它会生成一个特定格式的文本（如FUNCTION: getCurrentTime）作为请求。然后，开发者编写的代码会捕获并解析这个输出，实际执行相应的函数，并将函数返回的结果反馈给LLM，最后LLM利用这个新信息生成最终的答复。 在手动的工具创建方法中，LLM是直接调用函数吗？请解释其背后的机制。 不，LLM不直接调用函数。其机制是，LLM通过生成一种特定格式的文本输出来“请求”调用函数。开发者需要编写代码来监视LLM的输出，当检测到这个预定义的格式时，由开发者的代码来负责解析请求、执行相应的函数，并将结果返回给LLM。 像AISuite这样的现代库与旧的手动提示方法相比，是如何简化为LLM提供工具的过程的？ 现代库（如AISuite）能自动处理将函数描述给LLM的复杂过程。开发者无需手动编写冗长的提示来解释每个工具的用法，库会自动检查函数的文档字符串（docstring），并生成一个结构化的JSON模式。这个模式清晰地告诉LLM函数的名称、功能和参数，从而实现了工具的无缝集成。 在现代工具使用语法中，JSON模式（JSON schema）扮演什么角色？它通常包含哪些信息？ JSON模式扮演着向LLM详细描述可用工具的角色。它是一个结构化的数据格式，通常包含函数的名称（name）、功能的详细描述（description，通常从代码的文档字符串中提取）以及函数的参数列表（parameters），包括每个参数的类型和用途。这使得LLM能够准确理解何时以及如何调用该工具。 为什么说代码执行是LLM一个特别强大的工具？ 代码执行之所以强大，是因为它赋予了LLM极大的灵活性和解决复杂问题的能力。相比于为每个数学运算（加、减、平方根等）创建单独的工具，允许LLM编写并执行代码能让它处理几乎无限种类的计算和逻辑任务。这种通用性使得LLM能够用创新的代码解决方案来应对各种复杂的用户请求。 与LLM代码执行相关的主要风险是什么？缓解这种风险的最佳实践是什么？ 主要风险在于安全性，LLM可能生成任意甚至有害的代码，例如错误地删除文件（如remove star.py）。缓解此风险的最佳实践是在一个安全的沙盒环境（如Docker或E2B）中运行代码。沙盒环境可以隔离代码的执行，从而降低数据丢失、敏感数据泄露或对系统造成损害的风险。 模型上下文协议（MCP）旨在解决开发者社区面临的什么核心问题？ MCP旨在解决开发者在将各种工具和数据源（如Slack、GitHub、数据库）集 …"},{"title":"「吴恩达Agentic AI 模块3」关于AI如何使用“工具”：五个真相","url":"/courses/andrew-ng-agentic-ai/blog/lec-03/","section":"courses","date":"2025-10-17","summary":" 我们通常认为大型语言模型（LLM）是强大的文本生成器，擅长对话和写作。但如果它们能做的远不止于此呢？如果它们能够采取行动、查询数据、完成任务呢？“工具使用”（Tool Use）正是解锁这一能力的关键\n","description":"我们通常认为大型语言模型（LLM）是强大的文本生成器，擅长对话和写作。但如果它们能做的远不止于此呢？如果它们能够采取行动、查询数据、完成任务呢？“工具使用”（Tool Use）正是解锁这一能力的关键，而其工作原理可能比大多数人想象的要更令人惊讶。本文将揭示关于AI工具使用的五个最具影响力的真相。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" 我们通常认为大型语言模型（LLM）是强大的文本生成器，擅长对话和写作。但如果它们能做的远不止于此呢？如果它们能够采取行动、查询数据、完成任务呢？“工具使用”（Tool Use）正是解锁这一能力的关键\nhttps://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 「吴恩达Agentic AI 模块3」关于AI如何使用“工具”：五个真相 我们通常认为大型语言模型（LLM）是强大的文本生成器，擅长对话和写作。但如果它们能做的远不止于此呢？如果它们能够采取行动、查询数据、完成任务呢？“工具使用”（Tool Use）正是解锁这一能力的关键，而其工作原理可能比大多数人想象的要更令人惊讶。本文将揭示关于AI工具使用的五个最具影响力的真相。\n真相一：LLM实际上不“调用”工具，而是“请求”执行 这是最核心且违反直觉的一个机制：作为一个纯粹的文本生成器，LLM本身无法直接执行任何代码或调用一个外部函数。它完全依赖于开发者编写的应用程序代码来完成整个流程。\n实际的工作流是一个由开发者精心设计的、多步骤的协作过程：\n生成意图：当LLM判断需要使用工具时，它会生成一段特殊格式的文本，比如 FUNCTION: getCurrentTime。这串文本仅仅是表达了它想要使用工具的意图。 应用检测：开发者编写的应用程序代码会持续监控LLM的输出，并检测到这个特殊格式的文本请求。 应用执行：检测到意图后，应用程序会代替LLM去执行真正的函数（例如，调用Python的 getCurrentTime() 函数）。 结果反馈：函数执行后返回的结果（比如 \u0026ldquo;下午3:20\u0026rdquo;）会被应用程序捕获，并作为新的信息添加回与LLM的对话历史中。 最终回答：LLM读取包含新结果的对话历史，然后生成最终给用户的、通顺自然的回答（例如，“现在是下午3:20”）。 因此，这并非模型自身的直接行动，而是模型与外部应用程序之间的一场精心编排的“合作舞蹈”，LLM在其中扮演的是“请求者”的角色，而真正的“执行者”是开发者构建的系统。\n真相二：LLM自己决定何时使用工具 现代的工具使用方法之所以强大，在于其“智能代理”（Agentic）的特性。开发者只需向LLM提供一系列可用的工具，而由LLM根据用户的具体问题自主决定是否以及何时使用它们。\n这与“硬编码”的逻辑截然不同。在硬编码中，开发者会强制规定在某个步骤必须调用某个函数。但在智能代理模式下：\n当你给LLM一个 getCurrentTime 工具并提问“现在几点了？”，它会判断出需要使用这个工具来获取实时信息。 但如果你问的是“绿茶里有多少咖啡因？”，它会正确地判断出 getCurrentTime 工具对此问题毫无帮助，于是直接利用其内部知识库进行回答。 这种基于上下文的决策能力，使得系统能够灵活地适应千变万化的用户需求，是其强大功能的根源。\n真相三：代码执行是LLM的“超级工具” 与其为LLM创建大量功能单一的特定工具，不如给它一个更强大、更通用的“超级工具”：代码解释器。\n想象一下计算器：我们不需要为加、减、乘、除、开方、求幂等每一个运算都创建一个单独的工具。我们可以只提供一个工具——一个Python代码执行环境。\n工作流程如下：LLM被指示编写一小段代码来解决问题。应用程序提取并执行这段代码，然后将代码的运行结果反馈给LLM。\n例如，对于问题“2的平方根是多少？”，LLM不需要一个专门的 square_root 工具，它可以直接生成并请求执行代码：\nimport math; print(math.sqrt(2)) 正如吴恩达（Andrew Ng）在分享个人经验时所说：\n“有好几次，在我参与的一些智能代理应用项目中，它为了帮我完成各种任务而生成的代码解决方案，其巧妙程度都让我感到非常惊喜和高兴。”\n真相四：这个“超级工具”也伴随着真实的风险 允许LLM生成并运行任意代码，无疑是一把双刃剑。第三节中提到的那种能让模型巧妙解决复杂问题的灵活性，也正是其潜在风险的来源——它们是同一枚硬币的两面。\n一个令人印象深刻的真实案例是：一个开发团队使用的智能编程代理在执行任务时，错误地生成并执行了命令 remove star.py，这导致项目目录中一个关键的Python文件被删除了。事后，那个代理还“道歉”说：“我犯了个蠢到家的错误。”\n为了防范此类风险，最佳实践至关重要：必须在安全的沙箱环境（Sandbox Environment）中运行LLM生成的代码。像Docker或E2B这样的沙箱可以将代码的执行限制在一个隔离的容器内，即使代码出现问 …"},{"title":"「吴恩达Agentic AI 模块4」5个反直觉的开发原则","url":"/courses/andrew-ng-agentic-ai/blog/lec-04/","section":"courses","date":"2025-10-17","summary":" 吴恩达分享的一套强大且反直觉的开发“操作系统”。这套理念的核心是：用严谨的分析来驱动迭代，而非凭直觉盲目构建。采纳这套方法论，是精英AI团队的战略优势，能帮你避免在无法提升性能的功能上浪费数月时间，从而大幅提升开发流程的效率与成功率。\n","description":"构建高效的AI智能体（Agentic AI）工作流是一项复杂的挑战。许多开发团队投入数周甚至数月，却发现自己陷入了低效的“埋头构建、缺少分析”的循环，最终成果寥寥。这种困境的根源，往往不是技术能力不足，而是一种低效的开发原则。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" 吴恩达分享的一套强大且反直觉的开发“操作系统”。这套理念的核心是：用严谨的分析来驱动迭代，而非凭直觉盲目构建。采纳这套方法论，是精英AI团队的战略优势，能帮你避免在无法提升性能的功能上浪费数月时间，从而大幅提升开发流程的效率与成功率。\nhttps://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 「吴恩达Agentic AI 模块4」5个反直觉的开发原则 引言 构建高效的AI智能体（Agentic AI）工作流是一项复杂的挑战。许多开发团队投入数周甚至数月，却发现自己陷入了低效的“埋头构建、缺少分析”的循环，最终成果寥寥。这种困境的根源，往往不是技术能力不足，而是一种低效的开发原则。\n本文旨在揭示由吴恩达（Andrew Ng）分享的一套强大且反直觉的开发“操作系统”。这套理念的核心是：用严谨的分析来驱动迭代，而非凭直觉盲目构建。采纳这套方法论，是精英AI团队的战略优势，能帮你避免在无法提升性能的功能上浪费数月时间，从而大幅提升开发流程的效率与成功率。\n五个反直觉的开发原则 以下五个核心开发原则挑战了我们惯常的开发直觉，但它们环环相扣，共同构成了一个高效构建AI智能体的迭代循环。\n1. 先快速搭建，再求完美 我们通常倾向于在动手前进行详尽的规划，但这在AI智能体开发中可能适得其反。吴恩达的建议是，与其花费数周去构思一个完美的系统，不如先快速构建一个“粗糙但可用”的原型。需要强调的是，“快速粗糙”不等于“不负责任”。这个初始原型必须以安全、合理的方式构建，从一开始就确保数据隐私和安全。\n这个初始系统的目的不是为了完美无瑕，而是为了快速暴露工作流中的薄弱环节。它的核心价值在于产生失败案例——这些失败案例正是你进行分析的宝贵原材料，这自然引出了我们的下一条原则。\n\u0026ldquo;I find that it\u0026rsquo;s sometimes less useful to sit around for too many weeks theorizing and hypothesizing how to build it. It\u0026rsquo;s often better to just build a quick system\u0026hellip;\u0026rdquo;\n“我发现，有时坐在那里，花太多时间理论化和假设性地思考如何构建系统，比直接构建一个快速原型更有用。通常，直接构建一个快速原型会更好。\u0026quot;\n2. 从“足够好”的评估开始 许多团队在“评估”（evals）环节感到不知所措，认为必须建立庞大而全面的测试集，这种想法常导致项目停滞。吴恩达的建议是，让第一步原型中出现的失败来指导你构建评估。\n这个过程应该是：首先，运行你的初始原型，观察其输出，发现它在哪些方面表现不佳。例如，在一个处理发票的系统中，你运行了20张发票后，发现系统经常将“开票日期”误认为“付款截止日期”。然后，针对这一个具体问题，创建一个小型的评估集（比如10到20个案例），专门追踪系统在区分日期上的准确率。这种迭代式的评估方法能有效避免“分析瘫痪”，让你立即开始衡量和驱动改进。\n\u0026ldquo;I see quite a lot of teams that are almost paralyzed because they think building evals is this massive multi-week effort, and so they take longer than would be ideal to get started.\u0026rdquo;\n“我看到很多团队因为认为构建评估是一项巨大的多周努力，而几乎瘫痪。因此，他们比理想的要慢得多才能开始。\u0026quot;\n3. 让错误分析而非直觉指引你 拥有一个小型评估集来追踪关键失败后，下一步是系统性地找出失败的根本原因。一个团队开发效率的高低，很大程度上取决于他们是否拥有严谨的错误分析流程。依赖直觉决定修复方向，往往是低效的根源。\n吴恩达用一个“研究型智能体”（research agent）的例子生动地说明了这一点。假设智能体生成的报告“遗漏了关键要点”，问题出在哪？是“生成搜索词”环节太差，是“网络搜索结果”质量不高，还是“筛选信源”的步骤出了问题？数据驱动的方法是，使用电子表格等简单工具，对一批失败案例进行追踪，统计每个中间组件的出错频率。例如，你可能会发现“搜索词生成”只有5%的情况下不理想，但“搜索结果”在45%的情况下都质量堪忧。这个数据明确告诉你，当前最值得投入精力的环节是改进网络搜索，而不是其他。\n\u0026quot; …"},{"title":"「吴恩达Agentic AI 模块4」Agentic AI 工作流开发与优化学习指南","url":"/courses/andrew-ng-agentic-ai/guides/lec-04/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 「吴恩达Agentic AI 模块4」Agentic AI 工作流开发与优化学习指南 本指南旨在帮助您复习和巩固“Agentic AI 工作流的开发与优化”课程第四模块的核心概念。内容包括简答题测验、答案解析、开放式论述题以及关键术语词汇表，全部基于提供的源材料编写。\n测验 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n为什么在开发 Agentic AI 系统时，建议首先构建一个“快速而粗糙”的原型？ 课程中提到的评估（eval）流程是怎样的？请以发票处理工作流为例进行说明。 什么是“LLM作为评判者”（LLM-as-a-judge）？在什么情况下使用它比编写代码进行评估更合适？ 请解释评估的两个维度轴，并为每个象限提供一个源材料中提到的例子。 什么是错误分析（Error Analysis）？它在优化 Agentic AI 工作流中扮演什么关键角色？ 在进行错误分析时，“追踪”（Trace）和“跨度”（Span）分别指什么？ 与端到端评估相比，组件级评估（Component-level Evals）有哪些优势？ 当一个基于LLM的组件性能不佳时，可以采取哪些方法来解决问题？ 开发者应如何培养对不同大型语言模型（LLM）能力和适用场景的直觉？ 在优化 Agentic 工作流的成本和延迟时，首要步骤是什么？这如何帮助确定优化的重点？ 答案解析 为什么在开发 Agentic AI 系统时，建议首先构建一个“快速而粗糙”的原型？ 构建一个快速原型有助于开发者快速了解系统在实际应用中的表现，识别出其有效和无效的方面。通过观察初始原型的输出，可以更有针对性地集中精力解决实际存在的问题，而不是花费数周时间进行理论化和假设，从而大大提高开发效率。 课程中提到的评估（eval）流程是怎样的？请以发票处理工作流为例进行说明。 评估流程首先是构建系统并观察输出，以发现问题，例如发票的“到期日”被错误提取。接着，创建一个小规模的评估集（如10-20张发票），并为每个样本手动标注正确答案（即“基准真相”）。最后，编写代码或提示来衡量系统输出与基准真相的一致性，从而量化改进效果。 什么是“LLM作为评判者”（LLM-as-a-judge）？在什么情况下使用它比编写代码进行评估更合适？ “LLM作为评判者”是利用一个LLM来评估另一个AI系统输出质量的方法，通常用于更主观的评估。当评估标准难以通过简单的代码（如正则表达式）来客观衡量时，它尤其有用。例如，在评估研究报告是否充分涵盖了“金标准讨论要点”时，由于表达方式多样，使用LLM来判断会比模式匹配更有效。 请解释评估的两个维度轴，并为每个象限提供一个源材料中提到的例子。 评估的两个维度轴分别是评估方法（客观代码评估 vs. 主观LLM评判）和是否有“逐例基准真相”。 代码评估 \u0026amp; 有逐例基准真相: 检查发票到期日提取是否正确，因为每张发票有不同的正确日期。 代码评估 \u0026amp; 无逐例基准真相: 检查营销文案长度是否符合10个词的限制，因为所有例子的目标都相同。 LLM评判 \u0026amp; 有逐例基准真相: 统计研究论文中提及“金标准讨论要点”的数量，因为每个主题的要点都不同。 LLM评判 \u0026amp; 无逐例基准真相: 根据通用评分标准（如坐标轴标签是否清晰）来给图表打分。 什么是错误分析（Error Analysis）？它在优化 Agentic AI 工作流中扮演什么关键角色？ 错误分析是一个系统性的过程，通过检查系统出错的案例，找出导致最终输出不满意的根本原因在于工作流中的哪个组件。它的关键作用是帮助开发团队将精力集中在最能有效提升系统整体性能的薄弱环节上，避免在收效甚微的组件上浪费时间和资源。 在进行错误分析时，“追踪”（Trace）和“跨度”（Span）分别指什么？ “追踪”（Trace）指的是一次 Agent 运行过程中所有中间步骤输出的集合，它完整记录了从输入到最终输出的全过程。而“跨度”（Span）特指单个步骤的输出。通过检查追踪记录，开发者可以了解每个组件的具体表现。 与端到端评估相比，组件级评估（Component-level Evals）有哪些优势？ 组件级评估能为特定组件的性能提供更清晰、更直接的信号，避免了整个端到端系统中其他组件随机性带来的噪声干扰。这使得开发者可以更高效地对某个特定组件（如网络搜索功能）进行调优和迭代，同时也便于分工协作，让不同团队专注于优化各自负责的模块。 当一个基于LLM的组件性能不佳时，可以采取哪些方法来解决问题？ 可以采取多种方法改进。首先是改进提示（Prompts），如增加更明确的指令或使用少样本提示（few-shot prompting）。其次是尝试不同的LLM模型，选择更适合当前任务的模型。此外，还可以将复杂的任务分解为多个更简单的步骤，或者在穷尽其他方法后，考虑对模型进行微调（fine-tuning）以获得更高性能。 开发者应如何培养对不同大型语言模型（LLM）能力和适用场景的直觉？ 开发者可以通过多种方式培养直觉。首先是经常试用不同的模型，包括闭源和开源模型，了解它们的特性。其次是大量阅读他人编写的优秀提示，甚至深入开源软件包研究其提示设计。最后，在自己的工作流中尝试替换和评估不同的模型，结合追踪记录和评估指标，积累关于模型性能、成本和速度权衡的实践经验。 在优化 Agentic 工作流的成本和延迟时，首要步骤是什么？这如何帮助确定优化的重点？ 首要步骤是对工作流的每个步骤进行基准测试（benchmarking），即测量每个组件的执行时间（延迟）和花费（成本）。通过这种量化分析，可以清晰地识别出哪些步骤是主要的耗时或成本来源。这使得优化工作可以集中在影响最大的组件上，避免在对整体性能影响不大的地方浪费精力。 开放式论述题 请思考并详细阐述以下问题，这些问题没有标准答案，旨在激发更深入的思考。\n","description":"本指南旨在帮助您复习和巩固“Agentic AI 工作流的开发与优化”课程第四模块的核心概念。内容包括简答题测验、答案解析、开放式论述题以及关键术语词汇表，全部基于提供的源材料编写。","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 「吴恩达Agentic AI 模块4」Agentic AI 工作流开发与优化学习指南 本指南旨在帮助您复习和巩固“Agentic AI 工作流的开发与优化”课程第四模块的核心概念。内容包括简答题测验、答案解析、开放式论述题以及关键术语词汇表，全部基于提供的源材料编写。\n测验 简答题 请用2-3句话回答以下每个问题，以检验您对核心概念的理解。\n为什么在开发 Agentic AI 系统时，建议首先构建一个“快速而粗糙”的原型？ 课程中提到的评估（eval）流程是怎样的？请以发票处理工作流为例进行说明。 什么是“LLM作为评判者”（LLM-as-a-judge）？在什么情况下使用它比编写代码进行评估更合适？ 请解释评估的两个维度轴，并为每个象限提供一个源材料中提到的例子。 什么是错误分析（Error Analysis）？它在优化 Agentic AI 工作流中扮演什么关键角色？ 在进行错误分析时，“追踪”（Trace）和“跨度”（Span）分别指什么？ 与端到端评估相比，组件级评估（Component-level Evals）有哪些优势？ 当一个基于LLM的组件性能不佳时，可以采取哪些方法来解决问题？ 开发者应如何培养对不同大型语言模型（LLM）能力和适用场景的直觉？ 在优化 Agentic 工作流的成本和延迟时，首要步骤是什么？这如何帮助确定优化的重点？ 答案解析 为什么在开发 Agentic AI 系统时，建议首先构建一个“快速而粗糙”的原型？ 构建一个快速原型有助于开发者快速了解系统在实际应用中的表现，识别出其有效和无效的方面。通过观察初始原型的输出，可以更有针对性地集中精力解决实际存在的问题，而不是花费数周时间进行理论化和假设，从而大大提高开发效率。 课程中提到的评估（eval）流程是怎样的？请以发票处理工作流为例进行说明。 评估流程首先是构建系统并观察输出，以发现问题，例如发票的“到期日”被错误提取。接着，创建一个小规模的评估集（如10-20张发票），并为每个样本手动标注正确答案（即“基准真相”）。最后，编写代码或提示来衡量系统输出与基准真相的一致性，从而量化改进效果。 什么是“LLM作为评判者”（LLM-as-a-judge）？在什么情况下使用它比编写代码进行评估更合适？ “LLM作为评判者”是利用一个LLM来评估另一个AI系统输出质量的方法，通常用于更主观的评估。当评估标准难以通过简单的代码（如正则表达式）来客观衡量时，它尤其有用。例如，在评估研究报告是否充分涵盖了“金标准讨论要点”时，由于表达方式多样，使用LLM来判断会比模式匹配更有效。 请解释评估的两个维度轴，并为每个象限提供一个源材料中提到的例子。 评估的两个维度轴分别是评估方法（客观代码评估 vs. 主观LLM评判）和是否有“逐例基准真相”。 代码评估 \u0026amp; 有逐例基准真相: 检查发票到期日提取是否正确，因为每张发票有不同的正确日期。 代码评估 \u0026amp; 无逐例基准真相: 检查营销文案长度是否符合10个词的限制，因为所有例子的目标都相同。 LLM评判 \u0026amp; 有逐例基准真相: 统计研究论文中提及“金标准讨论要点”的数量，因为每个主题的要点都不同。 LLM评判 \u0026amp; 无逐例基准真相: 根据通用评分标准（如坐标轴标签是否清晰）来给图表打分。 什么是错误分析（Error Analysis）？它在优化 Agentic AI 工作流中扮演什么关键角色？ 错误分析是一个系统性的过程，通过检查系统出错的案例，找出导致最终输出不满意的根本原因在于工作流中的哪个组件。它的关键作用是帮助开发团队将精力集中在最能有效提升系统整体性能的薄弱环节上，避免在收效甚微的组件上浪费时间和资源。 在进行错误分析时，“追踪”（Trace）和“跨度”（Span）分别指什么？ “追踪”（Trace）指的是一次 Agent 运行过程中所有中间步骤输出的集合，它完整记录了从输入到最终输出的全过程。而“跨度”（Span）特指单个步骤的输出。通过检查追踪记录，开发者可以了解每个组件的具体表现。 与端到端评估相比，组件级评估（Component-level Evals）有哪些优势？ 组件级评估能为特定组件的性能提供更清晰、更直接的信号，避免了整个端到端系统中其他组件随机性带来的噪声干扰。这使得开发者可以更高效地对某个特定组件（如网络搜索功能）进行调优和迭代，同时也便于分工协作，让不同团队专注于优化各自负责的模块。 当一个基于LLM的组件性能不 …"},{"title":"「吴恩达Agentic AI 模块5」4个自主AI智能体构建模式:从指令执行者到战略家","url":"/courses/andrew-ng-agentic-ai/blog/lec-05/","section":"courses","date":"2025-10-17","summary":" 本文将从吴恩达（Andrew Ng）的课程中提炼出四个最具影响力的设计模式，帮助你理解如何构建这些高度自主的智能体。这些模式将彻底改变我们对AI能力的认知，并为开发者开辟了全新的可能性。\n","description":"我们大多数人与AI助手的互动经验，都停留在下达一次性指令并获得相应结果的模式上。我们让它写一封邮件、总结一篇文章或者回答一个简单的问题。这些AI系统虽然强大，但本质上是被动的指令执行者，等待着我们的下一步指示。","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" 本文将从吴恩达（Andrew Ng）的课程中提炼出四个最具影响力的设计模式，帮助你理解如何构建这些高度自主的智能体。这些模式将彻底改变我们对AI能力的认知，并为开发者开辟了全新的可能性。\nhttps://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 「吴恩达Agentic AI 模块5」4个自主AI智能体构建模式:从指令执行者到战略家 引言：从指令跟随者到战略思考者 我们大多数人与AI助手的互动经验，都停留在下达一次性指令并获得相应结果的模式上。我们让它写一封邮件、总结一篇文章或者回答一个简单的问题。这些AI系统虽然强大，但本质上是被动的指令执行者，等待着我们的下一步指示。\n然而，AI领域正在发生一场深刻的变革。新一代的“智能体AI”（Agentic AI）正在崛起，它们不再仅仅是指令的接收者，而是能够自主规划、制定策略并执行复杂多步骤任务的战略思考者。这种AI能够像人类一样，为了达成一个宏观目标而自主地分解任务、调用工具并协同工作。\n本文将从吴恩达（Andrew Ng）的课程中提炼出四个最具影响力的设计模式，帮助你理解如何构建这些高度自主的智能体。这些模式将彻底改变我们对AI能力的认知，并为开发者开辟了全新的可能性。\n1. 让AI自己写“待办清单”：规划模式 (Planning Pattern) 规划设计模式的核心思想很简单却极具颠覆性：开发者不再需要为AI硬编码任务执行的每一步顺序，而是让大语言模型（LLM）自己生成一个实现目标的详细步骤计划。\n以一个太阳镜零售店的客服智能体为例。当顾客提出一个复杂问题，比如“你们有库存的圆形太阳镜吗？价格要低于100美元。”在规划模式下，智能体会首先生成一个行动计划，可能包含以下步骤：获取商品描述以筛选出圆形太阳镜，然后检查库存确认是否有货，最后获取商品价格以判断是否低于100美元。接着，系统会条不紊地执行这个计划，将第一步的输出（例如，圆形太阳镜的列表）作为第二步的输入，再将第二步的结果（有库存的圆形太阳镜）作为第三步的输入，最终整合所有信息，生成给用户的最终答复。\n这种转变意义重大。它赋予了AI极大的灵活性和自主性，使其能够处理各种预料之外的任务组合，而无需开发者为每一种可能性都预先设计工作流。这标志着我们从“编程”AI的行为转向“引导”AI自主规划其行为。\n“构建能够自我规划的智能体，最酷的一点在于，你无需预先硬编码大语言模型为完成复杂任务可能采取的确切步骤顺序。”\n不过，作为 strategist，我们也必须认识到该模式的现状。目前，除了在自主编程等领域应用得非常成功外，规划模式在许多其他应用中仍处于“实验阶段”，尚未得到非常广泛的使用。其主要挑战在于可控性——由于开发者在运行时无法预知AI会生成什么样的计划，这使得系统行为变得“难以控制”。尽管如此，这项技术仍在不断成熟，潜力巨大。\n2. 终极规划是代码：让智能体为自己编程 传统的基于工具的方法有一个明显的问题。例如，在处理一个包含咖啡销售数据的电子表格时，如果用户的查询越来越复杂，开发者可能会发现自己需要创建无数个特定的工具（如“获取最大值”、“筛选行”等）。这种方法不仅效率低下，而且非常脆弱，无法应对无穷无尽的新需求。\n一个更强大且反直觉的解决方案是：让LLM直接编写并执行代码（例如，使用Python及其pandas库）来解决用户的查询。这种“代码即行动”（code as action）的模式，将规划提升到了一个全新的维度。\n这种方法之所以如此高效，是因为LLM在其训练数据中已经学习了数千个编程语言和库函数的用法。通过编写代码，AI可以从这个庞大的函数库中自由组合，创造出比简单地串联几个预定义工具远为丰富和复杂的执行计划。研究论文也证实，让模型通过编写代码来制定计划，其性能始终优于那些用JSON或纯文本制定计划的模型。\n“通过让你的大语言模型编写代码，它可以从成百上千个它已经见过大量数据并知道何时使用的相关函数中进行选择。这使得它能够从这个非常庞大的库中将不同的函数调用串联起来，从而为回答像这样相当复杂的查询制定出计划。”\n3. 告别“超级智能体”，构建AI“团队” 当我们面对一个极其复杂的任务时，与其试图构建一个无所不能的“超级智能体”，不如借鉴现实世界中的团队协作模式。这个核心理念就是“多智能体工作流”（multi-agent workflow）。\n开发者可以像招聘一个人类团队一样，将一个大任务分解成多个专门的角色。以制作市场营销手册为例，你可以创建三个独立的智能体：一个研究员智能体，负责分析市场趋势；一个平面设计师智能体，负责生成 …"},{"title":"「吴恩达Agentic AI 模块5」高度自主智能体AI模式学习指南","url":"/courses/andrew-ng-agentic-ai/guides/lec-05/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 「吴恩达Agentic AI 模块5」高度自主智能体AI模式学习指南 本学习指南旨在帮助您深入理解并掌握构建高度自主智能体AI的核心设计模式。内容涵盖了规划工作流、多智能体系统以及它们在实际应用中的实现方式。\n测验 简答题 请使用2-3句话回答以下问题，以检验您对核心概念的理解。\n在智能体AI中，“规划”设计模式的核心思想是什么？ 为什么让大型语言模型（LLM）以JSON或XML等结构化格式输出其计划是有益的？ 什么是“通过代码执行进行规划”？在什么情况下它特别有效？ 根据源材料，使用规划模式的一个主要挑战或缺点是什么？ 什么是多智能体工作流，其背后的核心理念是什么？ 在多智能体系统中，单个智能体通常是如何被创建的？ 请描述多智能体系统中的“线性”沟通模式。 请描述多智能体系统中的“层级式”沟通模式。 根据所提供的研究，将代码作为行动（规划）与使用JSON或纯文本相比，效果如何？ 材料中用什么类比来解释多智能体系统（即使在单台计算机上运行）的价值？ 答案解析 在智能体AI中，“规划”设计模式的核心思想是什么？ “规划”设计模式允许智能体AI灵活地自行决定完成任务所需的步骤顺序，而无需开发者预先硬编码步骤。智能体会首先生成一个多步骤的计划，然后逐一执行计划中的每个步骤，以响应复杂的用户请求。 为什么让大型语言模型（LLM）以JSON或XML等结构化格式输出其计划是有益的？ 使用JSON或XML等结构化格式，可以使下游代码以清晰、无歧义的方式解析计划的具体步骤。这确保了计划的每个部分（如步骤描述、使用的工具和参数）都能被系统可靠地、系统化地逐一执行。 什么是“通过代码执行进行规划”？在什么情况下它特别有效？ “通过代码执行进行规划”是指让LLM直接编写代码来表达和执行一个复杂的计划，而不是输出JSON等格式的步骤列表。当任务可以通过编程逻辑完成时，这种方法尤其强大，因为它允许LLM利用编程语言和库中成百上千个现有函数，从而能处理比预定义工具集更广泛、更复杂的查询。 根据源材料，使用规划模式的一个主要挑战或缺点是什么？ 使用规划模式的主要挑战是系统有时会变得难以控制。由于开发者无法预先知道智能体在运行时会生成什么样的计划，因此系统的行为会变得更难预测，这给控制带来了困难。 什么是多智能体工作流，其背后的核心理念是什么？ 多智能体工作流是指让多个智能体协作完成一项任务，而不是依赖单个智能体。其核心理念是将一个复杂的任务分解为多个子任务，并为每个子任务指派一个具有特定角色和技能的智能体，就像组建一个人类团队来分工合作一样。 在多智能体系统中，单个智能体通常是如何被创建的？ 单个智能体通常是通过向大型语言模型（LLM）提供特定的提示（prompting）来创建的。提示会指示LLM扮演一个特定的角色（如研究员、图形设计师或作者），并赋予其完成该角色任务所需的工具和背景信息。 请描述多智能体系统中的“线性”沟通模式。 线性沟通模式是一种工作流，其中智能体按顺序逐一完成其工作。第一个智能体完成任务后，将其输出传递给第二个智能体，第二个智能体再将其输出传递给第三个，以此类推，直到最终任务完成。例如，研究员先工作，然后是图形设计师，最后是作者。 请描述多智能体系统中的“层级式”沟通模式。 层级式沟通模式涉及一个“管理者”智能体，它负责协调和委派任务给其他多个“团队成员”智能体。管理者智能体制定计划，将具体任务分配给下属智能体，并接收它们的工作成果，然后决定下一步行动，形成一种自上而下的协调结构。 根据所提供的研究，将代码作为行动（规划）与使用JSON或纯文本相比，效果如何？ 研究表明，让LLM通过编写代码来表达计划和执行动作，其效果优于让它编写JSON或纯文本格式的计划。总体趋势是，代码规划优于JSON规划，而JSON规划又稍好于纯文本规划。 材料中用什么类比来解释多智能体系统（即使在单台计算机上运行）的价值？ 材料中使用的类比是计算机中的多进程或多线程。尽管计算机只有一个CPU，但开发者将工作分解为多个进程或线程，可以更容易地编写和管理复杂的程序。同样，将智能体任务分解为多个智能体，为开发者提供了一个有用的心智框架，可以更轻松地将复杂任务分解为可管理的子任务。 论述题 请思考并详细阐述以下问题，以深化您对相关概念的综合理解。\n","description":"本学习指南旨在帮助您深入理解并掌握构建高度自主智能体AI的核心设计模式。内容涵盖了规划工作流、多智能体系统以及它们在实际应用中的实现方式。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 「吴恩达Agentic AI 模块5」高度自主智能体AI模式学习指南 本学习指南旨在帮助您深入理解并掌握构建高度自主智能体AI的核心设计模式。内容涵盖了规划工作流、多智能体系统以及它们在实际应用中的实现方式。\n测验 简答题 请使用2-3句话回答以下问题，以检验您对核心概念的理解。\n在智能体AI中，“规划”设计模式的核心思想是什么？ 为什么让大型语言模型（LLM）以JSON或XML等结构化格式输出其计划是有益的？ 什么是“通过代码执行进行规划”？在什么情况下它特别有效？ 根据源材料，使用规划模式的一个主要挑战或缺点是什么？ 什么是多智能体工作流，其背后的核心理念是什么？ 在多智能体系统中，单个智能体通常是如何被创建的？ 请描述多智能体系统中的“线性”沟通模式。 请描述多智能体系统中的“层级式”沟通模式。 根据所提供的研究，将代码作为行动（规划）与使用JSON或纯文本相比，效果如何？ 材料中用什么类比来解释多智能体系统（即使在单台计算机上运行）的价值？ 答案解析 在智能体AI中，“规划”设计模式的核心思想是什么？ “规划”设计模式允许智能体AI灵活地自行决定完成任务所需的步骤顺序，而无需开发者预先硬编码步骤。智能体会首先生成一个多步骤的计划，然后逐一执行计划中的每个步骤，以响应复杂的用户请求。 为什么让大型语言模型（LLM）以JSON或XML等结构化格式输出其计划是有益的？ 使用JSON或XML等结构化格式，可以使下游代码以清晰、无歧义的方式解析计划的具体步骤。这确保了计划的每个部分（如步骤描述、使用的工具和参数）都能被系统可靠地、系统化地逐一执行。 什么是“通过代码执行进行规划”？在什么情况下它特别有效？ “通过代码执行进行规划”是指让LLM直接编写代码来表达和执行一个复杂的计划，而不是输出JSON等格式的步骤列表。当任务可以通过编程逻辑完成时，这种方法尤其强大，因为它允许LLM利用编程语言和库中成百上千个现有函数，从而能处理比预定义工具集更广泛、更复杂的查询。 根据源材料，使用规划模式的一个主要挑战或缺点是什么？ 使用规划模式的主要挑战是系统有时会变得难以控制。由于开发者无法预先知道智能体在运行时会生成什么样的计划，因此系统的行为会变得更难预测，这给控制带来了困难。 什么是多智能体工作流，其背后的核心理念是什么？ 多智能体工作流是指让多个智能体协作完成一项任务，而不是依赖单个智能体。其核心理念是将一个复杂的任务分解为多个子任务，并为每个子任务指派一个具有特定角色和技能的智能体，就像组建一个人类团队来分工合作一样。 在多智能体系统中，单个智能体通常是如何被创建的？ 单个智能体通常是通过向大型语言模型（LLM）提供特定的提示（prompting）来创建的。提示会指示LLM扮演一个特定的角色（如研究员、图形设计师或作者），并赋予其完成该角色任务所需的工具和背景信息。 请描述多智能体系统中的“线性”沟通模式。 线性沟通模式是一种工作流，其中智能体按顺序逐一完成其工作。第一个智能体完成任务后，将其输出传递给第二个智能体，第二个智能体再将其输出传递给第三个，以此类推，直到最终任务完成。例如，研究员先工作，然后是图形设计师，最后是作者。 请描述多智能体系统中的“层级式”沟通模式。 层级式沟通模式涉及一个“管理者”智能体，它负责协调和委派任务给其他多个“团队成员”智能体。管理者智能体制定计划，将具体任务分配给下属智能体，并接收它们的工作成果，然后决定下一步行动，形成一种自上而下的协调结构。 根据所提供的研究，将代码作为行动（规划）与使用JSON或纯文本相比，效果如何？ 研究表明，让LLM通过编写代码来表达计划和执行动作，其效果优于让它编写JSON或纯文本格式的计划。总体趋势是，代码规划优于JSON规划，而JSON规划又稍好于纯文本规划。 材料中用什么类比来解释多智能体系统（即使在单台计算机上运行）的价值？ 材料中使用的类比是计算机中的多进程或多线程。尽管计算机只有一个CPU，但开发者将工作分解为多个进程或线程，可以更容易地编写和管理复杂的程序。同样，将智能体任务分解为多个智能体，为开发者提供了一个有用的心智框架，可以更轻松地将复杂任务分解为可管理的子任务。 论述题 请思考并详细阐述以下问题，以深化您对相关概念的综合理解。\n论述智能体AI中“规划”模式的演变过程，从简单的文本文本计划，到JSON/XML格式，再到最终的通过代码执行进行规划。分析每个阶段的优势和局限性。 比较并对比“规划”设计模式 …"},{"title":"「吴恩达Agentic AI模块2简报」反思设计模式","url":"/courses/andrew-ng-agentic-ai/insights/lec-02/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/3c2ef484-a8c3-443f-bb90-11b8ae8904d9 「吴恩达Agentic AI模块2简报」反思设计模式 概述 本文档对“反思设计模式”（Reflection Design Pattern）进行了深入分析，该模式是提升大型语言模型（LLM）输出质量的关键技术。核心观点认为，通过一个两步流程——首先生成初步草稿，然后提示模型对该草稿进行批判性审视和改进——可以显著增强最终结果的准确性、完整性和质量。\n关键洞察 性能优势： 多项研究和实践表明，与一次性直接生成（或称“零样本提示”）相比，反思模式在各种任务中都能带来显著的性能提升，包括代码编写、结构化数据生成和创意写作。 外部反馈的力量： 反思模式最强大的应用形式是整合外部反馈。当模型不仅依赖自身知识，还能接收来自外部工具（如代码执行器、网络搜索或简单的验证脚本）的新信息时，其改进能力会得到指数级增强。这种方法能有效克服单纯提示工程所面临的性能瓶颈。 系统的评估至关重要： 由于反思会增加系统的复杂性和延迟，因此必须通过严格的评估来验证其有效性。对于有明确对错标准的任务，应采用基于“基准真相”数据的客观评估。对于主观任务（如评估图表美观度），基于评分细则（Rubric）的评估比直接让模型比较优劣更为可靠和一致，有效避免了位置偏见等问题。 提示词工程： 高效的反思依赖于精心设计的提示词。反思提示词应明确指示模型进行“审阅”或“反思”，并提供清晰、具体的评估标准（例如，“检查语气”、“验证事实”、“是否易于发音”），以引导模型进行有针对性的改进。 总之，反思设计模式不仅仅是一种简单的技巧，更是一种系统化的工作流，它将批判性思维和外部验证融入到人工智能生成过程中，从而实现更高水平的性能和可靠性。\n一、反思设计模式的核心概念 反思设计模式模仿了人类通过审阅和修改来改进工作的过程。它将一个任务分解为两个主要步骤，让大型语言模型（LLM）能够批判性地评估并优化其自身的输出。\n核心工作流程：\n初始生成（版本1）： 首先，向LLM提供一个提示，要求其生成任务的初步草案。这可以是任何类型的输出，例如一封电子邮件、一段代码或一篇短文。 反思与改进（版本2）： 接着，将生成的版本1草稿作为输入，连同一个新的“反思提示词”，再次提交给同一个或另一个LLM。这个新的提示词会引导模型扮演一个批判者的角色，根据特定标准检查草稿，并生成一个经过改进的版本2。 应用实例：\n电子邮件撰写： 版本1： 快速生成的邮件草稿可能包含拼写错误、模糊的日期（如“下个月”）且忘记署名。 反思： 提示模型检查邮件的清晰度、准确性和完整性。 版本2： 改进后的邮件将日期具体化（如“5号到7号”），修正错误并添加署名。 代码生成： 版本1： LLM生成的初始代码可能存在逻辑错误或语法问题。 反思： 提示模型检查代码是否存在缺陷（bugs）。 版本2： 修正了错误，功能更完善的代码。 模型选择策略： 在实现这一模式时，开发者可以选择不同的模型组合。例如，可以使用一个模型进行初始生成，而使用另一个具有强大推理能力的“思考模型”（Reasoning Model）来进行反思和纠错，因为后者在发现代码缺陷等任务上通常表现更佳。\n","description":"本文档对“反思设计模式”（Reflection Design Pattern）进行了深入分析，该模式是提升大型语言模型（LLM）输出质量的关键技术。核心观点认为，通过一个两步流程——首先生成初步草稿，然后提示模型对该草稿进行批判性审视和改进——可以显著增强最终结果的准确性、完整性和质量。","tags":["Agentic AI"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/3c2ef484-a8c3-443f-bb90-11b8ae8904d9 「吴恩达Agentic AI模块2简报」反思设计模式 概述 本文档对“反思设计模式”（Reflection Design Pattern）进行了深入分析，该模式是提升大型语言模型（LLM）输出质量的关键技术。核心观点认为，通过一个两步流程——首先生成初步草稿，然后提示模型对该草稿进行批判性审视和改进——可以显著增强最终结果的准确性、完整性和质量。\n关键洞察 性能优势： 多项研究和实践表明，与一次性直接生成（或称“零样本提示”）相比，反思模式在各种任务中都能带来显著的性能提升，包括代码编写、结构化数据生成和创意写作。 外部反馈的力量： 反思模式最强大的应用形式是整合外部反馈。当模型不仅依赖自身知识，还能接收来自外部工具（如代码执行器、网络搜索或简单的验证脚本）的新信息时，其改进能力会得到指数级增强。这种方法能有效克服单纯提示工程所面临的性能瓶颈。 系统的评估至关重要： 由于反思会增加系统的复杂性和延迟，因此必须通过严格的评估来验证其有效性。对于有明确对错标准的任务，应采用基于“基准真相”数据的客观评估。对于主观任务（如评估图表美观度），基于评分细则（Rubric）的评估比直接让模型比较优劣更为可靠和一致，有效避免了位置偏见等问题。 提示词工程： 高效的反思依赖于精心设计的提示词。反思提示词应明确指示模型进行“审阅”或“反思”，并提供清晰、具体的评估标准（例如，“检查语气”、“验证事实”、“是否易于发音”），以引导模型进行有针对性的改进。 总之，反思设计模式不仅仅是一种简单的技巧，更是一种系统化的工作流，它将批判性思维和外部验证融入到人工智能生成过程中，从而实现更高水平的性能和可靠性。\n一、反思设计模式的核心概念 反思设计模式模仿了人类通过审阅和修改来改进工作的过程。它将一个任务分解为两个主要步骤，让大型语言模型（LLM）能够批判性地评估并优化其自身的输出。\n核心工作流程：\n初始生成（版本1）： 首先，向LLM提供一个提示，要求其生成任务的初步草案。这可以是任何类型的输出，例如一封电子邮件、一段代码或一篇短文。 反思与改进（版本2）： 接着，将生成的版本1草稿作为输入，连同一个新的“反思提示词”，再次提交给同一个或另一个LLM。这个新的提示词会引导模型扮演一个批判者的角色，根据特定标准检查草稿，并生成一个经过改进的版本2。 应用实例：\n电子邮件撰写： 版本1： 快速生成的邮件草稿可能包含拼写错误、模糊的日期（如“下个月”）且忘记署名。 反思： 提示模型检查邮件的清晰度、准确性和完整性。 版本2： 改进后的邮件将日期具体化（如“5号到7号”），修正错误并添加署名。 代码生成： 版本1： LLM生成的初始代码可能存在逻辑错误或语法问题。 反思： 提示模型检查代码是否存在缺陷（bugs）。 版本2： 修正了错误，功能更完善的代码。 模型选择策略： 在实现这一模式时，开发者可以选择不同的模型组合。例如，可以使用一个模型进行初始生成，而使用另一个具有强大推理能力的“思考模型”（Reasoning Model）来进行反思和纠错，因为后者在发现代码缺陷等任务上通常表现更佳。\n二、反思与直接生成的性能对比 直接生成（Direct Generation），也称为“零样本提示”（Zero-shot Prompting），是指在不提供任何示例的情况下，仅通过一条指令让LLM一次性完成任务。虽然这种方法简单快捷，但研究表明，反思模式在多种应用场景下都能提供更优越的性能。\n根据Madaan等人的研究论文，在涵盖GPT-3.5和GPT-4等多种模型的实验中，使用反思模式（深色条）的性能普遍显著高于仅使用零样本提示（浅色条）的性能。\n反思模式的典型应用场景：\n应用领域 问题描述 反思任务 结构化数据生成 生成的HTML表格或复杂的嵌套JSON可能存在格式错误。 验证输出格式的正确性，并进行修正。 指令序列生成 生成的操作指南（如泡茶步骤）可能遗漏关键步骤。 检查指令的连贯性和完整性，补充缺失环节。 创意命名 头脑风暴出的域名可能存在意外的负面含义或难以发音。 检查名称是否有不当联想、负面含义，以及是否易于发音。 商业邮件 邮件草稿的语气可能不当，或事实、日期存在错误。 检查邮件的语气，并核实验中提到的所有事实、日期和承诺。 编写反思提示词的最佳实践：\n明确意图： 在提示词中清晰地使用“审阅”、“反思”或“批判”等词语，明确指示模型进入评估模式。 提供具体标准： 给出清晰的评估准则。例如，要求模型检查“清晰度、可 …"},{"title":"「吴恩达Agentic AI模块3简报」工具使用与模型上下文协议（MCP）","url":"/courses/andrew-ng-agentic-ai/insights/lec-03/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 「吴恩达Agentic AI模块3简报」工具使用与模型上下文协议（MCP） 概述 本简报综合分析了大型语言模型（LLM）通过“工具使用”实现代理式（Agentic）能力的核心概念、实现机制和未来趋势。关键洞察如下：\n工具是LLM能力的延伸：工具是开发者提供给LLM调用的外部函数，使其能够超越预训练知识的限制。这使得LLM能够执行获取实时信息、查询数据库、进行网络搜索和执行复杂计算等任务，从而极大地增强了应用的实用性。 代理式决策是核心：工具使用的关键在于LLM能够自主决定是否、何时以及调用哪个工具来完成任务。这种非硬编码的决策过程是实现代理式工作流的基础，其标准流程为：LLM接收提示 -\u0026gt; 决定调用工具 -\u0026gt; 外部代码执行工具并返回结果 -\u0026gt; 结果反馈给LLM -\u0026gt; LLM生成最终输出。 实现方式的演进：工具调用的实现已从早期依赖特定提示词（如输出“FUNCTION: function_name”）的繁琐方法，演变为现代LLM原生的、基于特定语法的调用方式。诸如AISuite之类的库通过自动从函数文档字符串生成JSON Schema来描述工具，进一步简化了开发流程，使开发者能更便捷地集成工具。 代码执行是最强大的工具之一：为LLM提供一个通用的代码执行工具，比创建无数个特定功能的工具更具灵活性和扩展性。LLM能够通过编写和执行代码来解决复杂的数学、逻辑和数据处理问题。然而，这也带来了严峻的安全风险。最佳实践是在沙盒环境（如Docker）中执行代码，以防止潜在的数据丢失或系统损坏。 模型上下文协议（MCP）推动生态标准化：MCP是由Anthropic提出并被广泛采用的一项新标准，旨在解决开发者在集成各类API（如Slack、GitHub）时重复造轮子的问题。它通过定义统一的客户端（应用）与服务端（工具提供者）交互协议，将社区开发工作量从M×N（应用×工具）的复杂性降低到M+N，极大地促进了一个可共享、可复用的工具生态系统的形成。 工具使用的核心概念与价值 工具使用是赋予LLM代理能力、使其能够与外部世界交互并执行具体行动的关键机制。它将LLM从一个单纯的文本生成器转变为一个能够主动解决问题的智能体。\n1.1 什么是工具？ 在LLM的语境下，“工具”是指开发者编写并提供给模型使用的外部代码函数。当LLM认为需要执行某个操作或获取特定信息时，它不会直接执行代码，而是会请求调用这些预先定义的函数。\n功能示例： 信息获取：getCurrentTime() 获取当前时间，网络搜索工具获取最新资讯。 数据操作：queryDatabase() 从销售数据库中查询特定客户信息。 计算任务：calculateInterest() 执行精确的金融计算。 系统交互：makeAppointment() 在日历中创建会议邀请。 1.2 工具使用的代理式决策过程 工具使用的核心特征在于其**代理式（Agentic）**本质，即LLM根据上下文自主决策，而非由开发者硬编码指定何时调用函数。\n","description":"本简报综合分析了大型语言模型（LLM）通过“工具使用”实现代理式（Agentic）能力的核心概念、实现机制和未来趋势。关键洞察如下：","tags":["Agentic AI"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/5f94ecc3-499e-4be2-8c8f-eadc42dda1ad 「吴恩达Agentic AI模块3简报」工具使用与模型上下文协议（MCP） 概述 本简报综合分析了大型语言模型（LLM）通过“工具使用”实现代理式（Agentic）能力的核心概念、实现机制和未来趋势。关键洞察如下：\n工具是LLM能力的延伸：工具是开发者提供给LLM调用的外部函数，使其能够超越预训练知识的限制。这使得LLM能够执行获取实时信息、查询数据库、进行网络搜索和执行复杂计算等任务，从而极大地增强了应用的实用性。 代理式决策是核心：工具使用的关键在于LLM能够自主决定是否、何时以及调用哪个工具来完成任务。这种非硬编码的决策过程是实现代理式工作流的基础，其标准流程为：LLM接收提示 -\u0026gt; 决定调用工具 -\u0026gt; 外部代码执行工具并返回结果 -\u0026gt; 结果反馈给LLM -\u0026gt; LLM生成最终输出。 实现方式的演进：工具调用的实现已从早期依赖特定提示词（如输出“FUNCTION: function_name”）的繁琐方法，演变为现代LLM原生的、基于特定语法的调用方式。诸如AISuite之类的库通过自动从函数文档字符串生成JSON Schema来描述工具，进一步简化了开发流程，使开发者能更便捷地集成工具。 代码执行是最强大的工具之一：为LLM提供一个通用的代码执行工具，比创建无数个特定功能的工具更具灵活性和扩展性。LLM能够通过编写和执行代码来解决复杂的数学、逻辑和数据处理问题。然而，这也带来了严峻的安全风险。最佳实践是在沙盒环境（如Docker）中执行代码，以防止潜在的数据丢失或系统损坏。 模型上下文协议（MCP）推动生态标准化：MCP是由Anthropic提出并被广泛采用的一项新标准，旨在解决开发者在集成各类API（如Slack、GitHub）时重复造轮子的问题。它通过定义统一的客户端（应用）与服务端（工具提供者）交互协议，将社区开发工作量从M×N（应用×工具）的复杂性降低到M+N，极大地促进了一个可共享、可复用的工具生态系统的形成。 工具使用的核心概念与价值 工具使用是赋予LLM代理能力、使其能够与外部世界交互并执行具体行动的关键机制。它将LLM从一个单纯的文本生成器转变为一个能够主动解决问题的智能体。\n1.1 什么是工具？ 在LLM的语境下，“工具”是指开发者编写并提供给模型使用的外部代码函数。当LLM认为需要执行某个操作或获取特定信息时，它不会直接执行代码，而是会请求调用这些预先定义的函数。\n功能示例： 信息获取：getCurrentTime() 获取当前时间，网络搜索工具获取最新资讯。 数据操作：queryDatabase() 从销售数据库中查询特定客户信息。 计算任务：calculateInterest() 执行精确的金融计算。 系统交互：makeAppointment() 在日历中创建会议邀请。 1.2 工具使用的代理式决策过程 工具使用的核心特征在于其**代理式（Agentic）**本质，即LLM根据上下文自主决策，而非由开发者硬编码指定何时调用函数。\n决策自主性：当面对一个问题时，LLM会评估其可用的工具集。 如果问题（如“绿茶中有多少咖啡因？”）能直接用其内部知识回答，它将不会调用任何工具。 如果问题（如“现在几点了？”）需要外部信息，它会决定调用最合适的工具（如getCurrentTime()）。 标准工作流程： 输入提示：用户向LLM提出请求。 LLM决策与请求：LLM分析请求，并从可用工具集中选择一个或多个进行调用。它生成一个符合特定格式的请求。 工具执行与结果返回：开发者编写的外部代码捕获LLM的请求，执行相应的函数，并将函数的返回值（如具体时间“下午3:20”）获取。 结果反馈与最终生成：函数返回值被送回LLM，作为其对话历史的一部分。LLM基于这个新信息，生成最终的、对用户更有帮助的回答。 这一流程可以连续进行，LLM在一个任务中可能会按顺序调用多个工具，例如先通过checkCalendar查找空闲时间，再通过makeAppointment创建日程。\n2. 实现工具调用的机制 工具调用的技术实现经历了从手动、繁琐到自动化、标准化的演进。\n2.1 早期实现方式：基于提示词的指令 在LLM原生支持工具调用之前，开发者需要通过详细的系统提示（System Prompt）来“教”会模型如何请求工具。\n机制：开发者在提示词中明确规定一种特殊格式，例如：“如果你需要调用getCurrentTime工具，请输出文本‘FUNCTION: …"},{"title":"「吴恩达Agentic AI模块4简报」工作流开发与优化","url":"/courses/andrew-ng-agentic-ai/insights/lec-04/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 「吴恩达Agentic AI模块4简报」工作流开发与优化 概要 本简报深入剖析了构建和优化 Agentic AI 工作流的系统化、迭代化流程。核心理念是避免长时间的理论构思，而是通过快速构建一个“粗糙但可用”的初始系统来启动开发周期。该流程强调在“构建”和“分析”之间进行持续循环，通过严谨的评估与错误分析来指导开发方向，从而实现高效、有针对性的系统改进。\n关键要点 迭代式开发循环： 成功的 Agentic AI 系统开发并非线性过程，而是一个在构建、评估、分析和改进之间不断循环的迭代过程。首先构建一个基本原型，然后通过分析其输出来发现问题，从而为下一步的构建工作指明方向。 评估（Evals）是核心驱动力： 建立评估体系是衡量进展和驱动改进的关键。评估应从简单的“端到端评估”开始，针对系统在特定方面的不足（如日期提取不准、文本长度超标）创建小规模的测试集（例如 10-20 个样本），以量化改进效果。 系统的错误分析： 为了确定应优先处理哪个系统组件，必须进行系统的错误分析。开发者应专注于系统出错的案例，审查每个步骤的“迹线”（traces），即中间输出，并使用电子表格等工具统计每个组件导致错误的频率。这能以数据驱动的方式揭示真正的瓶颈，避免凭直觉做出耗时且无效的决策。 组件级评估的重要性： 当错误分析指向特定组件时，建立“组件级评估”能极大提升优化效率。它能为该组件提供一个清晰、无干扰的性能信号，使得开发者可以快速迭代调整（如更换API、调整超参数），而无需每次都运行完整且昂贵的端到端评估。 优化策略的多样性： 针对不同组件的问题，需要采用不同的解决策略。对于非 LLM 组件，可以通过调整参数或更换工具来改进；对于基于 LLM 的组件，方法包括优化提示词、更换模型、分解任务，乃至在必要时进行模型微调。 成本与延迟的后期优化： 在系统输出质量达标之前，成本和延迟应是次要考虑因素。一旦系统性能稳定，可通过对工作流各环节进行基准测试，精确找出成本和时间开销最大的部分，然后有针对性地进行优化。 总之，高效的 Agentic AI 工作流开发依赖于一套严谨的分析方法论，它能将开发者的精力引导至最能提升系统整体性能的地方。\n","description":"本简报深入剖析了构建和优化 Agentic AI 工作流的系统化、迭代化流程。核心理念是避免长时间的理论构思，而是通过快速构建一个“粗糙但可用”的初始系统来启动开发周期。该流程强调在“构建”和“分析”之间进行持续循环，通过严谨的评估与错误分析来指导开发方向，从而实现高效、有针对性的系统改进。","tags":["Agentic AI","Andrew Ng"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/479456b0-c4a5-4d08-83c0-2b80e3911dd4 「吴恩达Agentic AI模块4简报」工作流开发与优化 概要 本简报深入剖析了构建和优化 Agentic AI 工作流的系统化、迭代化流程。核心理念是避免长时间的理论构思，而是通过快速构建一个“粗糙但可用”的初始系统来启动开发周期。该流程强调在“构建”和“分析”之间进行持续循环，通过严谨的评估与错误分析来指导开发方向，从而实现高效、有针对性的系统改进。\n关键要点 迭代式开发循环： 成功的 Agentic AI 系统开发并非线性过程，而是一个在构建、评估、分析和改进之间不断循环的迭代过程。首先构建一个基本原型，然后通过分析其输出来发现问题，从而为下一步的构建工作指明方向。 评估（Evals）是核心驱动力： 建立评估体系是衡量进展和驱动改进的关键。评估应从简单的“端到端评估”开始，针对系统在特定方面的不足（如日期提取不准、文本长度超标）创建小规模的测试集（例如 10-20 个样本），以量化改进效果。 系统的错误分析： 为了确定应优先处理哪个系统组件，必须进行系统的错误分析。开发者应专注于系统出错的案例，审查每个步骤的“迹线”（traces），即中间输出，并使用电子表格等工具统计每个组件导致错误的频率。这能以数据驱动的方式揭示真正的瓶颈，避免凭直觉做出耗时且无效的决策。 组件级评估的重要性： 当错误分析指向特定组件时，建立“组件级评估”能极大提升优化效率。它能为该组件提供一个清晰、无干扰的性能信号，使得开发者可以快速迭代调整（如更换API、调整超参数），而无需每次都运行完整且昂贵的端到端评估。 优化策略的多样性： 针对不同组件的问题，需要采用不同的解决策略。对于非 LLM 组件，可以通过调整参数或更换工具来改进；对于基于 LLM 的组件，方法包括优化提示词、更换模型、分解任务，乃至在必要时进行模型微调。 成本与延迟的后期优化： 在系统输出质量达标之前，成本和延迟应是次要考虑因素。一旦系统性能稳定，可通过对工作流各环节进行基准测试，精确找出成本和时间开销最大的部分，然后有针对性地进行优化。 总之，高效的 Agentic AI 工作流开发依赖于一套严谨的分析方法论，它能将开发者的精力引导至最能提升系统整体性能的地方。\n1. 核心开发理念：从快速原型到迭代优化 构建 Agentic AI 系统的首要原则是快速行动。与其花费数周时间进行理论探讨和假设，不如尽快构建一个“粗糙但可用”（quick and dirty）的初始系统。这个原型不必完美，但它提供了一个可供观察和分析的实体。\n关键步骤： 快速构建原型：以安全、负责任的方式（如避免数据泄露）快速搭建一个端到端的系统。 手动审查输出：运行原型并审查一小批（如 10-20 个）样本的最终输出。 识别错误模式：通过审查，识别系统常见的失败模式。例如，在发票处理工作流中，可能会发现系统经常混淆“发票开具日期”和“付款截止日期”。 确定优化焦点：这种初步分析能够揭示系统的主要弱点，从而帮助开发者决定应将精力集中在哪个方面进行评估和改进。例如，如果日期混淆是主要问题，那么就应该优先建立一个衡量日期提取准确率的评估体系。 “我发现，在开发一个 Agentic AI 系统时，很难预先知道它在哪里会运行良好，在哪里会表现不佳，因此也很难知道应该将精力集中在哪里。所以，一个非常普遍的建议是，先尝试构建一个哪怕是粗糙的系统，这样你就可以试用并观察它，看看哪些地方可能还没有达到你期望的效果，从而更有针对性地进行进一步开发。” Andrew Ng\n2. 评估（Evals）：衡量和驱动系统改进 一旦确定了系统的关键弱点，下一步就是建立评估（Evals）体系来量化问题并跟踪改进进度。评估是推动系统性能提升的基石。\n端到端评估的构建 端到端评估衡量的是从用户输入到最终输出的整个系统的性能。构建方法取决于具体应用场景和发现的错误模式。\n案例 1：发票处理（提取截止日期） 问题：系统经常混淆日期。 评估构建： 创建测试集：选取 10-20 张发票，手动记录下每张发票正确的“付款截止日期”，形成“真实标签”（ground truth）。 标准化输出：在提示词中要求 LLM 始终以标准格式（如 YYYY-MM-DD）输出日期。 编写评估代码：使用代码（如正则表达式）从 LLM 的输出中提取日期，并将其与真实标签进行比对。 计算准确率：通过计算匹配正确的百分比来衡量系统性能，并以此为指标来迭代优化提示词或系统其他部分。 案例 2：营销文案助手（控制文本长度） 问题：生成的文案经 …"},{"title":"「吴恩达Agentic AI模块5简报」高度自主的智能体AI模式","url":"/courses/andrew-ng-agentic-ai/insights/lec-05/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 「吴恩达Agentic AI模块5简报」高度自主的智能体AI模式 概要 本文档综合分析了构建高度自主的AI智能体的核心设计模式，重点阐述了规划（Planning）和多智能体系统（Multi-Agent Systems） 这两种前沿方法。这些模式旨在让AI超越预设的指令序列，能够灵活地自主决策以完成复杂任务。\n核心洞察包括 规划模式：此模式赋予大语言模型（LLM）根据用户请求和可用工具集，自主生成多步骤行动计划的能力。这种方法极大地增强了AI的灵活性和处理复杂任务的范围，但同时也降低了开发者对系统运行时行为的直接控制和可预测性。 规划的实现方式：为确保计划能被可靠执行，应采用结构化格式。其中，通过代码执行进行规划是最为强大的方法，它允许LLM利用其在编程语言（如Python）和大型代码库（如Pandas）上的丰富训练数据来构建复杂计划。相比之下，JSON和XML也是可靠的结构化格式，优于易产生歧义的Markdown或纯文本。 多智能体系统：此模式借鉴了人类团队协作的理念，将一个复杂的宏观任务分解为多个子任务，并分配给具有特定角色和工具的专业化智能体。这种方法不仅简化了复杂系统的开发过程，还促进了智能体模块的复用性。 智能体通信模式：多智能体系统的协作效率取决于其通信模式。目前最常见的两种模式是线性模式（按顺序依次执行）和层级模式（由一个“管理者”智能体协调多个“工作者”智能体）。此外，还存在更复杂但使用较少的深度层级模式和高度不可预测的“全体对全体”实验性模式。 核心设计模式：规划 规划是实现高度自主智能体的关键设计模式，它使智能体能够摆脱硬编码的步骤束缚，根据具体情境动态制定行动方案。\n概念与工作流程 规划模式的核心思想是让LLM扮演决策者的角色，而非仅仅是指令的执行者。其工作流程通常如下：\n接收输入：向LLM提供用户的原始请求以及一个包含可用工具及其功能的列表。 生成计划：提示LLM根据用户目标，返回一个分步执行的计划。 顺序执行：系统逐一执行计划中的每个步骤。每一步的输出结果都会作为上下文信息，被传递给下一步，以确保任务的连贯性。 这种模式的显著优势在于，开发者无需预先预测并编码所有可能的工具调用序列，从而使智能体能够应对更加多样化和复杂的请求。\n应用案例\n电商客服智能体：假设一个太阳镜零售店的客服智能体被问到：“你们有100美元以下的圆形太阳镜库存吗？” 该智能体可以规划出如下步骤： 使用get_item_descriptions工具筛选出所有描述为“圆形”的太阳镜。 将筛选结果传入check_inventory工具，确认哪些有库存。 最后使用get_item_price工具，检查有库存的圆形太阳镜价格是否低于100美元，并最终回答用户。 邮件助手：对于指令“请回复纽约Bob的邮件，告诉他我会参加，然后把他的邮件归档”，智能体可以生成计划： 使用search_email工具找到来自Bob的、提及“纽约”和“晚餐”的邮件。 生成并发送一封确认参加的回复邮件。 使用move_email工具将原邮件移动到归档文件夹。 现状与挑战 规划模式在某些领域已取得显著成功，但在其他领域的应用仍在发展中。\n","description":"本文档综合分析了构建高度自主的AI智能体的核心设计模式，重点阐述了**规划（Planning）和多智能体系统（Multi-Agent Systems）** 这两种前沿方法。这些模式旨在让AI超越预设的指令序列，能够灵活地自主决策以完成复杂任务。","tags":["Agentic AI"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ https://notebooklm.google.com/notebook/db2bdef5-1d8d-4a5a-b215-bdc56a369800 「吴恩达Agentic AI模块5简报」高度自主的智能体AI模式 概要 本文档综合分析了构建高度自主的AI智能体的核心设计模式，重点阐述了规划（Planning）和多智能体系统（Multi-Agent Systems） 这两种前沿方法。这些模式旨在让AI超越预设的指令序列，能够灵活地自主决策以完成复杂任务。\n核心洞察包括 规划模式：此模式赋予大语言模型（LLM）根据用户请求和可用工具集，自主生成多步骤行动计划的能力。这种方法极大地增强了AI的灵活性和处理复杂任务的范围，但同时也降低了开发者对系统运行时行为的直接控制和可预测性。 规划的实现方式：为确保计划能被可靠执行，应采用结构化格式。其中，通过代码执行进行规划是最为强大的方法，它允许LLM利用其在编程语言（如Python）和大型代码库（如Pandas）上的丰富训练数据来构建复杂计划。相比之下，JSON和XML也是可靠的结构化格式，优于易产生歧义的Markdown或纯文本。 多智能体系统：此模式借鉴了人类团队协作的理念，将一个复杂的宏观任务分解为多个子任务，并分配给具有特定角色和工具的专业化智能体。这种方法不仅简化了复杂系统的开发过程，还促进了智能体模块的复用性。 智能体通信模式：多智能体系统的协作效率取决于其通信模式。目前最常见的两种模式是线性模式（按顺序依次执行）和层级模式（由一个“管理者”智能体协调多个“工作者”智能体）。此外，还存在更复杂但使用较少的深度层级模式和高度不可预测的“全体对全体”实验性模式。 核心设计模式：规划 规划是实现高度自主智能体的关键设计模式，它使智能体能够摆脱硬编码的步骤束缚，根据具体情境动态制定行动方案。\n概念与工作流程 规划模式的核心思想是让LLM扮演决策者的角色，而非仅仅是指令的执行者。其工作流程通常如下：\n接收输入：向LLM提供用户的原始请求以及一个包含可用工具及其功能的列表。 生成计划：提示LLM根据用户目标，返回一个分步执行的计划。 顺序执行：系统逐一执行计划中的每个步骤。每一步的输出结果都会作为上下文信息，被传递给下一步，以确保任务的连贯性。 这种模式的显著优势在于，开发者无需预先预测并编码所有可能的工具调用序列，从而使智能体能够应对更加多样化和复杂的请求。\n应用案例\n电商客服智能体：假设一个太阳镜零售店的客服智能体被问到：“你们有100美元以下的圆形太阳镜库存吗？” 该智能体可以规划出如下步骤： 使用get_item_descriptions工具筛选出所有描述为“圆形”的太阳镜。 将筛选结果传入check_inventory工具，确认哪些有库存。 最后使用get_item_price工具，检查有库存的圆形太阳镜价格是否低于100美元，并最终回答用户。 邮件助手：对于指令“请回复纽约Bob的邮件，告诉他我会参加，然后把他的邮件归档”，智能体可以生成计划： 使用search_email工具找到来自Bob的、提及“纽约”和“晚餐”的邮件。 生成并发送一封确认参加的回复邮件。 使用move_email工具将原邮件移动到归档文件夹。 现状与挑战 规划模式在某些领域已取得显著成功，但在其他领域的应用仍在发展中。\n成功应用：在高度智能化的软件编码系统中，该模式表现出色。这些系统可以为复杂的软件开发任务制定详细的构建计划（如先构建组件A，再构建组件B，并进行测试），然后像执行清单一样逐步完成。 挑战与实验性：在其他行业，规划模式的应用尚不普及。主要挑战在于可控性和可预测性的降低。由于开发者不直接规定执行路径，系统的运行时行为变得难以预料，这给系统的稳定性和可靠性带来了挑战。 规划的实现与优化 为了让LLM生成的计划能够被下游代码清晰、无歧义地解析和执行，选择合适的输出格式至关重要。\n结构化规划格式 要求LLM以机器可读的格式输出计划，是确保其可靠执行的前提。不同格式的可靠性存在差异。\n格式 (Format)\t可靠性 (Reliability)\t备注 (Notes) 代码 (Code)\t非常高 (Very High)\t最强大的方法，直接生成可执行代码作为计划，利用现有编程语言和库的强大功能。 JSON\t高 (High)\t常见且可靠，通过键值对（如step, tool, arguments）清晰定义步骤。 XML\t良好 (Good)\t另一种可靠的结构化选项，使用标签明确定义计划结构。 Markdown\t中等 (Medium)\t解析时可能存在轻微歧义，不如JSON或XML严谨。 纯文本 (Plain …"},{"title":"吴恩达 Agentic AI 课程实录（完整版）","url":"/courses/andrew-ng-agentic-ai/agenticai/","section":"courses","date":"2025-10-17","summary":" https://learn.deeplearning.ai/courses/agentic-ai/ 吴恩达课程：Agentic AI 作者：吴恩达（Andrew Ng） 课程链接：DeepLearning.ai - Agentic AI 模块一：Agentic AI简介 1.0 简介 欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使用 Agentic AI 构建的真正有价值和有用的应用数量也增长得非常迅速，即使没有炒作那么快。在本课程中，我想向您展示构建 Agentic AI 应用的最佳实践。这将在您现在可以构建什么方面，为您开启许多新的机会。\n如今，agentic 工作流正被用于构建客户支持代理等应用，或进行深度研究以帮助撰写富有洞察力的研究报告，或处理棘手的法律文件，或查看患者输入信息并提出可能的医学诊断。在我带的许多团队中，我们构建的很多项目如果没有 agentic 工作流是根本不可能完成的。因此，知道如何用它们来构建应用是当今 AI 领域最重要和最有价值的技能之一。\n我发现，真正懂得如何构建 agentic 工作流的人与那些效率较低的人之间，最大的区别之一是能否推动一个规范的开发流程，特别是专注于评估和错误分析的流程。在本课程中，我将告诉您这意味着什么，并向您展示如何才能真正擅长构建这些 agentic 工作流。能够做到这一点是当今 AI 领域最重要的技能之一，它将为您开启更多的机会，无论是工作机会，还是亲手打造出色软件的机会。\n那么，让我们进入下一个视频，更深入地探讨什么是 agentic 工作流。\n1.1 什么是 Agentic AI 那么，什么是 Agentic AI？为什么 Agentic AI 工作流如此强大？让我们来看一看。\n","description":"欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使","tags":["Agentic AI","Andrew Ng","Course"],"categories":["Courses"],"content":" https://learn.deeplearning.ai/courses/agentic-ai/ 吴恩达课程：Agentic AI 作者：吴恩达（Andrew Ng） 课程链接：DeepLearning.ai - Agentic AI 模块一：Agentic AI简介 1.0 简介 欢迎来到这门关于 Agentic AI 的课程。当我创造 “agentic” 这个词来描述我所看到的一种重要且快速增长的、人们构建基础应用的方式时，我没有意识到的是，一群营销人员会抓住这个词，把它当成一个标签，贴在几乎所有能看到的东西上。这导致了对 Agentic AI 的炒作急剧升温。不过，好消息是，抛开炒作不谈，使用 Agentic AI 构建的真正有价值和有用的应用数量也增长得非常迅速，即使没有炒作那么快。在本课程中，我想向您展示构建 Agentic AI 应用的最佳实践。这将在您现在可以构建什么方面，为您开启许多新的机会。\n如今，agentic 工作流正被用于构建客户支持代理等应用，或进行深度研究以帮助撰写富有洞察力的研究报告，或处理棘手的法律文件，或查看患者输入信息并提出可能的医学诊断。在我带的许多团队中，我们构建的很多项目如果没有 agentic 工作流是根本不可能完成的。因此，知道如何用它们来构建应用是当今 AI 领域最重要和最有价值的技能之一。\n我发现，真正懂得如何构建 agentic 工作流的人与那些效率较低的人之间，最大的区别之一是能否推动一个规范的开发流程，特别是专注于评估和错误分析的流程。在本课程中，我将告诉您这意味着什么，并向您展示如何才能真正擅长构建这些 agentic 工作流。能够做到这一点是当今 AI 领域最重要的技能之一，它将为您开启更多的机会，无论是工作机会，还是亲手打造出色软件的机会。\n那么，让我们进入下一个视频，更深入地探讨什么是 agentic 工作流。\n1.1 什么是 Agentic AI 那么，什么是 Agentic AI？为什么 Agentic AI 工作流如此强大？让我们来看一看。\n如今，我们许多人使用大型语言模型（LLM）的方式是提示它，比如说，为我们写一篇关于某个主题 X 的文章。我认为这类似于去找一个人，或者在这种情况下，去找一个 AI，请它为我打出一篇文章，要求它从第一个词写到最后一个词，一气呵成，并且永远不能使用退格键。事实证明，我们人类并不能通过这种被迫以完全线性顺序写作的方式来完成我们最好的作品，AI 模型也是如此。但尽管受到这种写作方式的限制，我们的大语言模型表现得出奇地好。\n相比之下，使用 agentic 工作流，过程可能是这样的：你可能会让它首先写一个关于某个主题的文章大纲，然后问它是否需要进行任何网络研究。在进行了一些网络研究并可能下载了一些网页之后，再让它撰写初稿，然后阅读初稿，看看哪些部分需要修改或做更多研究，接着修改草稿，如此循环。这种工作流更类似于先进行一些思考和研究，然后进行一些修改，再进行更多的思考，等等。通过这种迭代过程，事实证明，agentic 工作流可能需要更长的时间，但它能交付出质量好得多的工作成果。\n所以，一个 Agentic AI 工作流是一个基于 LLM 的应用执行多个步骤来完成一项任务的过程。在这个例子中，你可能会使用一个 LLM 来撰写文章大纲，然后你可能会使用一个 LLM 来决定在网络搜索引擎中输入什么搜索词，或者说，用什么搜索词来调用网络搜索 API，以获取相关的网页。基于此，你可以将下载的网页输入到一个 LLM 中，让它撰写初稿，然后可能使用另一个 LLM 进行反思，并决定哪些地方需要更多修改。根据你设计这个工作流的方式，也许你甚至可以加入一个“人类在环”的步骤，让 LLM 可以选择请求人类审查某些关键事实。在此基础上，它可能会修改草稿，这个过程会产生一个好得多的工作输出。\n你在本课程中学到的关键技能之一，就是如何将像写文章这样的复杂任务分解成更小的步骤，让 agentic 工作流一次执行一步，从而获得你想要的工作输出。事实证明，知道如何将任务分解为步骤，以及如何构建组件来很好地执行单个步骤，是一项棘手但重要的技能，它将决定你为各种激动人心的应用构建 agentic 工作流的能力。\n在本课程中，我们将使用一个贯穿始终的例子，也是你将与我一起构建的东西——一个研究代理（research agent）。下面是它看起来的样子。你可以输入一个研究主题，比如“我如何建立一个新的火箭公司来与 SpaceX 竞争”。我个人不想与 SpaceX 竞争，但如果你想，你可以试着让研究代理帮助你做背景研究。\n这个代理首先会规划要使用哪些研究方法，包括调用网络搜索引擎下载一些网页，然后综合和排序研究发现，起草大纲，让一个“编辑代理”审查其连贯性，最后生成一份全面 …"},{"title":"关于诺贝尔奖的感想（2025）","url":"/post/about-nobel-prize-in-2025/","section":"post","date":"2025-10-08","summary":"一年一度的诺奖颁奖季过半，又是令国人失望的一年。作为一个打小热爱科学的人，我关注这个奖项十多次了，有那么几次挺开心的，比如钱永健、高锟、莫言、屠呦呦获奖，衷心地为他们高兴。但是其他年份，尤其是过去十年，心情越发不美丽。本文是我在当下的一些感想，其实也是我与朋友在失望之下的争吵中所达成的某种合理化解读。\n","description":"","tags":["诺奖","体制","社会"],"categories":["post"],"content":"一年一度的诺奖颁奖季过半，又是令国人失望的一年。作为一个打小热爱科学的人，我关注这个奖项十多次了，有那么几次挺开心的，比如钱永健、高锟、莫言、屠呦呦获奖，衷心地为他们高兴。但是其他年份，尤其是过去十年，心情越发不美丽。本文是我在当下的一些感想，其实也是我与朋友在失望之下的争吵中所达成的某种合理化解读。\n我自我感觉成熟了，已经对诺奖祛魅。但出于对自己“墙头草”秉性的清醒认识，相信如果哪天中国人频繁拿奖到手软，我不会因为“物以稀为贵”而开始嫌弃，而是会重新为这个奖项赋予更伟大的意义，并对其大加赞扬。不过这些打脸的评价是留给未来的，所以在本文的标题中我需要加上2025这个年份，现在先把话撂这儿！当前的看法，也就如下这样吧。\n欢乐是邻居的，我们什么也没有 2025年的颁奖还在继续，但是三项科学大奖已经名花有主。如往年一样，中国学者又一次地没能拿奖。后面还有三个奖项，想必大家兴趣不大，很多次颁发和平奖，我们感觉就像吃苍蝇一样，尤其是今年川普在颁奖季明显发力了，此时最好避其锋芒，毕竟此人惹不起，只能躲着。经济学奖，在当前形势下获奖显得有点不可思议，如果非要给，如果不颁给今日连发数文，抛出惊天理论的某日报，有点说不过去，我们姑且拭目以待吧！至于文学奖，莫言因为获得该奖，每每被推向舆论的风口浪尖，也算是个高危奖项。鉴于莫言得奖后在国内舆论获得的待遇，也可以认为中国人并不稀罕这种奖项。\n相对于和平、经济和文学这三大“文科奖项”，国人显然更在乎“理科奖项”。过去这几十年，中国在科技领域发展迅猛，因此每次颁奖的时候也都跃跃欲试。但是自从屠呦呦获奖，至今已经十年了，三大科学奖每一次都与我们“擦肩而过”。\n与自己的郁郁不得志相比，更气人的可能还是邻居家又在张灯结彩了。今年日本学者拿了生命科学医学奖和化学奖这两项大奖，刚刚看到石破茂现场直播打电话向生医奖得主坂口志文表示祝贺，电话列表里的名单又+1，化学奖颁给了北川进。今年的两次获奖使得他们在新世纪（2001年及之后）的25年里已经获得了22次诺奖，而且全都是科学类。日本曾在世纪初定下了“50年30次获奖”的宏伟目标，当时看起来如天方夜谭，结果时间刚过半，任务完成度已经超了70%。有人会说：诺奖都是颁给几十年前的工作，日本人在最近这二十年里水平是在下滑的。这或许是真的，但是凭借当前的惯性（优良传统的积累以及业界的普遍认可），剩下的25年再拿8次看起来不太难。\n当我们谈论诺奖，我们在谈论什么？ “诺奖情节”可能像“世界杯情节”一样，一直是国人心里最容易被激发的几个痛点。\n这两者背后存在很多共性。我国在各种中学生的学科竞赛中斩获无数，但在顶级科学大奖上收获寥寥；我国在各种单人运动项目上成果丰硕，奥运会等舞台上摘金夺银已成家常便饭，但是在大型的球类运动中却一直在“极度狼狈”和“比较狼狈”中上下波动。因为学科竞赛和个人体育项目的随机性要弱很多，而科学前沿的探索与足球场上的局面一样，充满了未知。作为乒乓球和羽毛球的强者，我们很容易派出十拿九稳的阵容，让观众充满了掌控感。但是这种掌控感在足球场上是很难获得的，即便是顶级球队也难说自己不会被爆冷。科研的创新则更是如此，很多发明都来自于出乎预料的尝试。所以我们都希望能出很多“诺奖级成果”，但是这方面急也没用，还是放平心态。\n同时，也应当认识到，虽然很难获得某种必然性的成果，但科学的进步却是有迹可循的。首先是人才的培养，然后是资源的投入。日本在2000年之前，诺奖得主也是大概十年一遇，但之后却迎来了持续25年的井喷。诺奖的滞后性也是我们需要对过去的努力保留善意的重要因素，比如即便是今年的诺奖成果，也多是三十年前做出来的，彼时恰逢日本经济繁荣期，对科技上的高投入在日后开花结果。这或许说明，“大面积”地获得诺奖是用钱砸出来的。\n我还是相信，用不了很久，我们会迎来丰收季节。在过去的几十年里，中国的科技发展是有目共睹的，人才积累已经到了堪比“人口红利”的程度，高强度的科研资金的投入也已经持续了数十年，虽然科研的投入产出比是另一回事，但是没有理由妄自菲薄。在之前，我对这种可能性还不甚乐观，但鉴于在近年数学界逐渐有很多华人学者崭露头角，在AI领域的科技突破几乎都成了在华与在美的华人内战，我的疑虑也逐渐消解。我在这里大胆预言：在国足下一次打进世界杯之前，中国学者一定可以拿到诺奖！\n诺奖真那么神吗？ 诺奖在19世纪的最后几年基于诺贝尔的遗嘱创立，至今一百余年。它作为最早的一批奖金丰厚的常设奖项，积累了极高的信誉度，以至于逐渐成了神一般的奖项，至少获奖者很容易在业内被碰上神坛。\n那么我们要问：“诺奖真那么神吗？”\n这个问题需要分两部分回答。一部分是获得诺奖的成果，一部分是颁发诺奖的这个机制。\n其实第一部分没啥好回答的，在知乎上类似的问题很多，比如“牛顿与爱因斯坦谁更伟大”、“陶哲轩是不是菲尔 …"},{"title":"跑步的记忆","url":"/post/memory-of-running/","section":"post","date":"2025-09-29","summary":"回顾数年跑步训练的片段、节奏、节点与那些与里程绑定的个人记忆。","description":"","tags":["跑步","训练","健康","回忆"],"categories":["生活方式"],"content":"我不知道自己喜不喜欢运动，但是在2021年的时候，确实进入了一段跑步的高潮。我在那年的10月份跑了200多公里，一度以为自己要进化成了以为“严肃跑者”，但在月底，我的脚踝受伤了，歇了几个月之后，跑步的习惯没有继续维系下去\u0026hellip;\nok\n"},{"title":"Git 历史回退自救手册：reflog 找回与分支指针恢复","url":"/labs/git-recover_guide/","section":"labs","date":"2025-09-28","summary":"该指南来自于一次自我感觉惊险的经历，\n一份精简但可操作的“掉历史”自救清单：当你在 GUI 或命令行误操作后，发现最近提交“消失”时，如何快速定位并安全恢复。\n","description":"","tags":["Git","数据恢复","分支管理"],"categories":["技术分享"],"content":"该指南来自于一次自我感觉惊险的经历，\n一份精简但可操作的“掉历史”自救清单：当你在 GUI 或命令行误操作后，发现最近提交“消失”时，如何快速定位并安全恢复。\n典型症状 近期提交不见了：git log 顶端不是你刚做的提交。 工作区回到旧版本：本地文件和远端网站内容明显“时光倒流”。 远端与本地不一致：git status -sb 显示 main...origin/main [ahead N|behind N]。 成因与诱因（简明） 直接成因：分支指针被移到旧提交，或进入分离 HEAD，导致近期提交不在当前分支历史中。 前几天可能埋下的雷： 直接 git checkout \u0026lt;commit\u0026gt; 查看旧版本后继续开发/提交； 用 GUI 的“Checkout/Reset/Restore”类功能回到旧提交但未建保护分支； 偶发 git reset --hard \u0026lt;old\u0026gt;； 未及时推送，远端与本地脱节，引发误判。 今天触发的动作：在 GUI 中对某个旧提交执行了“Checkout/Reset”，把当前分支（例如 main）指针回退了。 本次拯救（摘要） 用 reflog 找到目标提交 \u0026lt;sha\u0026gt;（历史未丢，只是指针偏了）。\n二选一恢复：\n# 安全合并法（推荐） git checkout -b restore-work \u0026lt;sha\u0026gt; git checkout main git merge restore-work # 或：直接重置法（快捷） git checkout main git reset --hard \u0026lt;sha\u0026gt; 如远端历史需以本地为准：git push -f origin main 恢复步骤 # 1) 快速浏览当前可达历史 git log --oneline -10 # 2) 关键：查看 HEAD 的移动“航迹记录” git reflog -20 在 reflog 输出中，找到你需要回到的那次正确提交的 SHA（记为 \u0026lt;sha\u0026gt;）。 两种恢复方式（择一） # 从目标提交创建保护分支 git checkout -b restore-work \u0026lt;sha\u0026gt; # 回到主分支并合并恢复 git checkout main git merge restore-work 说明：保留现场，历史连贯，最不容易出岔子。\ngit checkout main git reset --hard \u0026lt;sha\u0026gt; 说明：让当前分支指针“瞬移”到 \u0026lt;sha\u0026gt;；如远端不是你想要的历史，后续可能需要强推。\n同步远端（如有需要） # 仅当你确认“本地才是正确历史”时才执行强推！ git push -f origin main 预防与习惯（精简版） 查看旧版本用新分支：git checkout -b inspect-\u0026lt;date\u0026gt; \u0026lt;sha\u0026gt;。 小步提交，及时推送：远端即备份。 危险操作前打快照：git branch backup-\u0026lt;date\u0026gt; 或 git tag safety-\u0026lt;date\u0026gt;。 丢历史先看 reflog：优先 git reflog，再考虑其他。 "},{"title":"代码“劫后余生”:记一场Git灾难与抢救","url":"/post/git-incident-2025-09-28/","section":"post","date":"2025-09-28","summary":"经过一天的工作，我昨天配置的博客评论区离奇消失，查找记录时发现更惊悚的事：我近几天的Git提交记录全部失踪，并因一个误操作导致代码回滚到三天前的基础版本，所有后期工作成果瞬间清零。我向 GPT-5-Codex 求救，它立即启动高级Git命令进行抢救。最终，在GPT的指导下，我执行了几行恢复命令，代码起死回生，成功回到了误操作前的状态。这次劫后余生的经历，让我深切体会到Git的强大与学习它的必要性。\n","description":"","tags":["数据恢复","分支管理","编程","网站开发"],"categories":["工程实践"],"content":"经过一天的工作，我昨天配置的博客评论区离奇消失，查找记录时发现更惊悚的事：我近几天的Git提交记录全部失踪，并因一个误操作导致代码回滚到三天前的基础版本，所有后期工作成果瞬间清零。我向 GPT-5-Codex 求救，它立即启动高级Git命令进行抢救。最终，在GPT的指导下，我执行了几行恢复命令，代码起死回生，成功回到了误操作前的状态。这次劫后余生的经历，让我深切体会到Git的强大与学习它的必要性。\n如果你对类似的情况感到恐惧，或者对Git的使用还不够熟悉，可以参考我的自救手册。本文主要讲述这件事情的前后经过。\n故事的前后经过 今天我发现博客的评论区没了，我感到很奇怪，因为昨这是我昨天下午做的，当时也提交git了，可能是今天进行主题修改的时候，AI不小心把该部分代码删掉了。虽然有点小不爽，但想来也不是很复杂的事情，毕竟昨天配置评论区的时候也没费多大劲。\n然而，当我打开windsurf试图查找之前的记录，发现GPT-5-Codex一如既往地陷入了“内耗式”地查找，来来回回几圈之后，我忍不住叫停问它：到底找没找到？它的回答是，没有…\n我很震惊，因为这中情况不太可能发生：最近每一次都是完成一小波修改就git一下，已经养成了习惯。昨天的那个配置不可能丢失，我对那一部分工作印象很深刻，我坚信不是我出现幻觉，就是AI出现了幻觉。我打开了GitHub Desktop（我之前都是直接在IDE上随手提交的，功能相对简陋），试图依次查找与博客评论区相关的配置，我要跟AI对质！\n然而，真正惊悚的事情就此开始：我发现GitHub Desktop上记录的最新提交记录是3天前（25日）。我惊慌之余打开了GitHub网站，发现的确如此，我这几天的记录工作记录不存在。我很困惑，难道是创建了什么新的分支？于是就在GitHub Desktop上随便点点点，突然，当我切回Windsurf，发现该IDE上的文件夹目录已经回归到了25日… 话说，25日的时候，我这一系列工作才刚开始没多久啊！我傻了。\n之前曾经出现过对git的误操作导致一些记录找不到，不过那个时候的工作烂尾的较多，丢失也不心疼。这一次，我好不容易把网站搭好，在25日晚上线，颇有成就感。25日之后，我又对该网站进一步完善，增加了评论区、搜索页、GA4的网站流量监控、深色主题等配置。我对UI交互方面也进行了许多优化，自认为体验比最初的版本好了许多。而此时我能找到的版本仅仅是25日中午的版本，当时距离上线的基础款配置还有许多工作要做！\n此时的我感觉非常沮丧，因为在半个小时之前，我还沉浸在各种幻想中，想着未来如何更进一步地把网站功能与内容完善，然后以此为根据地做大做强… 瞬间，一切似乎都成了泡影，退回了解放前。如果真如此，恐怕我没有那个心气再搞一次解放战争了。\n慌乱之中，我强行镇定一下。我意识到，这件事情的发生大概率是自己对git的功能不够了解导致的，我常用的只有那几个命令，而且多数时候都是拿鼠标操作的。有很多高级命令，我或许见过，但根本不知道其背后的含义。当我使用GitHub Desktop的时候，对一些陌生操作背后发生的事情是无意识的。\nAI应该很懂这些！我更需要做的事情是减少自己的贸然操作，看能不能搬到救兵。于是我回到了windsurf，继续与GPT-5-Codex进行交流。把我刚才的所作所为、当前处境以及对过去几天已日渐模糊的记忆碎片一五一十地和盘托出。为了让它认真的拯救我，我甚至打出了感情牌，告诉它如果这几天的工作记录丢失，我将失去活下去的勇气…\n于是，GPT直接开始了各种工具调用，使用了很多我没听说过的 git命令，换着花样地审查各种git记录。我就像参加手术的患者家属一样在手术室门外焦急地等待着，而GPT在这个过程中，一句宽慰我的话都没说… 甚至除了思考，不被折叠的文字输出都很少。我一度想打断它，直接问：行不行啊？\n就在我耐心快用光之前，它告诉我那些记录还在，列出了我今天commit的几条记录。此时的我仿佛看到了曙光，开始反复问它能不能修复？它告诉我，记录还在，应该如何如何操作，给出了几行命令。但此时我并不理解这些命令的意义，只想它告诉我能不能恢复…它依然没有正面回应，只是从它的语气中感觉比较稳…\n于是，我在它的指导下执行了那几行陌生但寄托了我所有希望的命令，然后就是起死回生的时刻，我长舒了一口气。 此时，我的代码回到了我的”残之一手“点击GitHub Desktop的那一刻之前的状态，我博客配置中，评论区的内容依然找不到。但是我已经不再为此感到一丝一毫的不爽，全是劫后余生的庆幸。\n后面，借助Windsurf+GPT-5，我把评论区补上了，又将其它已经发现的小bug进行了修复，然后开始写了这篇博客。这件事情始于经验的缺乏带来的误操作，不过还好我用的是git，这款软件真是大慈大悲的发明。过去两年我一直 …"},{"title":"虽然有点卡，但我喜欢Liquid Glass","url":"/post/updates-apple-tahoe/","section":"post","date":"2025-09-12","summary":"更新了一下苹果的系统，先是手机，后是电脑，体验良好。\n不过接下来需要考虑的可能是更新一下设备，老旧的款式仿佛有点卡，只是别人不那么说，我也不太确信，也不希望如此。\n","description":"","tags":["生活方式","测试","苹果"],"categories":["","日常"],"content":"更新了一下苹果的系统，先是手机，后是电脑，体验良好。\n不过接下来需要考虑的可能是更新一下设备，老旧的款式仿佛有点卡，只是别人不那么说，我也不太确信，也不希望如此。\n我的好感来自于两点：\n上个版本的“黑金主题”，切换过来之后一直没搞清楚怎么恢复之前的模式，被恶心了一年； WWDC上呈现的效果过于拉胯。过了几个月之后，惊喜地发现它远比预期的漂亮。 顺便地，我也把MacOS系统界面进行了修改，设计很好看，但可能是适配性问题，在刷网页的时候有点卡。\n这一次，MacOS的系统代号为Tahoe，取名自著名的太浩湖。\n太浩湖 这是一个旅游胜地，在文化与科技史上都具有传奇色彩，比如《教父2》电影的取景，比如著名的“AlexNet拍卖会”。可参考太浩湖的传奇故事及其数据可视化版本。 "},{"title":"SymPy 基础入门教程：符号计算核心功能解析","url":"/labs/sympy_tutorials/","section":"labs","date":"2025-08-19","summary":"欢迎来到 SymPy 的世界！SymPy 是一个强大的 Python 库，用于进行符号数学运算。与数值计算不同，符号计算处理的是数学表达式本身，而不是它们的具体数值。本教程将带您了解 SymPy 的基本功能，并通过与我们先前讨论的钢轨动力学案例相关联，展示这些功能在实际问题中的应用。\n","description":"","tags":["符号计算"],"categories":["数学"],"content":"欢迎来到 SymPy 的世界！SymPy 是一个强大的 Python 库，用于进行符号数学运算。与数值计算不同，符号计算处理的是数学表达式本身，而不是它们的具体数值。本教程将带您了解 SymPy 的基本功能，并通过与我们先前讨论的钢轨动力学案例相关联，展示这些功能在实际问题中的应用。\n1. SymPy 是什么？为什么要用它？ 想象一下，您在纸上推导复杂的数学公式，进行微分、积分、化简…… SymPy 就是您在计算机上的“草稿纸”和“数学助手”。它可以帮助您：\n精确地表示数学符号和表达式。 执行符号微分、积分、求和等运算。 化简和变换复杂的数学表达式。 求解方程。 进行矩阵运算。 对于像钢轨动力学这样涉及大量复杂公式推导的问题，手动计算不仅耗时，还容易出错。SymPy 可以自动化这个过程，提高效率和准确性。\n2. 安装与基本设置 如果您还没有安装 SymPy，可以通过 Python 的包管理器 pip 轻松安装：\npip install sympy 在您的 Python 脚本或 Jupyter Notebook 中使用 SymPy 前，首先需要导入它。为了让输出的数学公式更易读（尤其是在 Jupyter 环境中），可以启用 SymPy 的“美观打印” (pretty printing) 功能。\nimport sympy # 导入 SymPy 库 # 初始化美观打印功能 # use_unicode=True 允许使用 Unicode 字符来显示更美观的公式 # wrap_line=False 防止长公式在某些终端或 Notebook 中被不恰当地折行 sympy.init_printing(use_unicode=True, wrap_line=False) # 定义一个简单的辅助函数，用于在教程中打印章节标题 def print_header(title): print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50) print(f\u0026#34; {title} \u0026#34;) print(\u0026#34;=\u0026#34;*50) 3. 核心概念一：符号 (Symbols) 符号是 SymPy 进行运算的基本单元。它们代表了数学表达式中的变量或常数，但没有被赋予具体的数值。\n定义单个符号： 使用 sympy.Symbol('名字')。 定义多个符号： 使用 sympy.symbols('名字1 名字2 ...')。 为符号添加假设： 在定义符号时，可以指定其数学属性，如 real=True (实数), positive=True (正数), integer=True (整数)。这些假设有助于 SymPy 进行更精确的化简。 示例： print_header(\u0026#34;3. 核心概念一：符号 (Symbols)\u0026#34;) # 定义单个符号 x x = sympy.Symbol(\u0026#39;x\u0026#39;) print(f\u0026#34;符号 x: {x}\u0026#34;) # 定义时间符号 t，并假设它是实数 t = sympy.Symbol(\u0026#39;t\u0026#39;, real=True) print(f\u0026#34;符号 t: {t}, 是实数吗? {t.is_real}\u0026#34;) # 一次定义多个符号，并假设它们是正实数 l, E, I_y, m_r = sympy.symbols(\u0026#39;l E I_y m_r\u0026#39;, positive=True, real=True) print(f\u0026#34;符号 l: {l}, 是正数吗? {l.is_positive}\u0026#34;) print(f\u0026#34;符号 E: {E}, I_y: {I_y}, m_r: {m_r}\u0026#34;) # 定义整数符号，常用于索引或计数 NM = sympy.Symbol(\u0026#39;NM\u0026#39;, integer=True, positive=True) # 例如，模态数 k_idx = sympy.Symbol(\u0026#39;k\u0026#39;, integer=True, positive=True) # 例如，求和索引 print(f\u0026#34;符号 NM: {NM}, 是整数吗? {NM.is_integer}\u0026#34;) print(f\u0026#34;符号 k_idx: {k_idx}\u0026#34;) 与钢轨案例的联系： 在钢轨动力学推导中，我们首先定义了所有物理参数（如 l,E,Iy,mr）、独立变量（x,t）、以及用于求和和表示模态的索引（k,h,s,j,NM,Nstotal​）作为 SymPy 符号。\n4. 核心概念二：构建表达式 (Expressions) 一旦定义了符号，就可以像在普通数学中一样，使用它们和标准的 Python 运算符（+, -, *, /, ** …"},{"title":"构建压缩感知算法代码","url":"/labs/compressed_sensing_gemini_pro/","section":"labs","date":"2025-08-19","summary":"构建压缩感知（Compressed Sensing, CS）算法代码通常涉及以下几个核心步骤。压缩感知理论指出，如果信号是稀疏的或可压缩的，那么就可以用远少于奈奎斯特采样定理所要求的采样点来精确地重建信号。\n","description":"","tags":["压缩感知"],"categories":["信号处理"],"content":"构建压缩感知（Compressed Sensing, CS）算法代码通常涉及以下几个核心步骤。压缩感知理论指出，如果信号是稀疏的或可压缩的，那么就可以用远少于奈奎斯特采样定理所要求的采样点来精确地重建信号。 以下是构建压缩感知算法代码的一般步骤和关键组成部分： 定义稀疏信号 生成或获取原始信号 (Generate or Acquire the Original Signal): 这可能是一个图像、音频信号、时间序列数据等。 稀疏表示 (Sparse Representation): 原始信号本身可能不是稀疏的，但可以在某个变换域中变得稀疏。常见的变换包括： 傅里叶变换 (Fourier Transform - FT): 适用于频域稀疏的信号。 离散余弦变换 (Discrete Cosine Transform - DCT): 常用于图像和视频压缩。 小波变换 (Wavelet Transform - WT): 适用于具有局部特征的信号。 自定义字典 (Custom Dictionary): 对于特定类型的信号，可能会有更合适的稀疏基。 在代码中，你需要实现或使用库函数来对原始信号进行这些变换，得到稀疏系数向量 $\\alpha$。通常，只有少数几个 $\\alpha_i$ 的值很大，而其余的接近于零。 构建测量矩阵 测量矩阵 $\\Phi$ 是一个 $M \\times N$ 的矩阵，其中 $N$ 是原始信号的维度（或稀疏表示的维度），$M$ 是测量次数 ($M \u003c N$)。 测量矩阵需要满足某些特性，如受限等距性质 (Restricted Isometry Property - RIP)，以保证能够从压缩测量中恢复稀疏信号。在实践中，通常使用随机矩阵，因为它们很大概率满足 RIP。常见的选择有： 高斯随机矩阵 (Gaussian Random Matrix): 矩阵的每个元素从高斯分布中独立采样。 伯努利随机矩阵 (Bernoulli Random Matrix): 矩阵的每个元素从 $\\{-1, +1\\}$ 中等概率随机选择。 部分傅里叶矩阵 (Partial Fourier Matrix): 随机选择傅里叶变换矩阵的 $M$ 行。 稀疏随机矩阵 (Sparse Random Matrix): 包含大量零元素的随机矩阵，可以加速计算。 在代码中，你需要生成这样一个矩阵。例如，在 Python 中，可以使用 numpy.random.randn 生成高斯矩阵。 执行压缩感知测量 压缩测量是通过将原始信号（或其稀疏表示）与测量矩阵相乘来获得的。 如果直接对原始信号 $x$（维度为 $N$）进行测量，则测量结果 $y = \\Phi x$（维度为 $M$）。 如果信号 $x$ 在某个正交基 $\\Psi$ 下是稀疏的，即 $x = \\Psi \\alpha$（其中 $\\alpha$ 是稀疏系数），那么测量可以表示为 $y = \\Phi \\Psi \\alpha = \\Theta \\alpha$，其中 $\\Theta = \\Phi \\Psi$ 称为传感矩阵或恢复矩阵。 在代码中，这一步通常是一个简单的矩阵向量乘法。 实现重建算法 这是压缩感知中最核心和最具挑战性的部分。目标是从压缩测量 $y$ 和已知的测量矩阵 $\\Phi$（或 $\\Theta$）中恢复稀疏信号 $\\alpha$（或 $x$）。 这是一个欠定方程组，因为 $M \u003c N$。重建算法利用信号的稀疏性来找到唯一的稀疏解。 常见的重建算法包括： L1 范数最小化 (L1-norm Minimization) / 基追踪 (Basis Pursuit - BP): $$\\min_{\\alpha} \\|\\alpha\\|_1 \\quad \\text{subject to} \\quad y = \\Theta \\alpha$$ 这通常可以转化为线性规划问题。 贪婪算法 (Greedy Algorithms): 匹配追踪 (Matching Pursuit - MP): 迭代地选择与当前残差最相关的原子（$\\Theta$ 的列）。 正交匹配追踪 (Orthogonal Matching Pursuit - OMP): MP 的改进版，在每一步都对已选原子进行正交化，以确保残差与已选原子正交。这是教学和基础实现中非常流行的一种算法。 压缩采样匹配追踪 (Compressive Sampling Matching Pursuit - CoSaMP): 每次迭代选择多个原子。 迭代硬阈值 (Iterative Hard Thresholding - IHT): 通过梯度下降和硬阈值操作交替进行。 其他算法: 如迭代软阈值算法 (Iterative Soft-Thresholding Algorithm - …"},{"title":"韩信评传：月光下的不世之功与宿命悲歌","url":"/post/the-fate-of-general-han-xin/","section":"post","date":"2025-08-07","summary":"在中国历史的长廊中，有些名字的陨落，比其升起更加令人扼腕。韩信，便是其中最耀眼的一位。他的一生，是一场关于天才、机遇与宿命的宏大戏剧，而贯穿始终的，是他那既能成就伟业、亦能招致杀身之祸的青春锋芒。\n","description":"","tags":["韩信"],"categories":["人物","历史评论"],"content":"在中国历史的长廊中，有些名字的陨落，比其升起更加令人扼腕。韩信，便是其中最耀眼的一位。他的一生，是一场关于天才、机遇与宿命的宏大戏剧，而贯穿始终的，是他那既能成就伟业、亦能招致杀身之祸的青春锋芒。\n韩信的传奇，始于一道清冷的月光。若无萧何月下决绝的追寻，韩信或许终其一生，也只是个在市井中忍受过胯下之辱的落魄青年。那一夜，注定了他此后的不凡，也为他未来的悲剧埋下了伏笔。他得到了年长近三十岁的刘邦的破格提拔与充分信任，这在当时是一场匪夷所思的豪赌。刘邦赌上了自己的天下，而韩信，则用他那属于年轻人的、无所畏惧的才华，回报了这份知遇之恩。\n在属于他的那短短数年高光时刻里，他几乎就是战争的化身。明修栈道，暗度陈仓；背水一战，置之死地而后生。这些军事史上的奇迹，无一不带着年轻人特有的傲慢、任性和敢于打破一切常规的锐气。可以说，正是这份不羁的性格，才让他立下了那样的不世之功。如果他是一个循规蹈矩、唯唯诺诺的人，他也绝不可能成为那个让项羽都束手无策的“兵仙”。\n然而，当天下大局已定，当开拓的时代让位于守成的时代，他性格中那些曾经的优点，便成了最致命的缺陷。最大的问题，在于他太年轻了。不到三十岁便功高盖主，这让他难以低下高傲的头颅，去对那位已经步入晚年、心思深沉的君主察言观色。\n对于刘邦而言，矛盾是显而易见的。在征讨英布中箭受伤后，他深知自己时日无多。此刻的他，需要的不再是一个能够改变战局的天才，而是一个能够确保政权平稳交接的“可控的听话的人”。韩信的危险，不在于他知道了什么秘密，而在于“听他的人太多了”。他的威望和能力，已经成了一个刘邦身后无人能镇住的巨大变数。\n这道横亘在君臣间的年龄鸿沟与权力逻辑，是无法跨越的。或许，如果刘邦能年轻三十岁，他会欣喜于与这样一位军事天才惺惺相惜，恨不得日日与他泡在一起。但现实是，衰老的帝王需要的是稳定，而如日中天的韩信代表的却是不可知的未来。\n因此，韩信的结局早已注定。他似乎也对此有所预感，明白自己踏入的是一场华丽却必将迎来清算的盛宴。他对刘邦的知遇之恩心怀感念，所以在死亡的罗网降临时，他没有做过多的反抗，带着几分认命的意味，走向了长乐宫的终点。吕后是行刑者，但其背后，无疑是刘邦的默许。那一句“且喜且怜之”，道尽了这位开国帝王最真实的心态：喜的是，一个足以动摇国本的隐患终于被铲除；怜的是，这样一位不世出的英才就此凋零。\n韩信的一生，是青春的赞歌，也是青春的挽歌。他用短暂的生命证明了，一个年轻人凭借天赋与胆识可以达到怎样的高度。但也用自己的鲜血警示后人，当这种无双的才华触碰到权力的红线时，会迎来何等冰冷的结局。他注定要在他生命中的伯乐之前死去，要么在精神上被磨去棱角，要么在肉体上被彻底消灭。而那个时代，终究没能给他第三条路。\n"},{"title":"R.I.P, Diogo!","url":"/post/jota-passed-away/","section":"post","date":"2025-07-05","summary":"利物浦前锋迪奥戈·若塔（Diogo Jota）不幸逝世，享年28岁。据传他因为做了气胸手术，无法乘坐飞机，于是驾驶自己的兰博基尼，在其兄弟安德烈的陪同下前往利物浦，但途径西班牙境内，路况糟糕，在试图超越一辆卡车时发生了爆胎事故，导致车辆失控，兄弟俩不幸遇难。\n","description":"","tags":["利物浦"],"categories":["足球"],"content":"利物浦前锋迪奥戈·若塔（Diogo Jota）不幸逝世，享年28岁。据传他因为做了气胸手术，无法乘坐飞机，于是驾驶自己的兰博基尼，在其兄弟安德烈的陪同下前往利物浦，但途径西班牙境内，路况糟糕，在试图超越一辆卡车时发生了爆胎事故，导致车辆失控，兄弟俩不幸遇难。\n在本博客中，我日常分享的第一篇就是类似于讣告的文章，这令人遗憾，但将其献给迪奥戈又是一种荣誉。\n作为利物浦球迷，对这件事情感到震惊，但又有些麻木。似乎霉运总是会在这只球队最欢快的时候找上门来。利物浦今年刚刚拿到了阔别数年的英超冠军，而28岁的迪奥戈·若塔正值生涯的黄金年龄，备受期待，但一切都戛然而止了。\n或许坚强地面对悲伤是利物浦球迷的必修课。我从2005年开始看球的时候就追随这家俱乐部，从“伊斯坦布尔之夜”开始，然后是近十年没有冠军进账的漫长等待，终于迎来了克洛普伟大的执教生涯以及克洛普与斯洛特堪称佳话的交接。在两天之前，我们经历了最开心最憧憬的一段时间，但现在需要重新开始。\n为什么热爱利物浦？因为这支队伍的命运与我们的艹蛋人生太押韵了。\n与这支球队共同经历过2005, 2019, 2020, 2025，两次欧冠，两次英超，次数没有那些豪门那么多，但每一次都是苦尽甘来。本以为40天之前的狂欢可以继续，但却传来永远不可挽回的噩耗。\n在球迷的意识星空中，1985、1989年的那些星星永远照亮着一个个黯淡的夜晚。2025，天上又多了一颗星星，照亮不屈的人们继续前行。感谢迪奥戈在过去的五个赛季里的兢兢业业，为球队带来了一个个进球和一座座冠军奖杯。\nYou will never walk alone！\n注：本文中的图片均来自于利物浦官网 "},{"title":"《暗淡蓝点》读后记","url":"/post/pale-blue-dot-review/","section":"post","date":"2025-07-01","summary":"在浩瀚宇宙中，地球只是微不足道的一粒尘埃。然而，正是这颗“暗淡蓝点”，承载了人类全部的历史与情感。本文将从科学与人文的视角，分享我对卡尔·萨根《暗淡蓝点》一书的阅读体会，探讨我们在宇宙中的位置与意义。\n","description":"","tags":["科学","卡尔·萨根","科普"],"categories":["评论"],"content":"在浩瀚宇宙中，地球只是微不足道的一粒尘埃。然而，正是这颗“暗淡蓝点”，承载了人类全部的历史与情感。本文将从科学与人文的视角，分享我对卡尔·萨根《暗淡蓝点》一书的阅读体会，探讨我们在宇宙中的位置与意义。\n卡尔·萨根去世于1996年，那时我还很小。几年后，在一本少儿科普读物中看到缅怀他的文章，于是他也成了我小学时记住的为数不多的几位外国科学家之一。\n中学时读过《日本平家蟹》，当时阅读能力有限，没能理解文章背后的思想。不喜欢那种娓娓道来的叙述方式，更不喜欢那些虾啊蟹啊之类的事物。大学时，纪录片《宇宙》真正把我带进了科学殿堂，对“平家蟹”的疑惑也得以解开。\n“平家蟹”揭示了一种造型上类似于日本武士的蟹类，被当地渔民认为是随平家沉入海底的武士们的化身。但卡尔·萨根告诉我们，当地渔民捞到长相与武士神似的螃蟹后会将其放生，以表达对平家武士的缅怀。久而久之，这类螃蟹的种群就繁衍壮大——本质上，这是一种“人工选择”（也可以说是经人之手的“自然选择”）。平家蟹的故事让我开始掌握用科学视角理解自然现象的技巧。\n年轻时喜欢科学视角，却更沉迷于宏大叙事。毕竟，煌煌大国，数千年绵长文明，沧海横流与歌舞升平交织的历史，总让人热血澎湃。然而，后来发现，当宏大叙事照进日常生活时，总感觉有些不协调。这很正常，毕竟历史中最不起眼的人名，在当时恐怕都是显赫一时的人物，而我们在历史长河中不过是注定被遗忘的一颗水滴或一粒沙尘。\n后来，在一个播客节目中，听到主播朗读《暗淡蓝点》中的那段文字，那种蓦然回首的动人瞬间在脑海中再也挥之不去：\n“再看看那个光点，它就在这里。这是家园，这是我们。你所爱的每一个人，你认识的每一个人，你听说过的每一个人，曾经有过的每一个人，都在它的上面度过他们的一生。我们的欢乐与痛苦聚集在一起。数以千计的自以为是的宗教、意识形态和经济学说，每一个猎人与采集者，每一个英雄与懦夫，每一个文明的缔造者与毁灭者，每一个国王与农夫，每一对年轻爱侣，每一个母亲和父亲，每一个满怀希望的孩子，每一个发明家和探险家，每一个德高望重的教师，每一个腐败的政客，每一个‘超级明星’，每一个‘最高领导者’，人类历史上的每一个圣人与罪犯，都生活在这里——一粒悬浮在太阳光中的细小尘埃。”\n这是“旅行者一号”的视角，也是科学探索者的视角。这个世界上，总有不少野心家吹嘘自己滔天的权势与不朽的功绩，但在宇宙之中，他所在乎的一切不过是某一尘埃之上瞬息之间的“自我感觉良好的错觉”。\n我们绝大多数人，在这个充斥着拥堵与摩擦的人世间，也常感慨生活的乏味与人性的险恶。但在宇宙尺度下，这种体验既不见得比微生物的冲撞与摩擦更高级，也不见得比王侯将相的自我陶醉更卑微。在这个被假象与真实反复折叠的世界里，开心也好，压抑也罢，都不过是人在与自然互动中产生的生理反应。当我们代入“旅行者一号”的视角，这些纷繁复杂的情感体验与生理反应在空间上的局域性以及在时间上的有限性都不超过那个暗淡的蓝点。\n自从人类发现了科学的用处，就再也无法压抑对太空的向往。齐奥尔科夫斯基说：“地球是人类的摇篮，但人类不可能永远生活在摇篮里。”然而，当人类终于能够跳出自己的摇篮，回望浩瀚星海中的暗淡蓝点时，又茫然发现这广袤宇宙中再也找不到另一个家园。或许这一百多年对宇宙的探索，最有价值的结论就是：“人类不可能永远生活在摇篮里，但是地球——那个人类的摇篮，是唯一一个能够长期包容人类的家园。”\n就像每一个在悸动的青春时代勇敢离家出走的孩子，终究会发现故乡是唯一让自己牵肠挂肚的地方；就像每一个攒够了钱等到假期的城市居民，当他们终于抵达魂牵梦绕的旅游景点时，发现自己可能更想回家躺着。\n一个人对社会了解越多，越感觉自己的有限；对世界了解得越多，越对自然存敬畏之心。科学的价值大抵如此——它可能没办法帮人类打造一个天堂，但它可以让人类更充分地认识自己。\n当我们把视角从“旅行者一号”拉回到地球家园，会发现自己虽然渺小，但拥有丰富的感知能力。一草一木、一花一叶、一缕阳光、一口空气，都足以在某个微小瞬间滋养精神世界。地球虽不过是诸多星球中不算特别的一个，但却足以让数十亿人类倾其所有去感受其万千气象。\n我们无需纠结于自己是否会青史留名，因为有些东西不过是某些特权群体为了塑造对自己有利的“共同意识”选择性编排的故事合集。我们可以在有限的认知范围内质疑其准确性，但终究没有解释权。\n但我们每个人，在人类视角中伟大也好渺小也罢，都拥有一个更加鲜活、更有温度的小世界。我们经历的每个瞬间都是真实的，仔细品来也常能感受到其精彩。将这些真实与精彩进行抽象整理，或许就会发现科学蕴藏其中，或许生活中的真理远比宏大叙述中的要丰富得多。\n本文写于2024年11月30日\n"},{"title":"《牛顿传》读后记","url":"/post/newton_biography_review/","section":"post","date":"2025-07-01","summary":"在阅读《牛顿传》之前，我对牛顿的印象还停留在课本和科普故事中。带着对这位科学巨匠的好奇，我试图通过这本传记，重新认识牛顿的真实人生与复杂性格。以下是我的一些思考与感受。\n","description":"","tags":["传记","学者","牛顿"],"categories":["评论"],"content":"在阅读《牛顿传》之前，我对牛顿的印象还停留在课本和科普故事中。带着对这位科学巨匠的好奇，我试图通过这本传记，重新认识牛顿的真实人生与复杂性格。以下是我的一些思考与感受。\n首先，毋庸置疑，牛顿是一个了不起的人物，很少有人能像他一样对我们每个人产生如此深刻的影响。他给世界带来的改变无需多言，但我们对他的认识和了解却非常肤浅。\n我小学时就开始借阅关于牛顿的书，但当时阅读能力太差，读不下去，只是对他的一些故事有所耳闻，比如知道他出生在克伦威尔时期，但那时我并不知道克伦威尔是谁。后来我对科学史有了一些初步的了解，但我一直有意避开牛顿这个人，因为他太过久远，是个难以理解的神秘人物，总有种敬而远之的心态。相对来说，我当时更愿意去了解其他人，包括爱因斯坦、居里夫人、费曼等科学家。\n这一次，我终于打开了牛顿的传记，阅读欲望很强烈，一气呵成地读完了全书。牛顿的生平非常丰富。首先，他寿命很长，活了 84 岁，这在当时属于非常难得的高寿。他的经历也很丰富。他出身不算差，后来在读书过程中一度并不出色，但突然奋发，从此之后就一直是最优秀的学生。后来他从事学术，接替了巴罗的位置，成为卢卡斯讲座的教授。他在20多岁的时候就做出了非常突出的成就，比如著名的万有引力，成了明星学者，并能够出入非常高级别的场所，在后来还成了政府重要的官员和国王的座上宾，成为全欧洲的话题人物。\n我们对他的了解更多的是他作为科学家的成就，比如提出了万有引力定律、牛顿三定律以及光学原理，当然还有微积分。除此之外，我们对他的了解相对较少，偶尔听说他对炼金术很感兴趣，并且他非常虔诚，是一个清教徒。此外，他还炒股，有着惊世骇俗的韭菜经历，他一度血赚，最终血亏，最终成为了南海金融泡沫的受难者。关于他作为皇家造币厂厂长的经历，我也有所了解，但了解得并不多。通过这些事情，我所了解的还是一个比较片面、比较干瘪的牛顿。而这次读他的传记，让我对他这个人生框架有了更加丰富、多维的感知。\n通过这部传记，可以感觉到牛顿脾气古怪。他性格孤僻，不喜欢教书，求胜欲很强，经常与一些人陷入难以言说的争斗。他非常小气、极端记仇，弱势的时候隐藏自己，强势的时候重拳出击，终身热爱玩弄权术。这些特质在他与莱布尼兹和胡克的争斗中有所体现。通过牛顿的视角，胡克可能是一个十恶不赦的小人，但由于对那些历史缺乏全面的了解，也难说清他俩之间的是非曲折。\n以上特质对一个人来说不够光彩，但对牛顿来说倒无所谓。因为他在科学上的贡献太突出了，整个现代社会都蒙受他的恩泽，相比之下性格上的缺点完全可以只被当作补充性的谈资。当然，牛顿也有很多朋友，如巴罗、惠更斯、哈雷和洛克等人，这些人给了他很多帮助，也在某种程度上弥补了他性格上的一些弱点。\n我们更应了解牛顿作为科研工作者的一些特质。他对科学的影响很深刻，一方面在于他提炼出的重要结论，这些结论奠定了未来数学和物理学的基础；另一方面在方法论层面，他对理论的简洁性和可理解性的追求非常执着。此外，他也是比较早追求理论可证伪性的学者。比如，他在《光学》这本书的出版上，除了列举他的研究工作之外，还增加了一些批判性的内容，以“质疑”的形式体现，并且每一次改版都会增加很多质疑内容。这些批判性内容出现在他自己书上是很有趣的，显得严谨、自信而强大。\n牛顿的动手能力极强，这些可能是被忽视的。他善于设计实验来对他的想法进行验证，也经常做一些发明创造，比如自己设计与研制三棱镜从而研究光学规律。他在担任皇家造币局的厂长期间，对硬币的设计与生产进行了革新，大大地打击了假币产业，很多发明甚至沿用至今。担任造币厂的经历体现了他出人意料的管理能力，但这并不是孤例。牛顿后来还拯救了声明濒临扫地的皇家科学院，只不过这段作为院长的履职经历也让该科学院打上了过深的牛顿烙印，很多人也承袭了牛顿性格上的缺点（或许这就是所谓的“顿感力”吧）。\n此外，牛顿非常勤奋。无论是做科研还是做皇家造币厂厂长，都能看出他是一个名副其实的“卷王”，工作态度认真，对细节一丝不苟。所以我们不能单纯地只认为他的伟大成就是建立在他天赋的基础之上，他的努力程度也是绝大多数人无法企及的。\n牛顿还有一个强大的地方就在于他的身体特别好。他几乎没有患上当时常见的一些疾病，而且去世时依然有两排健康的牙齿，这时他已经 84 岁了，这很令人惊叹（反观同时代同样长寿的路易十四\u0026hellip;）。另一个能体现他身体好的地方在于，在他的研究工作中，事实上花了很长时间进行炼金术的研究。在这个过程中，他做了大量实验，需要接触大量的重金属，这通常会对身体产生负面影响。但是，即便在牛顿体内检测出超标的重金属，他依然能够保持长寿和健康的状态。他也是到了八十岁左右身体才开始明显地被疾病困扰。更值得一提的是，他一生都保持着清醒的大脑，思维能力到了很晚的时候才开始有衰退迹象。\n牛顿职业生涯中，曾花费大量时间进行当 …"},{"title":"下载并打开你的应用","url":"/courses/build-with-andrew-ng-course/03.-download-and-open-your-app/","section":"courses","date":"2025-01-08","summary":"下载并打开你的应用 按照这三个步骤下载代码文件，在你的计算机上找到它，并在 Web 浏览器中打开它。\n第一步：下载你的代码 使用 DeepLearning.AI 聊天机器人（推荐） 在聊天机器人创建你的 HTML 代码后，你会看到它显示在聊天机器人屏幕上的灰色框中。\n定位代码框 - 它将包含你所有的 HTML 代码，看起来像一个灰色矩形区域 找到下载图标 - 在代码框的右上角查找此图标：下载图标 点击下载图标 - 你的浏览器将自动下载文件 注意确认 - 你应该在浏览器底部看到一个通知，显示文件正在下载 文件已保存 - 它应该自动保存到你的下载文件夹 注意：文件名将类似于 file.html 或具有类似的以 .html 结尾的名称。\n第二步：找到你下载的文件 接下来，定位你的计算机将文件保存到哪里。\n在 Windows 上： 方法 1：使用你的浏览器（最简单）\n查看浏览器窗口的底部 - 你应该看到那里列出了下载的文件 点击文件名旁边的小箭头 选择「在文件夹中显示」 一个窗口将打开，准确显示你的文件在哪里 方法 2：直接打开下载文件夹\n同时按下 Windows 键和 E（这将打开文件资源管理器） 在窗口的左侧，点击「下载」 查找名为 file.html 的文件 - 这是你的应用程序！ 在 Mac 上： 方法 1：使用你的浏览器（最简单）\n在浏览器右上角附近查找下载箭头图标 点击箭头查看你最近的下载 在列表中找到你的文件 点击放大镜图标或选择「在 Finder 中显示」 一个窗口将打开，准确显示你的文件在哪里 方法 2：直接打开下载文件夹\n","description":"","tags":["translated","AI","tutorial","guide"],"categories":[],"content":"下载并打开你的应用 按照这三个步骤下载代码文件，在你的计算机上找到它，并在 Web 浏览器中打开它。\n第一步：下载你的代码 使用 DeepLearning.AI 聊天机器人（推荐） 在聊天机器人创建你的 HTML 代码后，你会看到它显示在聊天机器人屏幕上的灰色框中。\n定位代码框 - 它将包含你所有的 HTML 代码，看起来像一个灰色矩形区域 找到下载图标 - 在代码框的右上角查找此图标：下载图标 点击下载图标 - 你的浏览器将自动下载文件 注意确认 - 你应该在浏览器底部看到一个通知，显示文件正在下载 文件已保存 - 它应该自动保存到你的下载文件夹 注意：文件名将类似于 file.html 或具有类似的以 .html 结尾的名称。\n第二步：找到你下载的文件 接下来，定位你的计算机将文件保存到哪里。\n在 Windows 上： 方法 1：使用你的浏览器（最简单）\n查看浏览器窗口的底部 - 你应该看到那里列出了下载的文件 点击文件名旁边的小箭头 选择「在文件夹中显示」 一个窗口将打开，准确显示你的文件在哪里 方法 2：直接打开下载文件夹\n同时按下 Windows 键和 E（这将打开文件资源管理器） 在窗口的左侧，点击「下载」 查找名为 file.html 的文件 - 这是你的应用程序！ 在 Mac 上： 方法 1：使用你的浏览器（最简单）\n在浏览器右上角附近查找下载箭头图标 点击箭头查看你最近的下载 在列表中找到你的文件 点击放大镜图标或选择「在 Finder 中显示」 一个窗口将打开，准确显示你的文件在哪里 方法 2：直接打开下载文件夹\n点击程序坞中的 Finder 图标（蓝色笑脸） 在窗口的左侧，点击「下载」 查找名为 file.html 的文件 - 这是你的应用程序！ 你正在寻找的内容：名称类似于 index.html、birthday-card.html 或 file.html 的文件\n第三步：打开你的应用程序 快到了！现在你只需要在 Web 浏览器中打开文件。\nWindows 和 Mac 都适用： 导航到你的文件 - 使用上述方法之一找到你的 .html 文件 双击文件 - 就像打开计算机上的任何其他文件一样 你的应用程序打开了！ - 文件将自动在你的默认 Web 浏览器（Chrome、Safari、Edge、Firefox 等）中打开 开始使用它 - 你的应用程序现在正在运行并准备使用！ 你会看到的内容：你的 Web 浏览器将打开并显示你的应用程序，就像访问网站一样\n使用不同的聊天机器人？ 如果你改用 Claude、ChatGPT 或 Gemini，请点击此处\n遇到问题了？ 常见问题和解决方案： 「我在任何地方都找不到我的文件！」 首先检查你的下载文件夹（这是大多数文件存放的地方） 检查你的桌面 - 有时文件保存在这里 检查你的文档文件夹 查看浏览器的下载历史 - 点击浏览器中的下载箭头/图标 「当我双击文件时，它以文本/代码而不是网页的形式打开」 文件在文本编辑器而不是浏览器中打开了 解决方法：右键单击文件 → 选择「打开方式」→ 选择你的 Web 浏览器（Chrome、Safari、Firefox、Edge） 「我的文件叫 something.txt 而不是 something.html」 从文本编辑器保存时会发生这种情况 解决方法：重命名文件 - 右键单击 → 重命名 → 确保它以 .html 而不是 .txt 结尾 「我多次下载它，现在我有许多副本」 你的计算机会添加数字以防止覆盖（如 index(1).html、index(2).html） 解决方法：使用编号最高的那个 - 那是最新版本 如果你愿意，可以删除旧版本 「应用程序打开了，但它看起来很奇怪且有问题」 确保你将其保存为 .html 而不是 .txt 确保你从聊天机器人复制了所有代码 在 Mac 上，确保在保存之前在文本编辑中使用了「制作纯文本」 有用提示 保持文件井井有条 在你的文档文件夹中创建一个名为「My Apps」的文件夹 将所有 HTML 文件保存在那里，以便你可以轻松找到它们 为文件提供描述性名称 与其使用 index.html，不如将其命名为 birthday-card-app.html 或 todo-list.html 这有助于你记住每个应用程序的作用 你的应用程序可以离线工作 一旦文件保存，你就不需要互联网来使用它 只需随时双击文件即可运行你的应用程序 你可以随时进行更改 回到聊天机器人，告诉它你想要更改什么 下载更新的版本 它将替换旧文件（或以不同的名称保存） 此文章由 AI 翻译\n"},{"title":"你现在是构建者了","url":"/courses/build-with-andrew-ng-course/06.-youre-a-builder-now/","section":"courses","date":"2025-01-08","summary":"你现在是构建者了 如果你已经走到这一步，恭喜你，你现在是一个 AI 构建者了。我见过一些人显然已经用 AI 构建软件好几个月了，但他们仍然不确定自己是否真的是一个构建者。我在这里告诉你，是的，你是，我把你算作我们中的一员，一个构建者。\n我希望你做最后一个活动并获得这门课程的证书。最后一个项目是制作你自己的填空故事生成器。我希望你使用提示词构建块来构建你自己的填空应用程序。最后一个项目必须包括三到五个输入字段、一个接受输入然后生成输出的按钮，以及一个显示输出的地方。\n你已经看过生日贺卡应用了。这里有一些其他想法，可能给你一些启发。这是一个有趣的产品评论生成器，空白是产品名称、一些数字、一个名词、一个身体部位等等，它会生成产品评论。注意它说这里有看起来像这样的完全诚实的产品评论。如果你想，请随意暂停视频阅读这个，但我很喜欢这个。\n或者这里是另一个请假申请生成器，你指定你想要请多少时间假、一个名词、一个身体部位、一个物品，然后它说我需要整整一个月的假，立即生效，因为复古评论需要我紧急关注。\n所以无论是生日贺卡、产品评论、请假，还是其他类型的故事生成器，请编写你自己的提示词并使用它来生成像这样的应用程序。\n在网站上，当你进入下一个项目时，你会看到通常的聊天机器人，你可以用它来编写提示词，获取 HTML 文件，并将其下载到你自己的计算机上。请这样做，运行它，看看它是否令你满意。你可以使用我们的网站，或者也欢迎使用外部网站，如 ChatGPT 或 Gemini 来生成你的 HTML 文件。\n当你完成构建这个项目时，请进入这个最终页面。加载可能需要一秒钟。然后点击「上传文件」。在这里我将导航到我下载的文件。这里是我实际生成的应用程序的预览。\n当你准备好时，点击「提交作业」。我们的 AI 将查看你的 HTML 文件，看看它似乎是否正确工作，并给你任何反馈。假设它有效，这将带你进入这门课程的最后一个练习，并获得证书。\n最有效的构建者继续参加课程并继续构建。我发现，如果有人只是构建，他们最终往往不知道核心概念，最终可能花费数月重新发明轮子或更糟糕的是，以非常奇怪的方式做事。\n相反，如果有人只是参加课程，那么他们最终会得到没有知道如何应用它的理论知识。所以构建和参加课程都很重要。请继续构建你喜欢的任何东西。我还将在下一个项目中为你建议一些额外的课程供你考虑。\n继续与朋友分享你的应用程序以获得反馈，或者只是为了让他们笑一下。成为一个构建者是世界上最有趣的事情之一。我很高兴你和我一起开始了这段旅程，我希望我们继续一起构建和学习。\n此文章由 AI 翻译\n","description":"","tags":["translated","AI","course","motivation"],"categories":[],"content":"你现在是构建者了 如果你已经走到这一步，恭喜你，你现在是一个 AI 构建者了。我见过一些人显然已经用 AI 构建软件好几个月了，但他们仍然不确定自己是否真的是一个构建者。我在这里告诉你，是的，你是，我把你算作我们中的一员，一个构建者。\n我希望你做最后一个活动并获得这门课程的证书。最后一个项目是制作你自己的填空故事生成器。我希望你使用提示词构建块来构建你自己的填空应用程序。最后一个项目必须包括三到五个输入字段、一个接受输入然后生成输出的按钮，以及一个显示输出的地方。\n你已经看过生日贺卡应用了。这里有一些其他想法，可能给你一些启发。这是一个有趣的产品评论生成器，空白是产品名称、一些数字、一个名词、一个身体部位等等，它会生成产品评论。注意它说这里有看起来像这样的完全诚实的产品评论。如果你想，请随意暂停视频阅读这个，但我很喜欢这个。\n或者这里是另一个请假申请生成器，你指定你想要请多少时间假、一个名词、一个身体部位、一个物品，然后它说我需要整整一个月的假，立即生效，因为复古评论需要我紧急关注。\n所以无论是生日贺卡、产品评论、请假，还是其他类型的故事生成器，请编写你自己的提示词并使用它来生成像这样的应用程序。\n在网站上，当你进入下一个项目时，你会看到通常的聊天机器人，你可以用它来编写提示词，获取 HTML 文件，并将其下载到你自己的计算机上。请这样做，运行它，看看它是否令你满意。你可以使用我们的网站，或者也欢迎使用外部网站，如 ChatGPT 或 Gemini 来生成你的 HTML 文件。\n当你完成构建这个项目时，请进入这个最终页面。加载可能需要一秒钟。然后点击「上传文件」。在这里我将导航到我下载的文件。这里是我实际生成的应用程序的预览。\n当你准备好时，点击「提交作业」。我们的 AI 将查看你的 HTML 文件，看看它似乎是否正确工作，并给你任何反馈。假设它有效，这将带你进入这门课程的最后一个练习，并获得证书。\n最有效的构建者继续参加课程并继续构建。我发现，如果有人只是构建，他们最终往往不知道核心概念，最终可能花费数月重新发明轮子或更糟糕的是，以非常奇怪的方式做事。\n相反，如果有人只是参加课程，那么他们最终会得到没有知道如何应用它的理论知识。所以构建和参加课程都很重要。请继续构建你喜欢的任何东西。我还将在下一个项目中为你建议一些额外的课程供你考虑。\n继续与朋友分享你的应用程序以获得反馈，或者只是为了让他们笑一下。成为一个构建者是世界上最有趣的事情之一。我很高兴你和我一起开始了这段旅程，我希望我们继续一起构建和学习。\n此文章由 AI 翻译\n"},{"title":"使用 AI 构建应用","url":"/courses/build-with-andrew-ng-course/01.-creating-an-app-with-ai/","section":"courses","date":"2025-01-08","summary":"使用 AI 构建应用 嗨，我是 Andrew Ng。我想向你展示如何使用 AI 构建有趣且实用的软件，无论你的背景或训练如何。如果你对使用 AI 或 Vibe Coding（氛围编程）构建很酷的软件感兴趣，或者不确定从哪里开始，这门课程就是为你准备的。\n让我向你展示一个有趣的生日贺卡应用，你将在几分钟内学会使用 ChatGPT、Gemini 或任何其他类似的 AI 系统来构建它，即使你之前从未写过一行代码。\n这个生日贺卡应用是一个称为 Web 应用（Web Application）的计算机程序，意味着它直接在你的 Web 浏览器中运行。该应用接收你输入的任何内容，并立即为你创建一张定制卡片。\n在下一节课中，你将通过描述你想要 AI 做什么来构建你自己的版本，AI 将为你编写所有代码。\n要使用这个应用，你需要输入姓名、年龄、爱好（比如企鹅时尚设计）、一个形容词（比如可疑的有趣）、以及一个复数名词（比如幸运袜子），然后点击「生成卡片」按钮。\n它会显示：Karen，克服一切困难，你已经 27 岁了。你对企鹅时尚设计的奉献——这已经成为传奇——以及你那可疑的有趣性格，真是标志性的人物。从统计上讲，大多数人都喜欢你。\n我觉得这很有趣。或者如果我不想自己填写所有这些词语，我可以点击「手气不错」，让它自动填充，然后生成一张不同的卡片。这里是另一张生日贺卡。这次你会得到一个不同的有趣故事或不同的有趣生日贺卡。\n注意这个应用会存储我们生成的所有卡片。如果我点击其中任何一张的「复制」按钮，生日消息就会复制到我的剪贴板，这样我就可以将消息粘贴到电子邮件或短信中，作为生日贺卡发送。\n你可能迫不及待地想要直接开始构建你自己的想法。如果是这样，那太好了。但在这样做之前，花一点时间自己构建这个生日贺卡应用会非常有帮助。这样做会让你对如何与 AI 对话有更好的感觉，并真正得到你想要的结果。\n这个练习过程将建立你的直觉，让你理解如何塑造 AI 实际创建的内容，你会更好地理解即使是微小的调整也可能导致非常不同的结果。一旦你对这些概念更加熟悉，构建你自己的想法就会变得更快、更容易，因为你对如何引导这个过程有了更好的感觉。\n这样的应用看起来可能构建起来很复杂，但在下一个视频中，我将向你展示如何在几分钟内构建类似的东西。让我们继续看下一个视频。\n此文章由 AI 翻译\n","description":"","tags":["translated","AI","course","no-code"],"categories":[],"content":"使用 AI 构建应用 嗨，我是 Andrew Ng。我想向你展示如何使用 AI 构建有趣且实用的软件，无论你的背景或训练如何。如果你对使用 AI 或 Vibe Coding（氛围编程）构建很酷的软件感兴趣，或者不确定从哪里开始，这门课程就是为你准备的。\n让我向你展示一个有趣的生日贺卡应用，你将在几分钟内学会使用 ChatGPT、Gemini 或任何其他类似的 AI 系统来构建它，即使你之前从未写过一行代码。\n这个生日贺卡应用是一个称为 Web 应用（Web Application）的计算机程序，意味着它直接在你的 Web 浏览器中运行。该应用接收你输入的任何内容，并立即为你创建一张定制卡片。\n在下一节课中，你将通过描述你想要 AI 做什么来构建你自己的版本，AI 将为你编写所有代码。\n要使用这个应用，你需要输入姓名、年龄、爱好（比如企鹅时尚设计）、一个形容词（比如可疑的有趣）、以及一个复数名词（比如幸运袜子），然后点击「生成卡片」按钮。\n它会显示：Karen，克服一切困难，你已经 27 岁了。你对企鹅时尚设计的奉献——这已经成为传奇——以及你那可疑的有趣性格，真是标志性的人物。从统计上讲，大多数人都喜欢你。\n我觉得这很有趣。或者如果我不想自己填写所有这些词语，我可以点击「手气不错」，让它自动填充，然后生成一张不同的卡片。这里是另一张生日贺卡。这次你会得到一个不同的有趣故事或不同的有趣生日贺卡。\n注意这个应用会存储我们生成的所有卡片。如果我点击其中任何一张的「复制」按钮，生日消息就会复制到我的剪贴板，这样我就可以将消息粘贴到电子邮件或短信中，作为生日贺卡发送。\n你可能迫不及待地想要直接开始构建你自己的想法。如果是这样，那太好了。但在这样做之前，花一点时间自己构建这个生日贺卡应用会非常有帮助。这样做会让你对如何与 AI 对话有更好的感觉，并真正得到你想要的结果。\n这个练习过程将建立你的直觉，让你理解如何塑造 AI 实际创建的内容，你会更好地理解即使是微小的调整也可能导致非常不同的结果。一旦你对这些概念更加熟悉，构建你自己的想法就会变得更快、更容易，因为你对如何引导这个过程有了更好的感觉。\n这样的应用看起来可能构建起来很复杂，但在下一个视频中，我将向你展示如何在几分钟内构建类似的东西。让我们继续看下一个视频。\n此文章由 AI 翻译\n"},{"title":"加入 DeepLearning.AI 论坛","url":"/courses/build-with-andrew-ng-course/08.-join-deeplearning-forum/","section":"courses","date":"2025-01-08","summary":"加入 DeepLearning.AI 论坛 加入 DeepLearning.AI 论坛以提问、获得支持或分享惊人的想法！\n当你深入学习材料时，我们鼓励你加入论坛以：\n获得支持 询问有关课程内容的问题、获得作业指导，或为任何技术问题寻找帮助。我们的社区和经验丰富的导师随时准备提供帮助！\n协作与分享 讨论惊人的想法、分享你的项目，并与热衷于 AI 的其他学习者建立联系。\n保持最新 获取有关新课程、特别活动和 DeepLearning.AI 新闻的最新更新。\n我们建立这个社区是为了成为每个人欢迎和支持的空间，我们很高兴你能成为其中的一部分。\n论坛新用户？ 请使用此链接创建你的免费账户：账户创建链接\n已经是会员？ 你可以在这里登录并直接进入讨论：论坛登录链接\n需要发帖帮助？ 查看这个关于如何开始新主题的 1 分钟视频：论坛主题视频链接\n我们期待在论坛中见到你！\n此文章由 AI 翻译\n","description":"","tags":["translated","community","forum","AI"],"categories":[],"content":"加入 DeepLearning.AI 论坛 加入 DeepLearning.AI 论坛以提问、获得支持或分享惊人的想法！\n当你深入学习材料时，我们鼓励你加入论坛以：\n获得支持 询问有关课程内容的问题、获得作业指导，或为任何技术问题寻找帮助。我们的社区和经验丰富的导师随时准备提供帮助！\n协作与分享 讨论惊人的想法、分享你的项目，并与热衷于 AI 的其他学习者建立联系。\n保持最新 获取有关新课程、特别活动和 DeepLearning.AI 新闻的最新更新。\n我们建立这个社区是为了成为每个人欢迎和支持的空间，我们很高兴你能成为其中的一部分。\n论坛新用户？ 请使用此链接创建你的免费账户：账户创建链接\n已经是会员？ 你可以在这里登录并直接进入讨论：论坛登录链接\n需要发帖帮助？ 查看这个关于如何开始新主题的 1 分钟视频：论坛主题视频链接\n我们期待在论坛中见到你！\n此文章由 AI 翻译\n"},{"title":"告诉 AI 你想要什么","url":"/courses/build-with-andrew-ng-course/02.-telling-ai-what-you-want/","section":"courses","date":"2025-01-08","summary":"告诉 AI 你想要什么 在 AI 时代，创建软件最简单的方式不再是自己输入代码，而是应该告诉 AI 让它为你完成。告诉 AI 做什么称为提示词（Prompting）。\n当给出精确的指令时，AI 可以为你做很多事情。在这个视频中，我想与你分享一些使用提示词让 AI 为你创建软件的最佳实践。\n首先，我们将一起过一遍一个例子。然后，你可以使用任何 AI 聊天系统自己尝试，比如 ChatGPT、Gemini、Claude 或本网站内置的系统。\n要使用这些 AI 系统中的任何一个，你需要给它一个提示词或一组指令。告诉它创建一个网页来帮助我写生日贺卡，当我提供一个人的姓名、年龄和爱好时，它应该给我一个有趣的消息。如果你这样做，它可能会生成一个看起来像这样的应用，这是一个不错的开始。你可以输入姓名、年龄和爱好，它会生成像这样的一个还可以的消息。\n但如果你不满意，你可以继续与 AI 对话说：通过添加一个节日标题和颜色让它更漂亮。这会给你第二个版本的应用，现在看起来更好一些。如果你仍然不满意，你可能会说：把颜色显示在右侧，让它看起来像在生日贺卡内部，你会得到第三个版本。如果你有让它变得更好的想法，你可以给它更多的指令，比如在顶部添加一个有趣的标题等等。\n这就是我在实践中使用 AI 为我编写代码时的工作方式。我通常从一组基本的指令开始，看看我能得到什么，然后反复告诉 AI 我想要如何改进它。\n事实证明，在构建软件应用程序时，你会最终在提示词中包含一些基本构建块。我经常包含的一件事是目标（Goal）。所以这里是创建一个网页来帮助我写生日贺卡的目标。\n你可能包含的另一件事是指定输入（Input）是什么，即用户必须告诉软件什么。这是我们告诉软件我们想要它输出什么，以及布局（Layout）。所以左边是什么？右边是什么？你如何安排这个应用的不同部分？最后，你想要包含的任何额外功能的特殊指令。\n有很多方法可以写出好的提示词，但当你开始告诉 AI 为你构建软件的旅程时，我鼓励你考虑这五个构建块作为你可能选择在提示词中包含的常见部分。即目标——你想要创建什么、用户将提供什么输入、软件应用程序的布局是什么、你想要什么特殊功能，以及你想要软件输出什么。\n在上一张幻灯片中，我们经历了一个来回的过程，我分四个步骤逐步添加我给 AI 的指令，告诉它我想要做什么。但如果你已经提前大致知道你想要构建什么，你也可以在单个提示词中指定所有构建块。\n例如，如果我已经知道我想要的软件规格，我可以写一个像这样的单个、更长的提示词来创建一个网页帮助我写生日贺卡，无论人的姓名、年龄和爱好是什么，你有一个有趣的消息，你指定颜色等等。在这个例子中，我采用了所有五个构建块，并将它们全部写到一个单个的、更长的提示词中。\n所以，不需要像你在上一个视频中看到的那样逐步构建东西，你也可以在单个、更长的提示词中给出所有指令，这可能会给你更好的第一个版本的应用，如果它还不是完全你想要的，你可以进一步改进它。\n现在，无论你是一次写一个单个的长提示词，还是逐步一个一个地给出构建块，我通常会先告诉 AI 我的目标是什么，然后在剩下的构建块中，有多种方法可以将它们组合在一起，你不必每次都使用所有构建块，顺序也不是很重要。\n从目标开始，告诉输入、输出、布局，可能不列出任何特殊功能。或者你可以用这种方式将构建块组合在一起，它可能工作正常。或者你可以按不同的顺序列出构建块，AI 通常相当擅长理解这些构建块的不同重新排列。\n如果你觉得，哇，这太多了，我会说别担心。如果你只是告诉 AI 你脑子里的任何东西，即使是部分的和不完美的，你随后可以与 AI 来回几次工作，将它打磨成你想要的东西。\n你随着时间的推移磨练的技能之一是给 AI 更具体指令的能力，因为事实证明，即使你给出相当具体的提示词，你得到的结果也可能有所不同。\n所以这里是你刚刚看到的一个长的、详细的提示词，指定了所有五个构建块。如果你多次给同一个 AI 系统相同的指令，可能有一次它会给你一个看起来像这样的应用，这相当不错，可能第二次它会构建像这样的东西，第三次它会构建像这样的东西。所有这些看起来都相当不错。我们可以看到它们之间有一些变化。\n相比之下，如果有人给出一个不太具体的提示词，一个不太清晰的提示词，这是一个非常短的提示词，只是说创建一个网页来帮助我写生日贺卡。因为这些是相对模糊的提示词，如果你通过 AI 系统多次运行它，你可能得到的结果可能一次是这样，第二次是这样，看起来完全不同，有不同的字段，第三次你会得到这个，又与前两次完全不同。\n你写的提示词越具体、越精确，结果就越可预测。但即使如此，也会有一些变化。所以如果你得到的结果与我在这个视频中显示的有点不同，别担心。这是 AI 系统行为的正常原因。\n但如果它给你一些你真的不喜欢的惊喜，那也没关系。只需给它额外的指令来引导或移动 AI 更接近你真正想要它做的事情。\n学习这个的最好方法是把你的手放在键盘上，自己尝试使用 AI。让我向你展示它会是什么样子。我希望你在这个视频之后做的是进入网站的这个部分，自己完成这个练习。\n这里的指令你可以稍后阅读，但这是一个类似于 ChatGPT、Gemini、Claude 等的 AI 系统。我将选择然后复制并粘贴这里的第一个提示词，告诉 AI 创建一个网页，告诉我写生日贺卡等等。我将点击这个将它发送给 AI。\n","description":"","tags":["translated","AI","prompting","course"],"categories":[],"content":"告诉 AI 你想要什么 在 AI 时代，创建软件最简单的方式不再是自己输入代码，而是应该告诉 AI 让它为你完成。告诉 AI 做什么称为提示词（Prompting）。\n当给出精确的指令时，AI 可以为你做很多事情。在这个视频中，我想与你分享一些使用提示词让 AI 为你创建软件的最佳实践。\n首先，我们将一起过一遍一个例子。然后，你可以使用任何 AI 聊天系统自己尝试，比如 ChatGPT、Gemini、Claude 或本网站内置的系统。\n要使用这些 AI 系统中的任何一个，你需要给它一个提示词或一组指令。告诉它创建一个网页来帮助我写生日贺卡，当我提供一个人的姓名、年龄和爱好时，它应该给我一个有趣的消息。如果你这样做，它可能会生成一个看起来像这样的应用，这是一个不错的开始。你可以输入姓名、年龄和爱好，它会生成像这样的一个还可以的消息。\n但如果你不满意，你可以继续与 AI 对话说：通过添加一个节日标题和颜色让它更漂亮。这会给你第二个版本的应用，现在看起来更好一些。如果你仍然不满意，你可能会说：把颜色显示在右侧，让它看起来像在生日贺卡内部，你会得到第三个版本。如果你有让它变得更好的想法，你可以给它更多的指令，比如在顶部添加一个有趣的标题等等。\n这就是我在实践中使用 AI 为我编写代码时的工作方式。我通常从一组基本的指令开始，看看我能得到什么，然后反复告诉 AI 我想要如何改进它。\n事实证明，在构建软件应用程序时，你会最终在提示词中包含一些基本构建块。我经常包含的一件事是目标（Goal）。所以这里是创建一个网页来帮助我写生日贺卡的目标。\n你可能包含的另一件事是指定输入（Input）是什么，即用户必须告诉软件什么。这是我们告诉软件我们想要它输出什么，以及布局（Layout）。所以左边是什么？右边是什么？你如何安排这个应用的不同部分？最后，你想要包含的任何额外功能的特殊指令。\n有很多方法可以写出好的提示词，但当你开始告诉 AI 为你构建软件的旅程时，我鼓励你考虑这五个构建块作为你可能选择在提示词中包含的常见部分。即目标——你想要创建什么、用户将提供什么输入、软件应用程序的布局是什么、你想要什么特殊功能，以及你想要软件输出什么。\n在上一张幻灯片中，我们经历了一个来回的过程，我分四个步骤逐步添加我给 AI 的指令，告诉它我想要做什么。但如果你已经提前大致知道你想要构建什么，你也可以在单个提示词中指定所有构建块。\n例如，如果我已经知道我想要的软件规格，我可以写一个像这样的单个、更长的提示词来创建一个网页帮助我写生日贺卡，无论人的姓名、年龄和爱好是什么，你有一个有趣的消息，你指定颜色等等。在这个例子中，我采用了所有五个构建块，并将它们全部写到一个单个的、更长的提示词中。\n所以，不需要像你在上一个视频中看到的那样逐步构建东西，你也可以在单个、更长的提示词中给出所有指令，这可能会给你更好的第一个版本的应用，如果它还不是完全你想要的，你可以进一步改进它。\n现在，无论你是一次写一个单个的长提示词，还是逐步一个一个地给出构建块，我通常会先告诉 AI 我的目标是什么，然后在剩下的构建块中，有多种方法可以将它们组合在一起，你不必每次都使用所有构建块，顺序也不是很重要。\n从目标开始，告诉输入、输出、布局，可能不列出任何特殊功能。或者你可以用这种方式将构建块组合在一起，它可能工作正常。或者你可以按不同的顺序列出构建块，AI 通常相当擅长理解这些构建块的不同重新排列。\n如果你觉得，哇，这太多了，我会说别担心。如果你只是告诉 AI 你脑子里的任何东西，即使是部分的和不完美的，你随后可以与 AI 来回几次工作，将它打磨成你想要的东西。\n你随着时间的推移磨练的技能之一是给 AI 更具体指令的能力，因为事实证明，即使你给出相当具体的提示词，你得到的结果也可能有所不同。\n所以这里是你刚刚看到的一个长的、详细的提示词，指定了所有五个构建块。如果你多次给同一个 AI 系统相同的指令，可能有一次它会给你一个看起来像这样的应用，这相当不错，可能第二次它会构建像这样的东西，第三次它会构建像这样的东西。所有这些看起来都相当不错。我们可以看到它们之间有一些变化。\n相比之下，如果有人给出一个不太具体的提示词，一个不太清晰的提示词，这是一个非常短的提示词，只是说创建一个网页来帮助我写生日贺卡。因为这些是相对模糊的提示词，如果你通过 AI 系统多次运行它，你可能得到的结果可能一次是这样，第二次是这样，看起来完全不同，有不同的字段，第三次你会得到这个，又与前两次完全不同。\n你写的提示词越具体、越精确，结果就越可预测。但即使如此，也会有一些变化。所以如果你得到的结果与我在这个视频中显示的有点不同，别担心。这是 AI 系统行为的正常原因。\n但如果它给你一些你真的不喜欢的惊喜，那也没关系。只需给它额外的指 …"},{"title":"推荐课程","url":"/courses/build-with-andrew-ng-course/07.-recommended-courses/","section":"courses","date":"2025-01-08","summary":"推荐课程 成功构建你的第一个应用程序后，以下是一些可以帮助你继续软件开发和 AI 之路的课程：\n课程：面向所有人的生成式 AI 学习生成式 AI 的工作原理，以及如何在你的生活和工作中使用它。了解生成式 AI 对商业和社会的影响，以制定有效的 AI 策略。\n提供方：DeepLearning.AI\n查看课程 →\n短课程：AI Python 初学者 学习 Python 编程基础知识以及如何集成 AI 工具进行数据操作、分析和可视化。从第一天开始构建实用的 AI 应用程序。\n提供方：DeepLearning.AI\n查看课程 →\n短课程：使用 Replit 的氛围编程 101 在使用 AI 编码代理调试、定制和加强你的编码技能的同时构建和共享应用程序。学习与编码代理合作的有效策略。\n提供方：Replit\n查看课程 →\n短课程：面向开发人员的 ChatGPT 提示词工程 学习应用程序开发的提示词工程最佳实践。了解如何使用 LLM 和 OpenAI API 快速构建新的强大应用程序。\n提供方：OpenAI\n查看课程 →\n短课程：与大语言模型结对编程 使用 LLM 简化你的代码并成为更高效的软件工程师。学习编写测试用例、调试和重构代码，以及记录复杂的代码库。\n提供方：Google\n查看课程 →\n此文章由 AI 翻译\n","description":"","tags":["translated","AI","courses","learning"],"categories":[],"content":"推荐课程 成功构建你的第一个应用程序后，以下是一些可以帮助你继续软件开发和 AI 之路的课程：\n课程：面向所有人的生成式 AI 学习生成式 AI 的工作原理，以及如何在你的生活和工作中使用它。了解生成式 AI 对商业和社会的影响，以制定有效的 AI 策略。\n提供方：DeepLearning.AI\n查看课程 →\n短课程：AI Python 初学者 学习 Python 编程基础知识以及如何集成 AI 工具进行数据操作、分析和可视化。从第一天开始构建实用的 AI 应用程序。\n提供方：DeepLearning.AI\n查看课程 →\n短课程：使用 Replit 的氛围编程 101 在使用 AI 编码代理调试、定制和加强你的编码技能的同时构建和共享应用程序。学习与编码代理合作的有效策略。\n提供方：Replit\n查看课程 →\n短课程：面向开发人员的 ChatGPT 提示词工程 学习应用程序开发的提示词工程最佳实践。了解如何使用 LLM 和 OpenAI API 快速构建新的强大应用程序。\n提供方：OpenAI\n查看课程 →\n短课程：与大语言模型结对编程 使用 LLM 简化你的代码并成为更高效的软件工程师。学习编写测试用例、调试和重构代码，以及记录复杂的代码库。\n提供方：Google\n查看课程 →\n此文章由 AI 翻译\n"},{"title":"现在你可以构建更多应用","url":"/courses/build-with-andrew-ng-course/05.-build-more-apps/","section":"courses","date":"2025-01-08","summary":"现在你可以构建更多应用 使用你现在拥有的提示词技能，让我们构建另一个应用程序。这一次，是一个乒乓球游戏。\n由于 AI 的存在，现在可能从想法到工作应用程序，有时只需几分钟。计算机历史上最早的电子游戏之一是一个叫 Pong 的游戏，这是一种两人乒乓球游戏，团队花了数周时间才构建完成。但现在，多亏了 AI，你可以在几分钟内构建类似的东西。\n让我们应用我们学到的提示词技术。我将快速开始并编写一个中等具体的提示词，说：给我构建一个乒乓球游戏作为单个 HTML 文件，使用玩家对战计算机模式，用方向键移动球拍。如果你这样做，你可能会得到一个看起来像这样的应用程序的第一个版本。我想，好的，这是一个不错的开始，但我想要它添加三个难度级别，让用户指定获胜所需的点数，并保持比分。这为我构建了第二个版本。\n这现在看起来像一个有趣的游戏，但我希望图形更花哨。所以我将说：让游戏区域变成绿色，球拍变成米色，球变成白色，并告诉它将这张图片插入背景。所以我最终得到了一个看起来像这样的游戏。\n这是游戏。实际上很有趣。球来回反弹。我实际上可以玩很长时间，可能比你真正想看我玩的时间更长。但我希望你会享受这个。提醒一下，在编写提示词时具体会得到更好的结果。所以你可以看看我使用的提示词，它在许多细节上相当具体，比如颜色、点数等等。\n如果你不确定提示词中要包含什么，请考虑构建块。你是否想要指定目标、输出、输入、布局、功能，作为要考虑包含的一组东西。最后，使用你的聊天机器人进行迭代、改进和故障排除。你不需要第一次就正确。你可以告诉你已经知道的、你已经在脑海中想到的，看看你得到什么，然后用它来进一步完善你告诉 AI 要做的事情。\n我在现实生活中实际上喜欢乒乓球。现在你也可以构建一个与计算机一起玩的游戏。请进入下一个学习项目自己尝试一下。此外，除了构建生日贺卡生成器或乒乓球游戏，如果你有其他你想尝试构建的想法，试一试。它可能有效也可能无效，但通过实践和探索我们的想法，我们所有人都变得擅长构建东西。\n此文章由 AI 翻译\n","description":"","tags":["translated","AI","gaming","course"],"categories":[],"content":"现在你可以构建更多应用 使用你现在拥有的提示词技能，让我们构建另一个应用程序。这一次，是一个乒乓球游戏。\n由于 AI 的存在，现在可能从想法到工作应用程序，有时只需几分钟。计算机历史上最早的电子游戏之一是一个叫 Pong 的游戏，这是一种两人乒乓球游戏，团队花了数周时间才构建完成。但现在，多亏了 AI，你可以在几分钟内构建类似的东西。\n让我们应用我们学到的提示词技术。我将快速开始并编写一个中等具体的提示词，说：给我构建一个乒乓球游戏作为单个 HTML 文件，使用玩家对战计算机模式，用方向键移动球拍。如果你这样做，你可能会得到一个看起来像这样的应用程序的第一个版本。我想，好的，这是一个不错的开始，但我想要它添加三个难度级别，让用户指定获胜所需的点数，并保持比分。这为我构建了第二个版本。\n这现在看起来像一个有趣的游戏，但我希望图形更花哨。所以我将说：让游戏区域变成绿色，球拍变成米色，球变成白色，并告诉它将这张图片插入背景。所以我最终得到了一个看起来像这样的游戏。\n这是游戏。实际上很有趣。球来回反弹。我实际上可以玩很长时间，可能比你真正想看我玩的时间更长。但我希望你会享受这个。提醒一下，在编写提示词时具体会得到更好的结果。所以你可以看看我使用的提示词，它在许多细节上相当具体，比如颜色、点数等等。\n如果你不确定提示词中要包含什么，请考虑构建块。你是否想要指定目标、输出、输入、布局、功能，作为要考虑包含的一组东西。最后，使用你的聊天机器人进行迭代、改进和故障排除。你不需要第一次就正确。你可以告诉你已经知道的、你已经在脑海中想到的，看看你得到什么，然后用它来进一步完善你告诉 AI 要做的事情。\n我在现实生活中实际上喜欢乒乓球。现在你也可以构建一个与计算机一起玩的游戏。请进入下一个学习项目自己尝试一下。此外，除了构建生日贺卡生成器或乒乓球游戏，如果你有其他你想尝试构建的想法，试一试。它可能有效也可能无效，但通过实践和探索我们的想法，我们所有人都变得擅长构建东西。\n此文章由 AI 翻译\n"},{"title":"自定义你的应用","url":"/courses/build-with-andrew-ng-course/04.-customize-your-app/","section":"courses","date":"2025-01-08","summary":"自定义你的应用 现在你已经构建了一个基本的生日贺卡应用，让我们看看如何为它添加额外的功能，以便它可以做更多的事情或更有趣。如果你把你的应用展示给其他人，他们可能也有关于添加什么的想法。\n我还想向你展示，如果 AI 为你构建的任何东西有问题，比如它没有按你的预期工作，该怎么办。\n像以前一样，我们将告诉 AI 添加什么功能的主要方式是通过提示词（Prompting）。\n为了理解在提示词时具体说明为什么很重要，让我用一个点餐的类比。如果你去一辆食品卡车说，给我一个三明治，如果那是你说的全部，谁知道他们会给你什么类型的三明治。\n但如果你说更具体的话，比如我想要一个素食三明治，那么这缩小了你可能得到的范围。或者如果你说，我想要一个带有鹰嘴豆泥、奶酪和多谷物面包的素食三明治，那么那个具体的指令真的意味着你得到的东西变得更加可预测。\n或者如果你说，我想要这样的素食三明治，并添加一杯饮料，请打包带走，那么食品卡车会给你回什么变得更加可预测。\n在食品卡车，通过对你要求的东西具体说明，你更有可能得到你想要的东西。提示词 AI 也是如此。通过编写更具体的提示词，你更有可能准确得到你想要的东西。哦，这正是我想要的。\n当你获得让 AI 为你编写代码的技能时，你会变得更擅长给 AI 更具体的指令，这将给你更好的结果。\n我之前使用这样的提示词构建了一个应用程序，要么一次性完成，要么来回几次。我现在想做的是添加一些新功能并修改一些现有功能。\n例如，与其像我们目前拥有的三个输入字段，姓名、年龄和爱好，你可能想要收集五条信息，所以用户可以输入五个输入字段，以便你可以创建更个性化的消息。\n或者你可能想要一个「手气不错」按钮，它会自动为你填写所有字段，这样你不必每次使用应用程序时都输入所有内容。\n或者你也可以更新外观和感觉，比如更改标题或添加一个将消息复制到剪贴板的按钮，这样你可以轻松地通过电子邮件发送给朋友，甚至重新设计整个配色方案以更好地匹配你的风格。\n在构建了一个基本应用程序后，你可以通过决定你想要添加什么功能来使应用程序成为你自己的。\n所以你可以继续聊天对话，告诉 AI 让它有五个清晰标记的输入：姓名、年龄、爱好、一个形容词和一个复数名词，它会将应用程序更新为这样。\n注意我在这里编写非常具体的指令。我不只是说让它有五个字段，我告诉它我想要它有哪五个字段。\n或者如果我添加一个「手气不错」按钮，同样，我尝试具体地告诉它我想要「手气不错」按钮做什么。我想要它自动用随机词填充空白，我想要预定义的词选择很有趣。在这里，我实际上同时添加了两个功能。\n我可能更新标题和副标题，并添加一个复制到剪贴板的按钮。这实际上是我同时添加的上一张幻灯片中的两个项目。实际上我喜欢蓝色，所以让颜色主题变成蓝色。请自己尝试这些提示词，或者更好的是，选择不同的颜色主题，或者告诉它实现你有的任何其他想法。\n如果你喜欢粉色，像我女儿那样，或者绿色，像我儿子那样，告诉它改用粉色或绿色主题，或者根据你想做的事情做一些其他更改。如果你改变主意，你也可以这样做。\n所以在我把颜色主题变成蓝色后，如果我决定我不想要蓝色，实际上我想要紫色，你可以写这样的提示词，AI 将顺从地把它改成紫色。\n现在，很多时候，AI 相当好地给你准确要求它做的事情。但有时有机会生成一个不起作用的 HTML 页面。在软件中，当有一些东西不能按预期工作时，我们称之为错误（Bug）。\n这里实际上是 AI 之前为我生成的应用程序的损坏版本，我在这些字段中输入了所有内容，但如果我点击「生成卡片」，什么也不会发生。所以这很奇怪。我在这里点击鼠标，但它实际上并没有生成卡片。\n原来「手气不错」有效，但同样，「生成卡片」没有为我生成任何卡片。如果发生这种情况，我鼓励你做的是清楚地告诉 AI 发生了什么。所以我在这里输入，当我点击「生成卡片」按钮时什么也不会发生。你能为我修复吗？\n如果你这样做，AI 通常相当擅长发现至少基本的错误并修复任何错误。当你这样做时，有时 AI 会写一些技术性的解释，说明出了什么问题。\n所以在这里它说，JavaScript 附加到按钮。点击事件，哇，那有很多技术术语。我现在会说，别担心 AI 在这里说什么。你不需要理解这些技术细节。如果你真的很好奇，你实际上可以问 AI，这些术语是什么意思？\n但你不必这样做。让 AI 进行思考，说出它想说的关于技术的话，然后专注于下载新的 HTML 文件，看看它是否有效。\n我希望你从基本的生日贺卡应用开始，要么尝试构建我在这个视频中早些时候建议的功能，或者也许你的朋友建议的功能或你自己的想法。事实上，如果你不确定还能做什么，你也可以向 AI 寻求想法。\n所以如果你问它，我怎样才能让这个生日贺卡应用变得很酷？它可能会给出几个建议，然后你可以选择一个或多个，并使用 AI 的想法让你的应用程序变得更酷。我在构建软件时实际上经常将 AI 作为头脑风暴工具，如果你还没有让你兴奋的想法，你也应该这样做。\n请进入这个网站的下一个项目，并享受添加功能或玩弄颜色主题或实现任何你喜欢的东西。当你回来时，我们将使用你已经学到的技能来构建第二个应用程序，一个乒乓球游戏。\n此文章由 AI 翻译\n","description":"","tags":["translated","AI","prompting","course"],"categories":[],"content":"自定义你的应用 现在你已经构建了一个基本的生日贺卡应用，让我们看看如何为它添加额外的功能，以便它可以做更多的事情或更有趣。如果你把你的应用展示给其他人，他们可能也有关于添加什么的想法。\n我还想向你展示，如果 AI 为你构建的任何东西有问题，比如它没有按你的预期工作，该怎么办。\n像以前一样，我们将告诉 AI 添加什么功能的主要方式是通过提示词（Prompting）。\n为了理解在提示词时具体说明为什么很重要，让我用一个点餐的类比。如果你去一辆食品卡车说，给我一个三明治，如果那是你说的全部，谁知道他们会给你什么类型的三明治。\n但如果你说更具体的话，比如我想要一个素食三明治，那么这缩小了你可能得到的范围。或者如果你说，我想要一个带有鹰嘴豆泥、奶酪和多谷物面包的素食三明治，那么那个具体的指令真的意味着你得到的东西变得更加可预测。\n或者如果你说，我想要这样的素食三明治，并添加一杯饮料，请打包带走，那么食品卡车会给你回什么变得更加可预测。\n在食品卡车，通过对你要求的东西具体说明，你更有可能得到你想要的东西。提示词 AI 也是如此。通过编写更具体的提示词，你更有可能准确得到你想要的东西。哦，这正是我想要的。\n当你获得让 AI 为你编写代码的技能时，你会变得更擅长给 AI 更具体的指令，这将给你更好的结果。\n我之前使用这样的提示词构建了一个应用程序，要么一次性完成，要么来回几次。我现在想做的是添加一些新功能并修改一些现有功能。\n例如，与其像我们目前拥有的三个输入字段，姓名、年龄和爱好，你可能想要收集五条信息，所以用户可以输入五个输入字段，以便你可以创建更个性化的消息。\n或者你可能想要一个「手气不错」按钮，它会自动为你填写所有字段，这样你不必每次使用应用程序时都输入所有内容。\n或者你也可以更新外观和感觉，比如更改标题或添加一个将消息复制到剪贴板的按钮，这样你可以轻松地通过电子邮件发送给朋友，甚至重新设计整个配色方案以更好地匹配你的风格。\n在构建了一个基本应用程序后，你可以通过决定你想要添加什么功能来使应用程序成为你自己的。\n所以你可以继续聊天对话，告诉 AI 让它有五个清晰标记的输入：姓名、年龄、爱好、一个形容词和一个复数名词，它会将应用程序更新为这样。\n注意我在这里编写非常具体的指令。我不只是说让它有五个字段，我告诉它我想要它有哪五个字段。\n或者如果我添加一个「手气不错」按钮，同样，我尝试具体地告诉它我想要「手气不错」按钮做什么。我想要它自动用随机词填充空白，我想要预定义的词选择很有趣。在这里，我实际上同时添加了两个功能。\n我可能更新标题和副标题，并添加一个复制到剪贴板的按钮。这实际上是我同时添加的上一张幻灯片中的两个项目。实际上我喜欢蓝色，所以让颜色主题变成蓝色。请自己尝试这些提示词，或者更好的是，选择不同的颜色主题，或者告诉它实现你有的任何其他想法。\n如果你喜欢粉色，像我女儿那样，或者绿色，像我儿子那样，告诉它改用粉色或绿色主题，或者根据你想做的事情做一些其他更改。如果你改变主意，你也可以这样做。\n所以在我把颜色主题变成蓝色后，如果我决定我不想要蓝色，实际上我想要紫色，你可以写这样的提示词，AI 将顺从地把它改成紫色。\n现在，很多时候，AI 相当好地给你准确要求它做的事情。但有时有机会生成一个不起作用的 HTML 页面。在软件中，当有一些东西不能按预期工作时，我们称之为错误（Bug）。\n这里实际上是 AI 之前为我生成的应用程序的损坏版本，我在这些字段中输入了所有内容，但如果我点击「生成卡片」，什么也不会发生。所以这很奇怪。我在这里点击鼠标，但它实际上并没有生成卡片。\n原来「手气不错」有效，但同样，「生成卡片」没有为我生成任何卡片。如果发生这种情况，我鼓励你做的是清楚地告诉 AI 发生了什么。所以我在这里输入，当我点击「生成卡片」按钮时什么也不会发生。你能为我修复吗？\n如果你这样做，AI 通常相当擅长发现至少基本的错误并修复任何错误。当你这样做时，有时 AI 会写一些技术性的解释，说明出了什么问题。\n所以在这里它说，JavaScript 附加到按钮。点击事件，哇，那有很多技术术语。我现在会说，别担心 AI 在这里说什么。你不需要理解这些技术细节。如果你真的很好奇，你实际上可以问 AI，这些术语是什么意思？\n但你不必这样做。让 AI 进行思考，说出它想说的关于技术的话，然后专注于下载新的 HTML 文件，看看它是否有效。\n我希望你从基本的生日贺卡应用开始，要么尝试构建我在这个视频中早些时候建议的功能，或者也许你的朋友建议的功能或你自己的想法。事实上，如果你不确定还能做什么，你也可以向 AI 寻求想法。\n所以如果你问它，我怎样才能让这个生日贺卡应用变得很酷？它可能会给出几个建议，然后你可以选择一个或多个，并使用 AI 的想法让你的应用程序变得更 …"},{"title":"致谢","url":"/courses/build-with-andrew-ng-course/09.-acknowledgments/","section":"courses","date":"2025-01-08","summary":"致谢 这门课程由一支由教育工作者、工程师、设计师和创作者组成的令人难以置信的团队带给生活。我们感谢他们的奉献和专业知识。\n教学及课程设计 Andrew Ng • Summer Rae • Tommy Nelson\n课程开发 Andres Gonzalez-Vargas • Renzo Cuadra\n实验室开发及工程 Andres Zarta • Yiwei Chen • Sherry Lin • Joe Chen • Damon Shen\nAlpha 测试及导师指导 Giovanni Lignarolo • Lesly Zerna\n市场营销 Rowan Bradley • Natalia Farfan • Ariana Faustini • Ray Banks\n视频制作 Nicholas Camp • Paul T. Layland • Jake Miller • Freddie Rupprecht\n行政及内容支持 Ariel Lin • Dapinder Dosanjh • Namita Gupta\n","description":"","tags":["translated","course","credits"],"categories":[],"content":"致谢 这门课程由一支由教育工作者、工程师、设计师和创作者组成的令人难以置信的团队带给生活。我们感谢他们的奉献和专业知识。\n教学及课程设计 Andrew Ng • Summer Rae • Tommy Nelson\n课程开发 Andres Gonzalez-Vargas • Renzo Cuadra\n实验室开发及工程 Andres Zarta • Yiwei Chen • Sherry Lin • Joe Chen • Damon Shen\nAlpha 测试及导师指导 Giovanni Lignarolo • Lesly Zerna\n市场营销 Rowan Bradley • Natalia Farfan • Ariana Faustini • Ray Banks\n视频制作 Nicholas Camp • Paul T. Layland • Jake Miller • Freddie Rupprecht\n行政及内容支持 Ariel Lin • Dapinder Dosanjh • Namita Gupta\n最重要的是\u0026hellip; 我们要衷心感谢 YOU（你）报名参加这门课程并加入我们的全球学习者社区。你是我们从事这项工作的原因！\n此文章由 AI 翻译\n"},{"title":"评论：《黑客与画家》","url":"/post/comment_hacker_painter/","section":"post","date":"2024-04-22","summary":"本文将围绕Paul Graham的《黑客与画家》展开评论，探讨黑客与艺术家的共通之处，以及黑客在工程科学等领域中所面临的现实困境与生存之道。\n","description":"","tags":["黑客","计算机科学","工程科学","生活方式"],"categories":["评论"],"content":"本文将围绕Paul Graham的《黑客与画家》展开评论，探讨黑客与艺术家的共通之处，以及黑客在工程科学等领域中所面临的现实困境与生存之道。\n《黑客与画家》是Paul Graham（保罗·格雷厄姆）的一篇博客文章，该文章被收录至其同名的著作中，称得上是计算机领域里影响巨大的一篇文章。\n保罗·格雷厄姆是知名的硅谷投资人，他创立了投资机构Y-Combinator，该机构孵化出了包括airbnb在内的无数家成功的初创企业。此外，在经营Y-Combinator的过程中，保罗发现并提拔了日后名声大噪的Sam Altman。与其成功的投资人经历相比，保罗的早年经历也挺传奇。他在哈佛拿到了计算机科学的博士学位，但后来又去罗德岛设计学院学习绘画。他在90年代，创建了叫做ViaWeb的互联网应用，并卖给了雅虎，赚得了自己的第一桶金。由此可以看出，《黑客与画家》这样一个主题正是作者个人经历的一个总结提炼。\n何为黑客？ 当提起“黑客”，不少人会想到制造计算机病毒的或者破解某个网站密码的程序员。这多少是在互联网发展早期一些从事非法工作的程序员给“黑客”带来的坏名声。在更早的时候，黑客就是泛指那些技术精湛的程序员自身，而没有什么道德评判或工种划分。当然，在本篇评论之中，“黑客\u0026quot;的内涵会被进一步泛化，他们将不再局限于进行计算机的编程工作，在任何领域里掌握着精湛技艺的人都可以成为黑客——这一点与作者所描述的黑客或许更为契合。\n在文中，作者多角度地类比了黑客与画家着两类职业，并指出黑客的本质在于他们依托一些技术进行创作。\n这种创作不同于科研工作，因为从事科研工作需要对他的研究方向的诸多细节有着深入的理解，他们需要对既有工作进行大量的学习，包括去了解相关的理论体系的、完成某项研究细节的实现，他们需要进行很多公式推导、科学实验，并逐渐形成对研究方向的直觉，这一切可以帮助他们在一个已经有了很多成果的方向上更进一步，或者说“站在巨人的肩膀上往远处再多看一眼”。\n黑客需要掌握很多工具，但他们并不需要将某一艰深的理论咬穿嚼透，他们只需要能够利用某项技术即可，就像画家不需要把颜料的化学理论搞清楚，他们只需要掌握调制不同色彩和质地的颜料即可。某项技术之于黑客不同于“巨人的肩膀”之于科学家，而是更像画笔之于画家。科学家需要非常扎实的肩膀，并在很长的时间里就住在了这个“肩膀”之上，因此“肩膀”不宜太多（如果踩着两个肩膀的话，大概就是在从事交叉学科的研究吧，但如果两个肩膀相距太远或同时踩着若干肩膀，就有点容易扯到，这恐怕会比较危险，）；与其形成鲜明对比的是，画家却可以拥有很多画笔，黑客也可以掌握非常多的技术，他们只需要把这些东西组合起来构成自己的工具库，足以服务于创作即可。\n科学家需要在已经很扎实的基础之上通过艰苦的工作来获得做出原创性成果的能力，但黑客的工作一开始就是原创的，然后他们就像画家一样拿着画笔去勾勒自己的想法，这个想法最初可能是脆弱的，但通过一步步的完善，他们最终将做出较为扎实的成果。也就是说，科学家工作的出发点是可靠的东西，目标是原创的；而黑客工作的出发点是原创的，目标是可靠的东西。\n既然与科研工作有如此差异，这种创作是不是更像某些企业的研发工作呢？答案恐怕也是否定的。研发工作的目的在于生产出在市场上卖得出去的产品，这些产品的特性是由市场上的潜在买家决定的，而产品的设计与开发之前，通常需要一定的市场调研。因此，产品的研发并不是那种纯粹的技术性工作。在一些非常成熟的“夕阳产业”中，企业的研发更是不需要什么“创作”过程，与其对想法原型进行不断地完善，企业的老板更喜欢看到员工按部就班地把产品做出来。\n因此，黑客的创作既不同于科研工作，又不同于产品研发。黑客更像是艺术家，比如画家、建筑师、音乐家等等。不过现实之中，黑客这个群体并不会自发地集结于某个场合，而是不得不委身于现有的各行各业中。\n从事工程科学的黑客 还是继续从计算机领域说起，在文章中，作者指出了计算机科学的“大杂烩”特性，堪称”科学界的南斯拉夫“。这一点使得计算机与诸如物理学等经典的科学领域相差很大，甚至显得计算机科学不太像是一门科学。不过要把计算机科学从科学范畴中开除，我是反对的，除非可以把我所从事的种种“工程科学”一并从科学领域中开除。工程领域的学科体系是围绕着某一项工程或工具而打造的，服务于具体的生产活动。比如机械、土木等行业的科研工作。这些学科会汇集一些诸如工业设计、材料科学、力学、计算机模拟乃至管理学等相关的研究方向，各方向之间的差异甚至可能不亚于物理学与生命科学这种一级学科之间的差异。鉴于这些工程学科的历史之悠久以及跨度之广泛，如果计算机科学算是”科学界的南斯拉夫“，它们可以被称为”科学界的日不落帝国“了。\n计算机科学领域存在的问题，在其它的工程科学领域同样存在。在工程科学领域的研究工作中，有 …"},{"title":"压缩感知-讲义","url":"/courses/compressive-sensing-lecture-notes/","section":"courses","date":"2007-07-01","summary":"压缩感知-讲义 摘要 本文是关于压缩感知理论的讲座笔记，介绍了一种突破传统奈奎斯特采样定理限制的信号采集方法。通过利用信号的稀疏性或可压缩性，采用线性测量矩阵与优化重构算法，以远低于奈奎斯特率的测量次数完成信号恢复。文中详细阐述了测量矩阵的稳定性条件，包括约束等距性和不相干性等核心概念。\n核心概念及解读 压缩感知（Compressive Sensing）——一种利用信号稀疏性，通过少量非自适应线性测量直接获取压缩表示并重构原始信号的新型采集框架：\n稀疏表示（Sparse Representation）——信号在某组基下仅由少数非零系数表示的特性，是压缩感知理论成立的核心前提：\n约束等距性（Restricted Isometry Property, RIP）——要求测量矩阵对所有稀疏向量近似保持其范数不变的条件，是保证稳定重构的充分条件：\n不相干性（Incoherence）——测量矩阵与稀疏化基之间互不可稀疏表示的性质，是确保测量有效捕获稀疏信号信息的另一关键条件：\n变换编码（Transform Coding）——传统的先采样后压缩方法，通过变换到稀疏基域保留大系数丢弃小系数实现压缩，是压缩感知所要改进的对象：\n好的，这是您要求的《Compressive Sensing》文章的全文翻译，使用Markdown格式，参考文献部分保留原文：\n压缩感知 (Compressive Sensing) Richard Baraniuk，莱斯大学 (Rice University)\n[《IEEE Signal Processing Magazine》讲座笔记] 第24卷，2007年7月\n1 范围 (Scope) 香农/奈奎斯特采样定理告诉我们，为了在对信号进行均匀采样时不丢失信息，我们的采样频率必须至少是其带宽的两倍。在许多应用中，包括数字图像和视频相机，奈奎斯特率可能非常高，导致我们最终得到过多的样本，必须进行压缩才能存储或传输。在其他应用中，包括成像系统（医学扫描仪、雷达）和高速模数转换器，将采样率或密度提高到超越当前技术水平是非常昂贵的。\n在本讲座中，我们将学习一种利用压缩感知 [1, 2] 来解决这些问题的新技术。我们将用一种更通用的线性测量方案结合优化方法来取代传统的采样和重构操作，以便以显著低于奈奎斯特率的速率获取某些类型的信号。\n2 相关性 (Relevance) 这里提出的思想可以用来阐明数据采集、线性代数、基展开、逆问题、压缩、降维和优化之间的联系，适用于从本科或研究生数字信号处理到统计学和应用数学的各种课程。\n3 先决条件 (Prerequisites) 理解和讲授这些材料所需的先决条件是线性代数、基础优化和基础概率论。\n4 问题陈述 (Problem Statement) 奈奎斯特率采样通过利用信号的带限性来完全描述信号。我们的目标是通过利用信号的可压缩性来减少完全描述信号所需的测量次数。特殊之处在于，我们的测量不是点采样，而是信号的更一般的线性泛函。\n考虑一个实值、有限长度、一维的离散时间信号 $\\mathbf { x }$，我们将其视为 $\\mathbb { R } ^ { N }$ 中的一个 $N \\times 1$ 列向量，其元素为 $x [ n ]$，$n = 1 , 2 , \\ldots , N$。对于图像或更高维的数据，我们通过将其向量化为一个长的一维向量来处理。\n","description":"本文是关于压缩感知理论的讲座笔记，介绍了一种突破传统奈奎斯特采样定理限制的信号采集方法。通过利用信号的稀疏性或可压缩性，采用线性测量矩阵与优化重构算法，以远低于奈奎斯特率的测量次数完成信号恢复。文中详细阐述了测量矩阵的稳定性条件，包括约束等距性和不相干性等核心概念。","tags":["压缩感知","稀疏表示","信号处理","约束等距性","变换编码"],"categories":[],"content":"压缩感知-讲义 摘要 本文是关于压缩感知理论的讲座笔记，介绍了一种突破传统奈奎斯特采样定理限制的信号采集方法。通过利用信号的稀疏性或可压缩性，采用线性测量矩阵与优化重构算法，以远低于奈奎斯特率的测量次数完成信号恢复。文中详细阐述了测量矩阵的稳定性条件，包括约束等距性和不相干性等核心概念。 核心概念及解读 压缩感知（Compressive Sensing）——一种利用信号稀疏性，通过少量非自适应线性测量直接获取压缩表示并重构原始信号的新型采集框架： 稀疏表示（Sparse Representation）——信号在某组基下仅由少数非零系数表示的特性，是压缩感知理论成立的核心前提： 约束等距性（Restricted Isometry Property, RIP）——要求测量矩阵对所有稀疏向量近似保持其范数不变的条件，是保证稳定重构的充分条件： 不相干性（Incoherence）——测量矩阵与稀疏化基之间互不可稀疏表示的性质，是确保测量有效捕获稀疏信号信息的另一关键条件： 变换编码（Transform Coding）——传统的先采样后压缩方法，通过变换到稀疏基域保留大系数丢弃小系数实现压缩，是压缩感知所要改进的对象： 好的，这是您要求的《Compressive Sensing》文章的全文翻译，使用Markdown格式，参考文献部分保留原文： 压缩感知 (Compressive Sensing) Richard Baraniuk，莱斯大学 (Rice University) [《IEEE Signal Processing Magazine》讲座笔记] 第24卷，2007年7月 1 范围 (Scope) 香农/奈奎斯特采样定理告诉我们，为了在对信号进行均匀采样时不丢失信息，我们的采样频率必须至少是其带宽的两倍。在许多应用中，包括数字图像和视频相机，奈奎斯特率可能非常高，导致我们最终得到过多的样本，必须进行压缩才能存储或传输。在其他应用中，包括成像系统（医学扫描仪、雷达）和高速模数转换器，将采样率或密度提高到超越当前技术水平是非常昂贵的。 在本讲座中，我们将学习一种利用压缩感知 [1, 2] 来解决这些问题的新技术。我们将用一种更通用的线性测量方案结合优化方法来取代传统的采样和重构操作，以便以显著低于奈奎斯特率的速率获取某些类型的信号。 2 相关性 (Relevance) 这里提出的思想可以用来阐明数据采集、线性代数、基展开、逆问题、压缩、降维和优化之间的联系，适用于从本科或研究生数字信号处理到统计学和应用数学的各种课程。 3 先决条件 (Prerequisites) 理解和讲授这些材料所需的先决条件是线性代数、基础优化和基础概率论。 4 问题陈述 (Problem Statement) 奈奎斯特率采样通过利用信号的带限性来完全描述信号。我们的目标是通过利用信号的可压缩性来减少完全描述信号所需的测量次数。特殊之处在于，我们的测量不是点采样，而是信号的更一般的线性泛函。 考虑一个实值、有限长度、一维的离散时间信号 $\\mathbf { x }$，我们将其视为 $\\mathbb { R } ^ { N }$ 中的一个 $N \\times 1$ 列向量，其元素为 $x [ n ]$，$n = 1 , 2 , \\ldots , N$。对于图像或更高维的数据，我们通过将其向量化为一个长的一维向量来处理。 $\\mathbb { R } ^ { N }$ 中的任何信号都可以用一组 $N \\times 1$ 的向量基 $\\{ \\psi _ { i } \\} _ { i = 1 } ^ { N }$ 来表示。为简单起见，假设该基是标准正交基。将这些向量 $\\{ \\psi _ { i } \\}$ 作为列堆叠起来，形成 $N \\times N$ 的基矩阵 $\\Psi : = [ \\pmb { \\psi } _ { 1 } | \\pmb { \\psi } _ { 2 } | \\cdot . . . | \\pmb { \\psi } _ { N } ]$，我们可以将任何信号 $\\mathbf { x }$ 表示为 $$ \\mathbf { x } = \\sum _ { i = 1 } ^ { N } s _ { i } \\psi _ { i } \\qquad { \\mathrm { 或 } } \\qquad \\mathbf { x } = \\Psi \\mathbf { s } \\quad (1) $$其中 s 是 $N \\times 1$ 的权重系数 $s _ { i } = \\langle \\mathbf { x } , \\psi _ { i } \\rangle = \\psi _ { i } ^ { T } \\mathbf { x }$ 的列向量，$\\cdot ^ …"}]