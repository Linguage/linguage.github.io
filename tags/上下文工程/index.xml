<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>上下文工程 on Linguista</title>
    <link>https://linguage.github.io/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B/</link>
    <description>Recent content in 上下文工程 on Linguista</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://linguage.github.io/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>「翻译」Anthropic:为人工智能代理进行有效的上下文工程</title>
      <link>https://linguage.github.io/labs/rosetta/anthropic_context_engineering/</link>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://linguage.github.io/labs/rosetta/anthropic_context_engineering/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anthropic.com/engineering&#34;&gt;Anthropic 工程团队&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;在提示工程（prompt engineering）成为应用人工智能领域关注焦点几年后，一个新术语开始崭露头角：&lt;strong&gt;上下文工程（context engineering）&lt;/strong&gt;。使用语言模型进行构建，正逐渐从为提示寻找合适的词语和短语，转变为回答一个更宏观的问题：“什么样的上下文配置最有可能产生我们模型的期望行为？”&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;上下文&lt;/strong&gt;指的是在从大语言模型（LLM）采样时包含的令牌（token）集合。眼下的&lt;strong&gt;工程&lt;/strong&gt;问题是，在 LLM 的固有约束下，优化这些令牌的效用，以持续实现期望的结果。要有效地驾驭 LLM，通常需要&lt;em&gt;在上下文中思考&lt;/em&gt;——换句话说：考虑 LLM 在任何给定时间可用的整体状态，以及该状态可能产生的潜在行为。&lt;/p&gt;&#xA;&lt;p&gt;在这篇文章中，我们将探讨上下文工程这门新兴的艺术，并为构建可控、有效的代理提供一个更精炼的心智模型。&lt;/p&gt;&#xA;&lt;h2 id=&#34;上下文工程与提示工程&#34;&gt;上下文工程与提示工程&lt;/h2&gt;&#xA;&lt;p&gt;在 Anthropic，我们将上下文工程视为提示工程的自然演进。提示工程指的是为获得最佳结果而编写和组织 LLM 指令的方法（请参阅&lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview&#34;&gt;我们的文档&lt;/a&gt;以获取概述和有用的提示工程策略）。&lt;strong&gt;上下文工程&lt;/strong&gt;则指的是在 LLM 推理过程中，为管理和维护最佳令牌（信息）集合而采取的一系列策略，包括可能出现在提示之外的所有其他信息。&lt;/p&gt;&#xA;&lt;p&gt;在早期使用 LLM 进行工程设计的阶段，提示是人工智能工程工作中最重要的部分，因为日常聊天互动之外的大多数用例都需要针对单次分类或文本生成任务进行优化的提示。顾名思义，提示工程的主要焦点是如何编写有效的提示，特别是系统提示。然而，随着我们朝着构建能够在多轮推理和更长时间范围内运行的、能力更强的代理迈进，我们需要策略来管理整个上下文状态（系统指令、工具、&lt;a href=&#34;https://modelcontextprotocol.io/docs/getting-started/intro&#34;&gt;模型上下文协议&lt;/a&gt;（MCP）、外部数据、消息历史等）。&lt;/p&gt;&#xA;&lt;p&gt;一个循环运行的代理会产生越来越多可能与下一轮推理相关的数据，这些信息必须周期性地进行提炼。上下文工程正是在这个不断演变的信息宇宙中，精心挑选哪些内容将进入有限的上下文窗口的&lt;a href=&#34;https://x.com/karpathy/status/1937902205765607626?lang=en&#34;&gt;艺术与科学&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffaa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png&amp;amp;w=3840&amp;amp;q=75&#34; alt=&#34;提示工程与上下文工程&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;与编写提示这项离散的任务相比，上下文工程是迭代的，并且每次我们决定向模型传递什么内容时，都会发生管理阶段。&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么上下文工程对构建强大的代理至关重要&#34;&gt;为什么上下文工程对构建强大的代理至关重要&lt;/h2&gt;&#xA;&lt;p&gt;我们观察到，尽管 LLM 速度快，能处理越来越大的数据量，但它们和人类一样，在某个点上会失去焦点或感到困惑。关于“大海捞针”式基准测试的研究揭示了&lt;a href=&#34;https://research.trychroma.com/context-rot&#34;&gt;上下文退化（context rot）&lt;/a&gt;的概念：随着上下文窗口中令牌数量的增加，模型准确回忆该上下文中信息的能力会下降。&lt;/p&gt;&#xA;&lt;p&gt;虽然某些模型的性能下降比其他模型更为平缓，但这一特性在所有模型中都会出现。因此，上下文必须被视为一种边际回报递减的有限资源。就像人类的&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0963721409359277&#34;&gt;工作记忆容量有限&lt;/a&gt;一样，LLM 在解析大量上下文时也有一个“注意力预算”。每引入一个新的令牌都会消耗这个预算的一部分，从而增加了仔细管理 LLM 可用令牌的必要性。&lt;/p&gt;&#xA;&lt;p&gt;这种注意力稀缺源于 LLM 的架构限制。LLM 基于 &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer 架构&lt;/a&gt;，该架构使每个令牌都能&lt;a href=&#34;https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need&#34;&gt;关注到&lt;/a&gt;整个上下文中的其他所有令牌。这导致 n 个令牌之间存在 n² 个成对关系。&lt;/p&gt;&#xA;&lt;p&gt;随着上下文长度的增加，模型捕捉这些成对关系的能力被拉伸，从而在上下文大小和注意力焦点之间产生了一种天然的张力。此外，模型的注意力模式是在训练数据分布中形成的，其中短序列通常比长序列更常见。这意味着模型对于上下文范围内的依赖关系经验较少，专门化的参数也较少。&lt;/p&gt;&#xA;&lt;p&gt;像&lt;a href=&#34;https://arxiv.org/pdf/2306.15595&#34;&gt;位置编码插值&lt;/a&gt;这样的技术，通过将长序列适应于最初训练的较小上下文，使得模型能够处理更长的序列，尽管在理解令牌位置方面会有一些性能下降。这些因素造成了性能的梯度下降而非悬崖式下跌：模型在较长上下文中仍然非常强大，但与在较短上下文中的表现相比，其信息检索和长程推理的精度可能会降低。&lt;/p&gt;&#xA;&lt;p&gt;这些现实情况意味着，深思熟虑的上下文工程对于构建强大的代理至关重要。&lt;/p&gt;&#xA;&lt;h2 id=&#34;有效上下文的剖析&#34;&gt;有效上下文的剖析&lt;/h2&gt;&#xA;&lt;p&gt;鉴于 LLM 受到有限的注意力预算的限制，&lt;em&gt;良好&lt;/em&gt;的上下文工程意味着找到&lt;em&gt;尽可能小&lt;/em&gt;的高信噪比令牌集合，以最大化实现某种期望结果的可能性。实现这一实践远比说起来容易，但在下一节中，我们将概述这一指导原则在上下文的不同组成部分中的实际意义。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;系统提示&lt;/strong&gt;应该极其清晰，使用简单、直接的语言，以&lt;em&gt;恰当的高度&lt;/em&gt;向代理呈现思想。恰当的高度是介于两种常见失败模式之间的“金发姑娘”区域。一个极端是，我们看到工程师在提示中硬编码复杂、脆弱的逻辑，以引出精确的代理行为。这种方法会产生脆弱性，并随着时间的推移增加维护复杂性。另一个极端是，工程师有时会提供模糊、高层次的指导，这无法为 LLM 提供期望输出的具体信号，或者错误地假设了共享的上下文。最佳高度则在两者之间取得了平衡：既足够具体以有效引导行为，又足够灵活，能为模型提供强大的启发式方法来指导行为。&lt;/p&gt;&#xA;&lt;p&gt;我们建议将提示组织成不同的部分（如 &lt;code&gt;&amp;lt;background_information&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;instructions&amp;gt;&lt;/code&gt;、&lt;code&gt;## Tool guidance&lt;/code&gt;、&lt;code&gt;## Output description&lt;/code&gt; 等），并使用 XML 标签或 Markdown 标题等技术来划分这些部分，尽管随着模型变得越来越强大，提示的确切格式可能正变得不那么重要。&lt;/p&gt;&#xA;&lt;p&gt;无论您决定如何构建系统提示，都应该力求使用最少的信息集来完整地勾勒出您期望的行为。（请注意，最少并不一定意味着简短；您仍然需要预先为代理提供足够的信息，以确保它遵循期望的行为。）最好的方法是，首先用可用的最佳模型测试一个最简化的提示，看看它在您的任务上的表现，然后根据初始测试中发现的失败模式，添加清晰的指令和示例来提高性能。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;工具&lt;/strong&gt;允许代理与其环境进行交互，并在工作时引入新的、额外的上下文。因为工具定义了代理与其信息/行动空间之间的契约，所以工具促进效率至关重要，既要返回令牌高效的信息，也要鼓励高效的代理行为。&lt;/p&gt;&#xA;&lt;p&gt;在&lt;a href=&#34;https://www.anthropic.com/engineering/writing-tools-for-agents&#34;&gt;为 AI 代理编写工具——借助 AI 代理&lt;/a&gt;一文中，我们讨论了构建能被 LLM 很好理解且功能重叠最小的工具。与设计良好的代码库中的函数类似，工具应该是自包含的、对错误具有鲁棒性，并且其预期用途应极其明确。输入参数同样应该是描述性的、无歧义的，并能发挥模型的内在优势。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
