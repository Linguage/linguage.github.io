<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>无监督学习 on Linguista</title><link>https://linguista.cn/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 无监督学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>Ilya Sutskever演讲 - 从压缩视角理解无监督学习的理论框架</title><link>https://linguista.cn/rosetta/technology/ilya-sutskever-unsupervised-learning-compression-theory/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/ilya-sutskever-unsupervised-learning-compression-theory/</guid><description>&lt;h1 id="ilya-sutskever演讲---从压缩视角理解无监督学习的理论框架"&gt;Ilya Sutskever演讲 - 从压缩视角理解无监督学习的理论框架&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;Ilya Sutskever在本次演讲中探讨了无监督学习的理论基础，提出压缩是理解无监督学习的关键。他从监督学习的数学保证出发，引入Kolmogorov复杂性作为终极压缩器的概念，论证了通过联合压缩未标记数据与目标数据可以揭示共享结构，从而实现有效的无监督学习。演讲还结合iGPT实验验证了该理论，并讨论了线性表征涌现等开放问题。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Kolmogorov复杂性 - 衡量数据最短程序描述长度的理论概念，代表理想的终极压缩器，虽不可计算但为无监督学习提供了理论上界&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;压缩即预测 - 压缩与预测在数学上等价，好的压缩器必然能捕捉数据规律，大型语言模型通过下一token预测本质上在执行压缩任务&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PAC学习 - 概率近似正确学习理论，为监督学习提供了严格的数学保证，即低训练误差加有限自由度可确保泛化成功&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iGPT - OpenAI于2020年提出的图像生成预训练模型，通过下一像素预测在视觉领域验证了压缩理论对无监督学习的解释力&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;联合压缩 - 通过同时压缩未标记数据X和标记数据Y来发现两者共享的隐藏结构，是将压缩理论应用于无监督学习的核心机制&lt;/strong&gt;：&lt;/p&gt;
&lt;h3 id="讲座介绍"&gt;讲座介绍&lt;/h3&gt;
&lt;p&gt;在这场深度思考的演讲中，Ilya Sutskever探讨了无监督学习这一人工智能领域核心挑战的理论基础。与监督学习相对成熟的数学框架不同，无监督学习的内在机制和成功保证长期以来显得较为模糊。Sutskever提出，理解无监督学习的关键可能在于“压缩”这一概念。&lt;/p&gt;
&lt;p&gt;他首先回顾了监督学习的成功要素，并指出了无监督学习在缺乏明确指导信号时面临的根本困惑。随后，他介绍了一种早期的、有理论保证的无监督学习方法——分布匹配，并承认其在实际应用中的局限性。演讲的核心论点是，一个理想的压缩器，特别是理论上的Kolmogorov压缩器，能够通过联合压缩未标记数据和有标记（或目标）数据，揭示它们之间的共享结构，从而实现有效的无监督学习。Sutskever进一步将这一高度抽象的理论与现实中的大型神经网络联系起来，认为随机梯度下降（SGD）在某种程度上是在进行程序搜索，而大型模型则是在逼近这种理想的压缩能力。&lt;/p&gt;
&lt;p&gt;为了验证这一思想，他展示了OpenAI在2020年关于iGPT的工作，证明了在视觉领域通过“下一像素预测”（一种压缩形式）可以获得强大的无监督学习表征。尽管该理论为无监督学习提供了一个富有洞察力的视角，Sutskever也坦诚地讨论了其局限性，例如它并未完全解释线性表征的涌现，并对未来研究方向提出了展望。本次演讲为我们提供了一个重新审视和理解无监督学习本质的独特框架。&lt;/p&gt;
&lt;h3 id="内容纲要"&gt;内容纲要&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 引言与背景
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 演讲者与主题介绍 (Ilya Sutskever, 无监督学习的旧成果)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 当前研究重点 (AI对齐)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 学习理论基础回顾
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 学习的本质与可行性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 监督学习 (Supervised Learning)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义与数学保证 (PAC学习, 统计学习理论)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 成功条件 (低训练损失, 自由度 &amp;lt; 训练集大小, 同分布假设)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 对VC维度的评论 (处理无限精度参数, 实际有限精度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 无监督学习的挑战与困惑
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义与目标 (发现隐藏结构)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 缺乏类似监督学习的保证
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 核心困惑 (优化一个目标，关心另一个；均匀分布下的失效)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 一种有保证的无监督学习方法：分布匹配 (Distribution Matching)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 核心思想 (函数f使得F(X)分布 ≈ Y分布)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 应用示例 (机器翻译, 密码破译)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 局限性 (设置理想化)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 无监督学习的核心思想：压缩 (Compression)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 压缩与预测的等价性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 核心思想实验 (联合压缩X和Y, $C(X \text{concat} Y)$ vs $C(X) + C(Y)$)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │ └── 共享结构/算法互信息
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 通过压缩实现无监督学习的形式化：遗憾 (Regret)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 低遗憾意味着充分利用未标记数据
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── Kolmogorov复杂性：终极压缩器
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义 ($K(X)$ - 输出X的最短程序长度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 作为最佳压缩器的特性 (模拟其他压缩器, 不可计算性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 与神经网络的联系 (SGD作为程序搜索, NN作为微型Kolmogorov压缩器近似)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 条件Kolmogorov复杂性：无监督学习的理论解
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义 ($K(Y|X)$ - 给定X输出Y的最短程序长度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 作为无监督学习的解决方案 (终极低遗憾, 不可计算)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 联合压缩与条件压缩的等价性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 应用条件Kolmogorov复杂性的挑战 (对大数据集条件化困难)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 重要结果 ($K(X,Y)$ 在预测Y上 ≈ $K(Y|X)$)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 最大似然估计与联合压缩 (自然契合机器学习)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 理论的实证验证：iGPT
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── GPT模型与压缩理论的关系 (可解释, 但非唯一解释)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 在视觉领域寻求直接验证 (iGPT, 2020)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 方法 (图像转像素序列, Transformer进行下一像素预测)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 结果 (CIFAR-10, ImageNet上的表现, 显示可扩展性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 关于线性表征的思考
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 压缩理论的局限性 (不直接解释线性可分性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 线性表征的普遍性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 观察与推测 (自回归模型 vs BERT, 长程结构的重要性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└── 问答环节要点
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 关于推测的详细说明 (下一像素预测 vs BERT掩码预测)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 鲁棒的二维下一像素预测 (扩散模型等概率模型)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── Kolmogorov复杂性与神经网络训练动态的差异
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 密码学中分布区分、预测与压缩的联系
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── VC维度的辩护 (Scott Aaronson的例子)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── Transformer/SGD作为最佳压缩程序的理解 (数据集总损失)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 压缩框架、iGPT与线性表征的关系 (线性表征是额外好处)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 无监督理论对监督学习的启示 (函数类别, 忽略计算成本)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 自回归建模的重要性 (与其他最大似然方法比较)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── GPT-4作为压缩器与压缩器自身大小的问题 (固定数据集 vs 无限测试集)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 使用gzip和k-NN进行文本分类的讨论 (gzip压缩能力有限)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; └── 课程学习效应 (与架构优化难度相关)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id="ilya-sutskever关于大型语言模型llm与未来的演讲"&gt;Ilya Sutskever关于大型语言模型（LLM）与未来的演讲&lt;/h1&gt;
&lt;h2 id="一-引言与背景"&gt;一、 引言与背景&lt;/h2&gt;
&lt;p&gt;YEJIN CHOI: 它的引用次数达到了六位数，超过了139,000次，甚至更多。所以非常期待听到Ilya关于LLM和未来的分享。交给你了。&lt;/p&gt;</description></item><item><title>神经网络理论研究中的物理学思想与应用</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/physics-neural-network-theory-ising-model/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/physics-neural-network-theory-ising-model/</guid><description>&lt;h1 id="神经网络理论研究中的物理学思想与应用"&gt;神经网络理论研究中的物理学思想与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文从物理学视角系统阐述神经网络的理论基础，将神经网络视为数据驱动的自适应物理模型。文章从伊辛模型出发，介绍神经网络的基本属性DNA（数据、网络、算法），深入剖析感知机学习的几何景观与解空间结构，探讨无监督学习中的对称性破缺机制，以及非平衡神经动力学的伪势表示方法。最后通过线性回归模型揭示大语言模型示例泛化的物理本质，为理解智能提供物理学洞察。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章开篇指出神经网络在人工智能领域的核心地位，强调物理学思想对神经网络研究的深远影响。作者认为，神经网络本质上是数据驱动的自适应物理模型，其学习过程是在高维权重空间中执行朗之万动力学，服从玻尔兹曼分布。&lt;/p&gt;
&lt;p&gt;从伊辛模型这一统计物理标准模型入手，文章建立了自旋系统与神经网络的深刻联系。伊辛模型描述格点上磁矩的集体行为，蕴含相变、自发对称性破缺、普适性等丰富物理图像。神经网络将相互作用强度J作为可训练参数，外场对应偏置，通过定义目标函数和梯度下降算法驱动网络更新，其学习过程可视为在势能函数下的随机游走。&lt;/p&gt;
&lt;p&gt;感知机模型的研究揭示了解空间的几何景观。当学习样本数与连接数之比达到临界值α≈0.833时，自由熵消失，学习问题无解。解空间呈现大量孤岛形态，局域算法难以找到解，但存在稀有的稠密解团簇能够吸引高效算法。这一发现近期已被数学家严格证明。&lt;/p&gt;
&lt;p&gt;无监督学习建模为受限玻尔兹曼机，学习过程呈现对称性破缺现象。随着数据量增长，学生网络逐步推断教师网络的结构，经历自发对称破缺和置换对称破缺两个阶段，最终实现对隐藏节点内在顺序的区分。&lt;/p&gt;
&lt;p&gt;认知动力学层面的非平衡过程可通过伪势表示法研究。定义正则系综分析非平衡神经动力学稳态，发现当相互作用强度达到临界值时，系统经历从有序到混沌的连续相变，响应函数在相变点附近出现峰值，印证了混沌边缘的优越性。&lt;/p&gt;
&lt;p&gt;大语言模型的示例泛化能力可归结为两体自旋模型的基态求解。通过线性回归函数类和随机任务向量生成预训练数据，模型参数服从具有两体相互作用的哈密顿量。基态分析显示，即使在有限尺寸网络中仍可获得最优解，任务向量的多样性对预训练效果至关重要。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;伊辛模型与神经网络&lt;/strong&gt;：伊辛模型是统计物理的标准模型，描述格点上磁矩的集体行为。其态方程为迭代方程，包含自旋间相互作用J、磁化强度m和外加磁场h。当相互作用较弱时系统处于顺磁态，增大相互作用至临界值时出现铁磁解，此过程称为自发对称性破缺。神经网络可类比为伊辛模型，连接权重对应相互作用，偏置对应外场，学习过程是势能函数下的随机游走，平衡态服从玻尔兹曼分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知机解空间几何&lt;/strong&gt;：感知机学习问题存在存储容量极限α=P/N≈0.833，超过此临界值问题无解。在可解区域内，解空间呈现大量孤岛形态，导致梯度下降等局域算法难以找到解。解空间中存在稀有的稠密解团簇，这些团簇能够吸引高效经验算法，使其避开孤岛成功收敛。这一几何结构解释了为何某些算法在实际中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对称性破缺与无监督学习&lt;/strong&gt;：无监督学习让机器从原始数据中发现隐藏规律，类似人类婴儿的观察学习过程。通过受限玻尔兹曼机建模，学生网络从教师网络生成的数据中推断连接矩阵。学习过程呈现对称性破缺：初始状态具有完全对称性，随着数据增长，学生首先推断权重相同的部分（自发对称破缺），进而区分权重不同的部分（置换对称破缺），最终实现对隐藏节点内在顺序的认知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;非平衡动力学的伪势表示&lt;/strong&gt;：认知动力学大多不存在梯度力，但可通过定义伪势函数研究非平衡稳态。对于相互作用矩阵为非厄米随机矩阵的神经元系统，当耦合强度g增至1时触发连续动力学相变，系统从有序走向混沌。序参量为网络神经活动水平的涨落，响应函数在相变点附近出现峰值，验证了混沌边缘在认知功能上的优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大语言模型示例泛化的物理本质&lt;/strong&gt;：大语言模型的示例泛化能力可通过线性回归模型理解。固定随机任务向量，生成多个随机输入计算标签得到预训练数据。模型参数服从具有两体相互作用的哈密顿量，其基态是示例泛化能力的根源。高斯分布假设下的基态求解显示，有限尺寸网络仍可获得最优解，任务向量多样性对预训练效果起关键作用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/KbS3dAuyjb6pQV5g9RGzYw"&gt;神经网络理论研究的物理学思想&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;现代物理知识杂志&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>关于《对泛化的一个观察》的笔记</title><link>https://linguista.cn/rosetta/technology/notes-on-an-observation-on-generalization/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/notes-on-an-observation-on-generalization/</guid><description>&lt;h1 id="关于对泛化的一个观察的笔记"&gt;关于《对泛化的一个观察》的笔记&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文是对 Ilya Sutskever 在2023年西蒙斯研究所研讨会演讲的详细笔记。演讲从压缩理论的视角解释无监督学习为何有效，核心论点是预测与压缩之间存在一一对应关系。通过引入柯尔莫戈洛夫复杂度、条件复杂度和联合压缩等概念，论证了好的无监督学习算法本质上是好的压缩器，并最终将联合压缩与最大似然估计联系起来。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;压缩与预测的等价性&lt;/strong&gt;：所有压缩器和所有预测器之间存在一一对应关系，能更好预测下一个字符就能更好压缩数据，这为理解无监督学习提供了理论基础&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;柯尔莫戈洛夫复杂度&lt;/strong&gt;：一个对象的柯尔莫戈洛夫复杂度是能输出该对象的最短程序的长度，它代表了理论上的终极压缩器，虽然不可计算但为分析提供了理论上界&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;条件柯尔莫戈洛夫复杂度&lt;/strong&gt;：K(Y|X) 衡量在已知数据集 X 的情况下描述 Y 所需的最短程序长度，它形式化了无监督数据对下游任务的价值&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分布匹配&lt;/strong&gt;：在没有对应关系的两个数据源之间寻找映射函数，使一个数据源的变换分布近似另一个数据源的分布，是理解无监督学习的重要桥梁&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;联合压缩与最大似然估计&lt;/strong&gt;：将两个数据集拼接后联合压缩等价于最大似然估计，这将压缩理论的分析框架与机器学习的标准训练范式统一起来&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文链接：&lt;a href="https://sumanthrh.com/post/notes-on-generalization/"&gt;Notes on &amp;ldquo;An Observation on Generalization&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;深入探讨 Ilya Sutskever 在 2023 年西蒙斯研究所大型语言模型与 Transformer 研讨会上的演讲“对泛化的一个观察”。&lt;/p&gt;
&lt;p&gt;Ilya Sutskever，OpenAI 的联合创始人之一，最近在 2023 年西蒙斯研究所（Simons Institute）的大型语言模型与 Transformer 研讨会上发表了演讲。该演讲已被录制，并&lt;a href="https://www.youtube.com/live/AKMuA_TVz3A?si=vBMLcQcKzXPpFeom"&gt;可在 YouTube 上观看&lt;/a&gt;。演讲的主题是“对泛化的一个观察”（An Observation on Generalization），Ilya 探讨了我们如何利用压缩理论的视角来理解无监督学习。我觉得这次演讲非常有见地，但有时可能难以跟上思路，更难把握整体脉络，至少对于像我这样（大脑皮层功能没那么强大）的人来说是这样。不幸的是，这是研讨会中少数几个没有附带详细论文的演讲之一。我觉得这是一个很好的机会，可以写下我从演讲中做的笔记。鉴于 Ilya 阐述其理论的方式非常精彩，很难真正用不同的方式来解释和呈现。因此，我首先&lt;em&gt;转录&lt;/em&gt;了演讲内容，然后添加了我自己的笔记，以提供更好的背景信息或更详细的说明以求清晰。&lt;/p&gt;
&lt;h1 id="对泛化的一个观察"&gt;对泛化的一个观察&lt;/h1&gt;
&lt;p&gt;这次演讲的核心是试图真正理解为什么无监督学习能够奏效，并对其进行数学上的推理。为了达到这个目的，Ilya 首先提出了学习本身（从数据中学习）的概念以及为什么机器学习会起作用。这里的期望是数据具有规律性，而机器学习模型被&lt;em&gt;期望&lt;/em&gt;学习我们数据中的这种规律性。谈到监督学习，他提出了这个等式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;低训练误差 + 训练数据量 &amp;gt; “自由度” = 低测试误差&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;现在，对于无监督学习，我们的梦想是，给定所有这些未标记的数据（图像、文本块等），我们期望机器学习模型能够发现数据中“真实”的、隐藏的结构。无监督学习的一个关键点是，我们通常优化一个代理目标（例如下一个词预测或重构），而我们关心的是一个不同的目标（例如学习数据中的隐藏模式以进行序列分类）。这为什么会奏效呢？&lt;/p&gt;
&lt;h2 id="通过分布匹配进行无监督学习"&gt;通过分布匹配进行无监督学习&lt;/h2&gt;
&lt;p&gt;为了理解这一点，我们首先来看分布匹配。考虑两个数据源 X 和 Y，它们之间没有任何对应关系。这可能就像是不同语言的数据集（比如英语和法语），样本之间没有对应。分布匹配的思想是找到一个映射 F，使得：&lt;/p&gt;
&lt;p&gt;distribution(F(X)) ∼ Y&lt;/p&gt;
&lt;p&gt;在我们上面的例子中，就是：distribution(F(English)) ∼ French&lt;/p&gt;
&lt;p&gt;之前已经提出了许多方法（一个相关的例子：&lt;a href="https://arxiv.org/abs/1711.00043"&gt;无监督机器翻译&lt;/a&gt;），表明即使对于高维度的 X 和 Y，这种方法也是可行的。&lt;/p&gt;</description></item></channel></rss>