<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>深度学习 on Linguista</title><link>https://linguista.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 深度学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 04 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>动物与幽灵</title><link>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</link><pubDate>Sat, 04 Oct 2025 00:00:00 +0000</pubDate><guid>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</guid><description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;动物可能是一个很好的灵感来源。内在动机、乐趣、好奇心、赋能、多智能体自我博弈、文化。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Andrej Karpathy在本文中探讨了人工智能领域中“动物”智能与大语言模型（“幽灵”）的差异，反思了学习机制、进化和预训练的本质，并提出动物或许能为AI带来新的灵感和范式。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts1.tc.mm.bing.net/th/id/R-C.5b6ee267317d44b17842e771033bc7f4?rik=U6xjpiLj2AM6Mw&amp;amp;riu=http%3a%2f%2fn.sinaimg.cn%2fsinakd20117%2f762%2fw1000h562%2f20230516%2f61f2-929bc75cae6683aec311ff02b0528b5e.jpg&amp;amp;ehk=YX8liBBPYwNpcJkyffP6ne%2b5uHTTJzeDIJnnV%2f%2fKEPM%3d&amp;amp;risl=&amp;amp;pid=ImgRaw&amp;amp;r=0" alt=""&gt;&lt;/p&gt;</description></item><item><title>AI研究人员如何意外发现他们对学习的固有认知是错误的</title><link>https://linguista.cn/static/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</link><pubDate>Mon, 18 Aug 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</guid><description>本文深入探讨了机器学习领域关于“偏差-方差权衡”的铁律如何被2019年的“双重下降”现象打破。文章阐述了研究人员如何通过扩大模型规模，意外发现大型神经网络不仅能避免过拟合，反而能涌现出惊人的新能力。这一发现挑战了三百年的统计理论，引发了以 OpenAI 为首的“越大越好”行业变革，并最终由“彩票假说”从数学上解释了为何海量冗余参数是通向简单高效解决方案的必经之路。</description></item><item><title>扩散语言模型解析</title><link>https://linguista.cn/static/zed-diffustion-model/</link><pubDate>Tue, 08 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/zed-diffustion-model/</guid><description>本文深入探讨了扩散语言模型这一文本生成的新范式。文章通过交互式图表和对比分析，阐述了非自回归模型如何通过迭代去噪过程生成文本，并重点分析了模型在生成质量与推理速度之间的关键权衡，以及其在文本生成领域的独特优势与潜在挑战。</description></item><item><title>黄仁勋对话Transformer八子</title><link>https://linguista.cn/static/jensen-huang-transformer8/</link><pubDate>Tue, 01 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/jensen-huang-transformer8/</guid><description>本文记录了英伟达CEO黄仁勋在GTC 2024大会上与Transformer论文八位作者的史诗级对话。文章深入探讨了该架构如何通过自注意力机制解决并行计算瓶颈，八位作者离开谷歌后的创业历程，以及他们对未来AI发展的多维展望，涵盖推理、数据效率及新架构形态。</description></item><item><title>苦涩的教训</title><link>https://linguista.cn/essays/bitter-lesson/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>https://linguista.cn/essays/bitter-lesson/</guid><description>&lt;p&gt;本文是里奇·萨顿（Richard Sutton）2019年著名文章《&lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;The Bitter Lesson（苦涩的教训）&lt;/a&gt;》。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts3.tc.mm.bing.net/th/id/OIP-C.5-o1C9ggTpwDMynLnfilkgHaE8?cb=12&amp;amp;rs=1&amp;amp;pid=ImgDetMain&amp;amp;o=7&amp;amp;rm=3" alt=""&gt;&lt;/p&gt;</description></item><item><title>阅读收藏</title><link>https://linguista.cn/bookmarks/reading_list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://linguista.cn/bookmarks/reading_list/</guid><description>&lt;p&gt;本页为我在阅读过程中所收藏的网页目录。&lt;/p&gt;</description></item></channel></rss>