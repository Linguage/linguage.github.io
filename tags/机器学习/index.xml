<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on Linguista</title><link>https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 04 Oct 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>动物与幽灵</title><link>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</link><pubDate>Sat, 04 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</guid><description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;动物可能是一个很好的灵感来源。内在动机、乐趣、好奇心、赋能、多智能体自我博弈、文化。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Andrej Karpathy在本文中探讨了人工智能领域中“动物”智能与大语言模型（“幽灵”）的差异，反思了学习机制、进化和预训练的本质，并提出动物或许能为AI带来新的灵感和范式。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts1.tc.mm.bing.net/th/id/R-C.5b6ee267317d44b17842e771033bc7f4?rik=U6xjpiLj2AM6Mw&amp;amp;riu=http%3a%2f%2fn.sinaimg.cn%2fsinakd20117%2f762%2fw1000h562%2f20230516%2f61f2-929bc75cae6683aec311ff02b0528b5e.jpg&amp;amp;ehk=YX8liBBPYwNpcJkyffP6ne%2b5uHTTJzeDIJnnV%2f%2fKEPM%3d&amp;amp;risl=&amp;amp;pid=ImgRaw&amp;amp;r=0" alt=""&gt;&lt;/p&gt;</description></item><item><title>大型语言模型与世界模型第一部分LLMs如何理解它们的世界</title><link>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-part-1/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-part-1/</guid><description>&lt;h1 id="大型语言模型与世界模型第一部分llms-如何理解它们的世界"&gt;大型语言模型与世界模型（第一部分）：LLMs 如何理解它们的&amp;quot;世界&amp;quot;&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文由 Melanie Mitchell 撰写，深入探讨了大型语言模型（LLMs）是否发展出了类似人类的&amp;quot;世界模型&amp;quot;以理解其运作的&amp;quot;世界&amp;quot;。文章回顾了早期机器学习系统的脆弱性问题，介绍了世界模型的概念定义与分类方法，并围绕 LLMs 是否真正具备世界模型能力展开了学术界的重要辩论。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先通过多个经典案例揭示了早期 AI 系统的根本局限性——它们依赖于训练数据中的启发式规则和表面特征，而非真正的概念理解。从皮肤病变分类错误地依赖尺子存在，到语言模型仅凭词汇重叠判断逻辑关系，再到强化学习系统在游戏设置微小变化下的性能崩溃，这些案例都指向同一个问题：缺乏对世界因果结构的理解。&lt;/p&gt;
&lt;p&gt;接着文章转向当前备受争议的话题——大型语言模型是否突破了这一局限。OpenAI 联合创始人 Ilya Sutskever 认为，通过预测下一个词的训练目标，LLMs 确实学习了世界的压缩表征，包括人类的情感和动机。然而，包括 Yann LeCun 在内的多位研究者对此表示强烈怀疑，认为仅靠语言训练无法达到真正的理解。2022 年的一项调查显示，NLP 研究者群体在这一问题上几乎呈现对半分的分裂态势。&lt;/p&gt;
&lt;p&gt;为了厘清这一辩论，文章详细梳理了&amp;quot;世界模型&amp;quot;的多种定义。从最基础的内部表征到保留因果结构的复杂模型，再到能够支持反事实推理的完整模拟器。MIT 教授 Jacob Andreas 提出了一个清晰的分类框架，从静态查找表、地图、机械天体仪到完整的模拟器，每种类型代表了对世界理解的不同深度。人类正是通过这样的世界模型，才能快速理解复杂场景、预测因果关系并规划行动。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;：指智能体内部形成的、对外部世界的压缩且可模拟的表征，它不仅能够存储信息，还能捕捉世界的因果结构，支持预测、规划和回答反事实问题。人类的世界模型使我们能够在瞬间理解街景照片中的复杂场景，推断行为者的意图和可能的后续发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启发式规则与表面特征&lt;/strong&gt;：早期机器学习系统依赖的捷径思维，它们通过发现训练数据中的统计关联来解决问题，但这种方式缺乏真正的理解。就像皮肤病变分类器记住&amp;quot;尺子=恶性&amp;quot;这样的关联，当环境变化时就会失效，因为系统并不理解尺子和病变之间的真实关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;情境模型&lt;/strong&gt;：LLMs 可能具备的一种中间层次世界模型，能够跟踪文本中的行为者、状态和动作变化。这类似于一个机械天体仪，可以模拟特定场景中的动态过程，但可能缺乏对更广泛世界因果知识的整合。目前尚不清楚 LLMs 的情境模型能否推广到训练数据之外的全新场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因果模拟模型&lt;/strong&gt;：世界模型的最高层次，能够回答复杂的&amp;quot;如果-那么&amp;quot;类型反事实问题，需要对世界的深层因果结构有精确理解。目前缺乏证据表明 LLMs 具备这种能力，这是判断它们是否真正&amp;quot;理解&amp;quot;世界的关键检验标准。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://aiguide.substack.com/p/llms-and-world-models-part-1"&gt;LLMs and World Models, Part 1 - How do Large Language Models Make Sense of Their &amp;ldquo;Worlds&amp;rdquo;?&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Melanie Mitchell&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未明确说明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>强化学习基础与贝尔曼方程详解</title><link>https://linguista.cn/curated/henrinotes_2025_p3/reinforcement-learning-bellman-equation-fundamentals/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/reinforcement-learning-bellman-equation-fundamentals/</guid><description>&lt;h1 id="强化学习基础与贝尔曼方程详解"&gt;强化学习基础与贝尔曼方程详解&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于西湖大学赵世钰老师《强化学习的数学原理》课程，系统性地介绍了强化学习的核心概念体系。从策略、奖励、回报等基本要素出发，逐步构建马尔可夫决策过程的理论框架，重点阐述了贝尔曼方程的数学原理及其在状态值和动作值计算中的应用，为深入理解强化学习提供了清晰的数学基础。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先建立了强化学习的基本概念体系，通过具体示例说明了策略的概率分布特性、奖励的数值含义以及回报的累积计算方式。作者强调，奖励是依赖于当前状态和动作的标量值，可正可负，而回报则是沿轨迹收集的所有奖励之和，用于评估策略优劣。文章还引入了Episode的概念，将完整轨迹定义为一次回合。&lt;/p&gt;
&lt;p&gt;在理论基础部分，文章详细解析了马尔可夫决策过程的三大核心要素：马尔可夫属性的无记忆性特征、决策过程的策略定义、以及涉及状态转移概率和奖励概率的完整过程描述。这部分内容为后续的贝尔曼方程推导奠定了数学基础。&lt;/p&gt;
&lt;p&gt;文章的核心内容围绕贝尔曼方程展开，介绍了理查德·贝尔曼在1950年代提出的最优性原理。作者通过状态价值函数和动作价值函数两种形式，详细阐述了如何通过未来可能价值来计算当前价值。文章特别强调了回报与状态值的本质区别：回报针对单个轨迹，而状态值是对多个轨迹求回报后的平均值。&lt;/p&gt;
&lt;p&gt;在应用层面，文章讲解了动作值的定义和计算方法，阐明了状态值与动作值之间的数学关系，即状态值等于不同动作对应的动作值的加权平均。最后通过Bellman最优性方程，引入了最优策略的存在性和唯一性证明，完整构建了从基础概念到高级理论的强化学习知识体系。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;策略（Policy）&lt;/strong&gt;：策略定义了智能体在给定状态下应采取各种动作的概率分布。文章通过表格示例展示了在S1状态下执行a2、a3动作各为0.5的概率配置，并提供了相应的编程实现代码。策略的核心作用是将状态映射到动作空间，是强化学习中决策机制的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bootstrap概念&lt;/strong&gt;：这是贝尔曼方程的核心思想，描述了所有状态之间值的相互依赖关系。通过bootstrap，当前状态的价值可以通过未来状态的价值来估计，这种递归性质使得强化学习能够通过迭代计算逐步优化策略。文章在策略评价和最优值计算中都强调了这一概念的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;状态值与动作值的关系&lt;/strong&gt;：状态值V(s)表示从某个状态出发的期望回报，而动作值Q(s,a)则表示从某个状态执行特定动作的期望回报。两者的关键关系在于：状态值等于该状态下所有可能动作的动作值按策略概率加权平均。这一关系为策略改进和最优策略求解提供了数学基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最优性原理&lt;/strong&gt;：贝尔曼提出的这一原理指出，如果一个策略在每个子问题上都是最优的，那么它对整个问题就是全局最优的。这一原理不仅具有深刻的数学美感，更为动态规划和强化学习算法提供了理论支撑，是理解最优策略存在性和唯一性的关键。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/I1TjXMwSInlW7CFe1SXlGw"&gt;强化学习：概念和贝尔曼方程&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;赵世钰（西湖大学）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;《强化学习的数学原理》课程整理&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>快速入门卷积神经网络CNN</title><link>https://linguista.cn/curated/henrinotes_2025_p3/cnn-convolutional-neural-network-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/cnn-convolutional-neural-network-guide/</guid><description>&lt;h1 id="快速入门卷积神经网络cnn"&gt;快速入门卷积神经网络（CNN）&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;卷积神经网络（Convolutional Neural Network，CNN）是专门设计用于处理具有空间结构数据（如图像）的深度学习架构。本文通过通俗易懂的语言和生动的比喻，系统介绍了CNN的核心组成部分——卷积层、激活函数、池化层和全连接层，并提供PyTorch和Keras两种框架的完整代码实现示例，帮助读者从零开始理解并构建CNN模型。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了CNN的基本定义和核心优势——能够自动从图像等空间数据中提取特征。作者通过一个生动的例子：当我们观察一张苹果照片时，眼睛会先识别边缘、颜色、纹理等局部特征，大脑再整合这些信息最终识别出&amp;quot;苹果&amp;quot;。CNN正是模拟这一过程，通过层层特征提取实现图像识别。&lt;/p&gt;
&lt;p&gt;接下来，文章详细拆解了CNN的四大核心组件。卷积层如同用放大镜观察图像局部，通过滑动过滤器提取特征；激活函数引入非线性变换，使网络能够学习复杂的模式；池化层对特征图进行降维，减少计算量同时保留关键信息；全连接层则负责整合所有特征并输出最终分类结果。&lt;/p&gt;
&lt;p&gt;最后，文章以经典的CIFAR-10图像分类数据集为例，提供了完整的CNN模型实现代码，分别使用PyTorch和Keras两种主流深度学习框架，让读者能够直接上手实践。这种理论与实践相结合的方式，使抽象的神经网络概念变得具体可操作。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;卷积层&lt;/strong&gt;：CNN的核心特征提取组件，通过可学习的过滤器（Filter）在输入数据上滑动，执行卷积运算生成特征图。每个过滤器专注于提取特定类型的特征（如边缘、纹理、颜色模式），浅层卷积层提取简单特征，深层卷积层组合形成更复杂的抽象特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：为神经网络引入非线性的关键组件，最常用的是ReLU（Rectified Linear Unit），它将所有负值置零而保持正值不变。这种简单而有效的操作使网络能够学习和表示复杂的非线性关系，避免了线性模型的表达能力限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;池化层&lt;/strong&gt;：用于降低特征图空间维度的下采样操作，最大池化（Max Pooling）取每个局部区域的最大值作为输出。池化不仅减少了计算量和参数数量，还提供了一定程度的平移不变性，使模型对物体位置的微小变化更加鲁棒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全连接层&lt;/strong&gt;：位于CNN末尾的分类器组件，将前面卷积层提取的局部特征整合成全局特征向量，通过矩阵运算输出每个类别的得分。全连接层负责将特征映射到最终的类别空间，完成从特征到决策的转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征层次化&lt;/strong&gt;：CNN的核心设计理念是通过多层网络实现特征的层次化提取。低层网络学习简单的边缘和颜色特征，中层网络组合这些基础特征形成纹理和形状，高层网络则识别出完整的物体部件和整体概念，这种层层递进的特征抽象使CNN具备了强大的视觉理解能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/2ynT7bYrrWwNOP-NH8EbwQ"&gt;快速入门一个算法，CNN&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;chal1ce&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年01月05日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>重新思考 MoE 技术及其在大模型中的应用</title><link>https://linguista.cn/curated/henrinotes_2025_p3/moe-technology-rethinking-large-language-models/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/moe-technology-rethinking-large-language-models/</guid><description>&lt;h1 id="重新思考-moe-技术及其在大模型中的应用"&gt;重新思考 MoE 技术及其在大模型中的应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统性地梳理了混合专家技术在大语言模型中的演进历程。作者从内部世界模型对齐的视角出发，深入分析了GShard、DeepSeek MoE、DeepSeek-V3等架构的技术特点，探讨了RPC-Attention和MoDE等创新方法，并最终从认知框架层面重新解读了MoE的本质——一种基于先验知识的跨范畴采样策略。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章开篇提出了一个关键观点：MoE大模型如果不能在内部世界模型上实现对齐和互换，其输出将呈现人格分裂的状态。作者将内部世界模型定义为&amp;quot;现实的共享统计模型&amp;quot;或&amp;quot;以概率为表征的丰富范畴&amp;quot;，这为后续的技术分析奠定了理论基础。&lt;/p&gt;
&lt;p&gt;在架构演进方面，文章详细剖析了四个关键发展阶段。GShard作为早期的MoE实现，通过轻量级标注API和XLA编译器扩展，实现了万亿级参数的支持。DeepSeek MoE针对GShard存在的知识混杂和冗余问题，创新性地提出了细粒度专家分段和共享专家隔离策略。DeepSeek-V3则进一步整合了MLA注意力机制和无辅助损失的负载均衡策略，在6710亿总参数中每个token仅激活370亿参数，实现了训练与推理的高效平衡。&lt;/p&gt;
&lt;p&gt;技术创新部分，文章介绍了RPC-Attention通过核PCA框架推导自注意力，提供了对数据污染具有鲁棒性的解决方案。MoDE则将MoE思想扩展到扩散模型，通过噪声调节路由和专家缓存机制，将推理成本降低了90%。&lt;/p&gt;
&lt;p&gt;最后，作者从认知科学角度重新诠释了MoE的本质，将其视为一种分布式采样策略，这一策略可以基于人类先验知识，在高维语言概率空间中实现跨范畴的智能推理。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;内部世界模型对齐&lt;/strong&gt;：这是作者对MoE技术提出的核心要求。内部世界模型指模型对现实的共享统计表征，当不同的专家模块在这个表征上实现对齐时，模型才能产生连贯一致的输出，而非人格分裂的结果。这一概念将技术问题提升到了认知科学的高度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;细粒度专家分段&lt;/strong&gt;：DeepSeek MoE的核心创新之一。通过在保持参数量不变的情况下，将FFN中间隐藏维度拆分为更细粒度的专家，使模型能够激活更多专家并实现更灵活的组合。这种设计让多样化的知识能够更精确地分配到专门的专家中，提升专精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;共享专家隔离&lt;/strong&gt;：针对知识冗余问题的解决方案。将某些专家设计为始终激活的共享专家，专门捕获跨上下文的共享知识，从而减少其他路由专家之间的冗余，提高整体参数效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跨范畴采样&lt;/strong&gt;：作者对MoE本质的哲学解读。MoE中的&amp;quot;专家&amp;quot;本质上是&amp;quot;特定范畴&amp;quot;的形象化表达，模型推理的过程是在高维语言概率空间的子空间中进行采样，而类比推理则是跨范畴的采样过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;噪声条件路由&lt;/strong&gt;：MoDE架构的关键机制。将路由器设计为噪声条件化的，使得在推理前可以预先计算每个噪声水平使用的专家，从而去除路由器，仅保留选定专家，显著提升网络效率。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/p7C9zXZRY5iE0Q-d8GRkaQ"&gt;重新思考 MoE&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>N-Gram模型与自然语言处理入门</title><link>https://linguista.cn/curated/henrinotes_2025_p3/n-gram-model-natural-language-processing-introduction/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/n-gram-model-natural-language-processing-introduction/</guid><description>&lt;h1 id="n-gram模型与自然语言处理入门"&gt;N-Gram模型与自然语言处理入门&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统介绍了N-Gram模型的基本原理及其在自然语言处理中的重要作用。作为理解概率语言模型的基础和深入理解现代语言模型如GPT的重要前奏，文章详细阐述了N-Gram模型的定义、构建过程、优缺点以及实际应用场景。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先强调N-Gram模型在自然语言处理学习路径中的核心地位，指出理解N-Gram是迈向GPT等现代语言模型的第一步。GPT的核心思想正是对N-Gram思想的深度拓展和优化。&lt;/p&gt;
&lt;p&gt;文章详细介绍了N-Gram模型的基本概念：通过将文本分割成连续的N个词的组合来近似描述词序列的联合概率。根据N的不同取值，可以分为Unigram（一元组）、Bigram（二元组）和Trigram（三元组）等不同类型。每种类型都有其特定的上下文依赖假设，从仅考虑当前词到依赖前面N-1个词的序列。&lt;/p&gt;
&lt;p&gt;在实现方面，文章阐述了N-Gram模型的构建过程，包括文本分割、词频统计、条件概率计算和预测生成四个步骤。通过训练语料中的频率统计，可以近似得到条件概率，从而预测下一个词出现的可能性。文章还特别强调了&amp;quot;词&amp;quot;的定义在不同语言中的差异，以及子词分词算法在处理未登录词等问题上的优势。&lt;/p&gt;
&lt;p&gt;文章客观分析了N-Gram模型的优点和局限性。优点包括简单高效、广泛适用和灵活可调；缺点主要体现在数据稀疏性、上下文局限和维度增长等方面。尽管存在这些限制，N-Gram模型仍在拼写检查、机器翻译和文本生成等实际应用场景中发挥着重要作用。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;N-Gram定义&lt;/strong&gt;：N-Gram是指文本中连续的N个词的组合。它的核心思想是用有限的上下文信息（N-1个词）来近似预测下一个词的概率。这种简化使得概率计算成为可能，是构建统计语言模型的基础方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;条件概率估计&lt;/strong&gt;：通过训练语料中的频率统计来近似计算条件概率。具体公式为给定前N-1个词时，下一个词出现的条件概率等于该N-gram在语料中的共现次数除以前缀的出现次数。这种基于统计的方法简单直接，不需要复杂的参数学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据稀疏性&lt;/strong&gt;：N-Gram模型面临的主要挑战之一。当N值较大时，很多可能的N-gram组合在训练语料中从未出现，导致概率为零，这在实际应用中会造成问题。平滑技术是解决这一问题的常用方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上下文局限&lt;/strong&gt;：N-Gram模型只能捕获有限长度（N-1个词）的上下文信息，无法理解文本中的长距离依赖关系。这是相对于现代深度学习语言模型的一个主要限制，也是GPT等模型通过注意力机制试图突破的瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;子词分词&lt;/strong&gt;：将单词切分成更小有意义部分的算法。对于处理未登录词、拼写错误和词汇变化等问题非常有效。比如将&amp;quot;embedding&amp;quot;切分为[&amp;ldquo;em&amp;rdquo;, &amp;ldquo;bed&amp;rdquo;, &amp;ldquo;ding&amp;rdquo;]，能够在保证语义完整性的同时提高模型的泛化能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/itcpsLv6x-4Thk9B2-BtTQ"&gt;入门GPT（一）| N-Gram 带你了解自然语言处理（1）&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;原作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-09&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>2025年获取机器学习职位的六个秘密技巧</title><link>https://linguista.cn/curated/henrinotes_2025_p3/ml-job-tips-2025-six-secret-strategies/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/ml-job-tips-2025-six-secret-strategies/</guid><description>&lt;h1 id="2025年获取机器学习职位的六个秘密技巧"&gt;2025年获取机器学习职位的六个秘密技巧&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;在2025年，获取机器学习职位看似充满挑战，但关键在于如何有效地展示自己的技能和价值。本文作者结合自身成为顶尖AI初创公司研究员的经历，总结了六个实用技巧：通过个人项目展示实战能力、深入研究开源代码并贡献修复、积极参与开源社区、在社交媒体建立个人品牌、持续学习提升技能、以及利用专业网络平台建立联系。这些策略帮助求职者在竞争激烈的AI领域中脱颖而出。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从求职者的实际困境出发，指出了许多人在申请机器学习职位时面临的核心问题——不知道如何有效展示自己的能力。作者以亲身经历为例，说明仅仅拥有理论知识是不够的，更重要的是将技能转化为可证明的成果。&lt;/p&gt;
&lt;p&gt;文章主体部分详细阐述了六个具体可操作的技巧。第一个技巧强调个人项目的重要性，这不仅是技能的展示，更是解决实际问题能力的证明。第二个技巧通过真实案例说明深入研究开源代码的价值——作者的朋友在使用Hugging Face transformers库时发现并修复了一个bug，不仅提升了性能，也展示了对技术的深度理解。&lt;/p&gt;
&lt;p&gt;接下来的四个技巧构成了一个完整的职业发展闭环：参与社区和开源项目能够增加曝光率并建立有价值的行业联系；在社交媒体上建立个人品牌则能让潜在雇主看到你的专业见解和持续学习的热情；保持学习状态确保技能不过时；而有意识的网络建设则为机会创造条件。&lt;/p&gt;
&lt;p&gt;文章最后强调，在2025年的AI求职市场中，成功不再取决于单一的学历或证书，而是通过综合运用这些策略，构建起立体化的专业形象和技能证明体系。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;个人项目展示&lt;/strong&gt;：这不仅是代码练习，更是将抽象的机器学习知识转化为具体解决方案的过程。优秀的个人项目应该解决实际问题，而不仅仅是复现论文或教程。这种实战经验向雇主证明你能够将理论应用于真实场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;开源贡献&lt;/strong&gt;：深入研究开源代码库并发现bug、提出改进，展示了对技术的深度理解。这种贡献不仅证明技术能力，更体现主动性和对社区的参与度，是很多顶尖公司看重的品质。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;个人品牌建设&lt;/strong&gt;：在LinkedIn、Twitter等平台分享学习心得、项目经验和技术见解，能够建立起持续学习和专业思考的形象。这种可见度在被动求职时尤为重要，很多机会来源于雇主对你的主动关注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;社区参与&lt;/strong&gt;：积极参与开源项目、技术论坛和线下活动，能够接触到行业前沿动态，建立有价值的职业网络。在AI领域，很多机会来自于人脉推荐而非常规申请渠道。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;持续学习&lt;/strong&gt;：机器学习领域更新迅速，保持学习状态是基本要求。但更重要的是将学习转化为可见的输出——项目、文章、贡献，而不仅仅是完成课程或阅读论文。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://towardsai.net/p/artificial-intelligence/my-6-secret-tips-for-getting-an-ml-job-in-2025?ref=dailydev"&gt;My 6 Secret Tips for Getting an ML Job in 2025&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Boris Meinardus&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年1月7日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>下一代提示工程的七种技术</title><link>https://linguista.cn/curated/henrinotes_2025_p3/next-generation-prompt-engineering-techniques/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/next-generation-prompt-engineering-techniques/</guid><description>&lt;h1 id="下一代提示工程的七种技术"&gt;下一代提示工程的七种技术&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文深入探讨了七种下一代提示工程技术，这些技术旨在优化大型语言模型的性能和输出质量。文章通过详细的表格对比，系统介绍了每种技术的工作原理、应用场景、优势及挑战，并提供了具体示例，为AI从业者提供了实用的提示工程实践指南。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章开篇即点明提示工程在提升LLM性能中的关键作用，随后通过结构化的方式逐一介绍七种先进技术。每种技术都从定义、工作原理、优势、挑战和实际应用示例五个维度进行阐述，使读者能够全面理解并快速应用到实践中。&lt;/p&gt;
&lt;p&gt;这七种技术涵盖了从简单到复杂的多个层面：Meta Prompting让LLM自身生成和优化提示；Least-to-Most Prompting通过问题分解降低复杂度；Multi-Task Prompting实现单次执行多个任务；Role Prompting通过角色定位增强专业性；Task-Specific Prompting针对特定任务优化；PAL引入编程环境增强问题解决能力；CoVe则通过验证机制提升准确性。&lt;/p&gt;
&lt;p&gt;文章强调，这些技术并非孤立存在，而是可以根据具体需求组合使用。选择合适的技术需要考虑任务复杂性、所需输出质量以及模型的知识储备等因素。通过系统化的提示工程，用户可以显著提升LLM在实际应用中的表现。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Meta Prompting（元提示）&lt;/strong&gt;：这是一种递归式的提示方法，将提示本身视为输出内容。利用LLM生成、解释和优化提示，包括优化其自身的提示。这种方法特别适合需要持续迭代和调整的复杂任务，但效果受限于LLM的知识库范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Least-to-Most Prompting（最少到最多提示）&lt;/strong&gt;：核心思想是将复杂问题拆解为一系列有序的子问题，引导模型逐步解决。这种方法能够提高准确性，减少错误累积，特别适用于已知解决方案路径的复杂推理任务，如数学计算或逻辑推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-Task Prompting（多任务提示）&lt;/strong&gt;：在一个提示中集成多个相关任务，要求模型同时处理并输出结果。这提高了效率，保持了上下文的连贯性，但随着任务数量增加，输出准确性可能下降，需要合理控制任务复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Program-Aided Language Models (PAL)&lt;/strong&gt;：将外部编程环境引入提示工程，让模型生成程序代码来解决需要精确计算的问题。这种方法特别适合数学、逻辑推理等传统LLM表现不佳的领域，通过代码执行获得准确结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chain-of-Verification (CoVe) Prompting&lt;/strong&gt;：通过自我验证机制减少LLM的幻觉问题。模型先生成初步答案，然后生成验证问题并回答，最后整合验证结果优化输出。这种三步验证流程显著提高了事实性任务的准确性。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://machinelearningmastery.com/7-next-generation-prompt-engineering-techniques/?ref=dailydev"&gt;7 Next-Generation Prompt Engineering Techniques&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Cornellius Yudha Wijaya&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年1月7日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>从机器学习模型的视角看机器学习研究</title><link>https://linguista.cn/curated/henrinotes-2025-p1/kaiming-he-ml-research-from-model-perspective/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/kaiming-he-ml-research-from-model-perspective/</guid><description>&lt;h1 id="从机器学习模型的视角看机器学习研究"&gt;从机器学习模型的视角看机器学习研究&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;何恺明教授在 NeurIPS 2024 NewInML Workshop 上发表演讲，从机器学习模型自身的视角出发，通过四个生动类比来阐释机器学习研究的本质与方向。这四个类比分别是：研究如同随机梯度下降、研究的目标是寻找&amp;quot;惊喜&amp;quot;、未来是真正的测试集、以及研究需要具备可扩展性。演讲深刻揭示了研究过程中的不确定性、探索性与长远价值。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;何恺明教授的这次演讲以一种独特的视角切入——用机器学习自身的概念和框架来审视机器学习研究本身。这种&amp;quot;元认知&amp;quot;式的思考方式，不仅展现了深刻的学术洞察力，也为研究者提供了一套理解和反思自身工作的思维工具。&lt;/p&gt;
&lt;p&gt;演讲的核心结构围绕四个类比展开。第一个类比将研究过程比作在非凸损失函数上执行随机梯度下降（SGD），强调研究中充满噪声和不确定性，大学习率代表快速探索新领域，小学习率则代表在已知方向上深入挖掘。第二个类比指出，机器学习模型追求的是期望收益的最大化，而研究的真正价值在于发现那些挑战现有认知的&amp;quot;惊喜&amp;quot;。&lt;/p&gt;
&lt;p&gt;第三个类比提出&amp;quot;未来是真正的测试集&amp;quot;这一深刻观点，提醒研究者警惕对当前基准和评估体系的&amp;quot;过拟合&amp;quot;，真正有价值的研究应当经得起未来的检验。第四个类比则聚焦于可扩展性，随着计算能力的持续提升，研究工作是否具备良好的扩展规律，将决定其长期影响力。&lt;/p&gt;
&lt;p&gt;整体而言，演讲通过这些精妙的类比，鼓励研究者在探索中保持开放心态，关注研究的长远价值与实际应用，而非仅仅追求短期的指标提升。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;研究即随机梯度下降&lt;/strong&gt;：何恺明将研究过程比作 SGD 在非凸损失函数中的优化过程。研究充满不确定性和噪声，每一次实验就像一步梯度更新。大学习率意味着敢于跳出舒适区探索全新方向，小学习率则意味着在有前景的方向上精细打磨。这个类比提醒研究者，既要有勇气进行大胆探索，也要有耐心做深入研究，二者的平衡至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;寻找&amp;quot;惊喜&amp;quot;而非期望&lt;/strong&gt;：机器学习模型以最大化期望收益为目标，但研究的突破往往来自于那些违反直觉、挑战常识的意外发现。这些&amp;quot;惊喜&amp;quot;可能最初只是偶然观察到的异常现象，但经过严谨验证后，可能颠覆现有理论，成为未来新范式的基石。这一类比鼓励研究者重视实验中的异常结果，而非简单忽略或丢弃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;未来作为测试集&lt;/strong&gt;：在机器学习中，模型的真正能力体现在从未见过的测试数据上的表现。类似地，研究成果的真正价值取决于其对未来的影响力，而非仅仅在当前基准上的分数。研究者需要警惕对现有评估体系的&amp;quot;过拟合&amp;quot;——即过度针对特定数据集或指标进行优化，而忽视了方法的普适性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;研究的可扩展性&lt;/strong&gt;：随着算力和数据规模的持续增长，研究方法是否具备良好的扩展规律（Scaling Law）变得愈发重要。一个在小规模上表现优异但无法扩展的方法，其长期价值可能有限。理解并利用扩展规律，有助于研究者将工作聚焦于那些能够随技术发展而持续受益的方向，从而在快速演进的领域中保持竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;探索与利用的平衡&lt;/strong&gt;：贯穿整个演讲的一个隐含主题是研究中&amp;quot;探索&amp;quot;与&amp;quot;利用&amp;quot;的平衡问题。这与强化学习中的经典难题一脉相承——何时应该探索全新的未知方向，何时应该深入利用已知的有效路径。何恺明的四个类比从不同角度回应了这一根本问题，为研究者提供了多维度的思考框架。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/QtfAJ-I5KZfWSBmCwEam3A"&gt;何恺明 NeurIPS 2024 Talk 分享&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;何恺明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024年12月27日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;会议&lt;/td&gt;
 &lt;td&gt;NeurIPS 2024 NewInML Workshop&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;论文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://people.csail.mit.edu/kaiming/neurips2024workshop/neurips2024_newinml_kaiming.pdf"&gt;PDF链接&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深度神经网络的优势与应用</title><link>https://linguista.cn/curated/henrinotes-2025-p1/deep-neural-networks-advantages-applications/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/deep-neural-networks-advantages-applications/</guid><description>&lt;h1 id="深度神经网络的优势与应用"&gt;深度神经网络的优势与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文探讨了深度神经网络相较于三层浅层网络的核心优势。虽然万能逼近定理证明了三层网络理论上可逼近任何连续函数，但深度网络在特征提取效率、参数利用和泛化能力上具有显著优势。通过分层特征学习、参数共享和稀疏连接等机制，深度网络能够更高效地处理复杂数据结构，在现代人工智能应用中发挥着不可替代的作用。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了三层神经网络的理论基础——万能逼近定理，同时指出了理论在实际应用中的局限性。浅层网络虽然理论上完备，但为了逼近高维复杂函数往往需要指数级增长的神经元数量，这在计算上是不可行的。此外，浅层网络在优化过程中容易陷入局部最优，且缺乏分层特征表示能力。&lt;/p&gt;
&lt;p&gt;接着，文章详细介绍了深度神经网络的三大核心优势。首先是分层特征提取机制，网络能够从低层的简单模式（如边缘、纹理）逐层抽象到高层的语义概念。其次是参数共享与稀疏连接设计，这大幅降低了模型复杂度和计算需求。最后是深度网络更强的表达能力，研究表明其可以用更少的参数实现比浅层网络更高的逼近能力。&lt;/p&gt;
&lt;p&gt;文章还介绍了深度神经网络的训练优化技术，包括反向传播、激活函数改进、批归一化、正则化技术以及预训练与迁移学习等。在实际应用层面，深度学习已在图像处理、自然语言处理和强化学习等领域取得突破性进展，成为推动人工智能发展的关键技术。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;万能逼近定理&lt;/strong&gt;：这是神经网络理论的基石，证明了具有单个隐藏层的前馈神经网络在合适的条件下可以逼近任何连续函数。然而，这并不意味着三层网络就是最优选择，因为定理仅保证了存在性，并未考虑实现这种逼近所需的计算资源和训练难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分层特征提取&lt;/strong&gt;：深度网络的核心优势在于其能够自动学习多层次的特征表示。低层捕获简单模式如边缘和纹理，中层识别局部结构和形状，高层则理解全局概念和语义信息。这种层级抽象机制使得深度网络能够高效处理复杂的视觉和语言数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参数共享与稀疏连接&lt;/strong&gt;：这是卷积神经网络等架构的关键设计理念。通过在整个输入空间共享同一组权重，并限制神经元只与局部邻域连接，模型可以用更少的参数处理高维数据，这不仅降低了计算复杂度，也提高了模型的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;反向传播与优化技巧&lt;/strong&gt;：深度网络的训练依赖于反向传播算法和梯度下降优化。为了解决深层网络训练中的梯度消失和过拟合问题，现代深度学习发展了多种技术，包括ReLU激活函数、批归一化、Dropout正则化以及预训练与迁移学习等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;泛化能力&lt;/strong&gt;：深度网络通过学习更具普适性的特征表示，能够在未见过的数据上保持良好性能。这种泛化能力源于深度架构能够捕获数据中的本质规律而非仅仅记忆训练样本，这也是深度学习在实际应用中取得成功的关键因素。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D47cZT7DvATC1VGwWjUFgw"&gt;一般来说，三层神经网络可以逼近任何一个非线性函数，为什么还需要深度神经网络？为什么深度学习模型能够自动提取多层次特征？｜深度学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;深度学习&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-04&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>稀疏贝叶斯学习的理论与应用</title><link>https://linguista.cn/curated/henrinotes-2025-p1/sparse-bayesian-learning-theory-application/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/sparse-bayesian-learning-theory-application/</guid><description>&lt;h1 id="稀疏贝叶斯学习的理论与应用"&gt;稀疏贝叶斯学习的理论与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;稀疏贝叶斯学习（SBL）是一种结合稀疏性与贝叶斯统计的学习方法，广泛应用于信号处理和机器学习领域。本文从贝叶斯统计与稀疏表示的理论基础出发，详细阐述了SBL模型的构建与优化过程，包括边缘似然最大化和EM算法的具体步骤，并通过实验对比了SBL与Ridge回归在不同噪声条件下的表现，验证了SBL在高噪声环境中更强的稳定性和适应性。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了稀疏贝叶斯学习的基本定义与核心特点。SBL与传统稀疏方法（如L1正则化）的关键区别在于，它不需要预先指定稀疏度参数，而是通过贝叶斯推理自适应地发现数据中的稀疏结构，这使得模型在应用中具有更高的灵活性。&lt;/p&gt;
&lt;p&gt;在理论基础部分，文章系统回顾了贝叶斯统计的核心要素——先验分布、似然函数、后验分布和边缘似然，并介绍了稀疏表示的基本概念，包括Lasso、L1和L2正则化等常见方法，为后续的模型构建奠定了理论基础。&lt;/p&gt;
&lt;p&gt;模型构建与优化是文章的核心内容。文章以线性回归模型为框架，采用零均值高斯分布作为参数的稀疏先验分布，通过独立的精度参数控制每个参数的方差。在优化过程中，采用期望最大化（EM）算法，通过E步骤计算参数的后验分布，M步骤更新超参数和噪声方差，逐步最大化边缘似然。&lt;/p&gt;
&lt;p&gt;实验部分通过生成高维稀疏数据，对比了SBL与Ridge回归在不同噪声水平下的性能表现，使用MSE、MAE、最大误差和R²分数等多维指标进行评估，结果表明SBL在高噪声条件下展现出更强的稳定性和抗噪能力。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;稀疏贝叶斯学习（SBL）&lt;/strong&gt;：一种将贝叶斯推理与稀疏性约束相结合的学习框架。其核心优势在于无需手动设定稀疏度参数，而是通过贝叶斯推理自动识别数据中的稀疏结构。模型通过精度参数的自适应调节实现稀疏性——当某个精度参数值很大时，对应的参数方差趋近于零，从而自然地实现参数的&amp;quot;裁剪&amp;quot;，得到稀疏解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边缘似然最大化&lt;/strong&gt;：SBL模型优化的核心策略。边缘似然是在对模型参数积分后得到的数据概率，它综合考虑了模型复杂度和数据拟合度，天然具备奥卡姆剃刀效应。通过最大化边缘似然来估计精度参数和噪声方差，避免了过拟合风险，使模型能够在数据拟合与结构简洁之间取得平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;期望最大化（EM）算法&lt;/strong&gt;：SBL模型参数估计的迭代优化方法。E步骤在当前超参数下计算模型参数的后验分布，获得均值和协方差；M步骤利用后验统计量更新精度参数和噪声方差。两个步骤交替迭代直至收敛，逐步逼近边缘似然的最大值，是SBL实现自适应稀疏学习的关键算法机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;稀疏先验分布&lt;/strong&gt;：SBL模型中对参数施加的先验假设，通常采用零均值高斯分布，并为每个参数引入独立的精度（逆方差）超参数。这种层级化的先验设计使模型能够对不同参数施加不同强度的约束，精度越大则参数越可能为零，从而在贝叶斯框架下自然地引导出稀疏解，而非通过硬性的正则化惩罚项实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SBL与Ridge回归的对比实验&lt;/strong&gt;：实验在不同噪声水平下对比了两种方法的性能。在低噪声条件下，两者表现相当；但在高噪声环境中，Ridge回归的解释方差（R²分数）波动较大，而SBL保持相对稳定，体现了贝叶斯方法在不确定性建模方面的天然优势，以及SBL通过自适应稀疏机制过滤噪声干扰的能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D21HuJN6qq8QBjv86V5BTA"&gt;稀疏贝叶斯学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>苦涩的教训</title><link>https://linguista.cn/essays/bitter-lesson/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0800</pubDate><guid>https://linguista.cn/essays/bitter-lesson/</guid><description>&lt;p&gt;本文是里奇·萨顿（Richard Sutton）2019年著名文章《&lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;The Bitter Lesson（苦涩的教训）&lt;/a&gt;》。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts3.tc.mm.bing.net/th/id/OIP-C.5-o1C9ggTpwDMynLnfilkgHaE8?cb=12&amp;amp;rs=1&amp;amp;pid=ImgDetMain&amp;amp;o=7&amp;amp;rm=3" alt=""&gt;&lt;/p&gt;</description></item><item><title>阅读收藏</title><link>https://linguista.cn/bookmarks/reading_list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://linguista.cn/bookmarks/reading_list/</guid><description>&lt;p&gt;本页为我在阅读过程中所收藏的网页目录。&lt;/p&gt;</description></item></channel></rss>