<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on Linguista</title><link>https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 04 Oct 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>动物与幽灵</title><link>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</link><pubDate>Sat, 04 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/essays/rosetta/animals-vs-ghosts/</guid><description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;动物可能是一个很好的灵感来源。内在动机、乐趣、好奇心、赋能、多智能体自我博弈、文化。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Andrej Karpathy在本文中探讨了人工智能领域中“动物”智能与大语言模型（“幽灵”）的差异，反思了学习机制、进化和预训练的本质，并提出动物或许能为AI带来新的灵感和范式。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts1.tc.mm.bing.net/th/id/R-C.5b6ee267317d44b17842e771033bc7f4?rik=U6xjpiLj2AM6Mw&amp;amp;riu=http%3a%2f%2fn.sinaimg.cn%2fsinakd20117%2f762%2fw1000h562%2f20230516%2f61f2-929bc75cae6683aec311ff02b0528b5e.jpg&amp;amp;ehk=YX8liBBPYwNpcJkyffP6ne%2b5uHTTJzeDIJnnV%2f%2fKEPM%3d&amp;amp;risl=&amp;amp;pid=ImgRaw&amp;amp;r=0" alt=""&gt;&lt;/p&gt;</description></item><item><title>从机器学习模型的视角看机器学习研究</title><link>https://linguista.cn/curated/henrinotes-2025/kaiming-he-ml-research-from-model-perspective/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025/kaiming-he-ml-research-from-model-perspective/</guid><description>&lt;h1 id="从机器学习模型的视角看机器学习研究"&gt;从机器学习模型的视角看机器学习研究&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;何恺明教授在 NeurIPS 2024 NewInML Workshop 上发表演讲，从机器学习模型自身的视角出发，通过四个生动类比来阐释机器学习研究的本质与方向。这四个类比分别是：研究如同随机梯度下降、研究的目标是寻找&amp;quot;惊喜&amp;quot;、未来是真正的测试集、以及研究需要具备可扩展性。演讲深刻揭示了研究过程中的不确定性、探索性与长远价值。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;何恺明教授的这次演讲以一种独特的视角切入——用机器学习自身的概念和框架来审视机器学习研究本身。这种&amp;quot;元认知&amp;quot;式的思考方式，不仅展现了深刻的学术洞察力，也为研究者提供了一套理解和反思自身工作的思维工具。&lt;/p&gt;
&lt;p&gt;演讲的核心结构围绕四个类比展开。第一个类比将研究过程比作在非凸损失函数上执行随机梯度下降（SGD），强调研究中充满噪声和不确定性，大学习率代表快速探索新领域，小学习率则代表在已知方向上深入挖掘。第二个类比指出，机器学习模型追求的是期望收益的最大化，而研究的真正价值在于发现那些挑战现有认知的&amp;quot;惊喜&amp;quot;。&lt;/p&gt;
&lt;p&gt;第三个类比提出&amp;quot;未来是真正的测试集&amp;quot;这一深刻观点，提醒研究者警惕对当前基准和评估体系的&amp;quot;过拟合&amp;quot;，真正有价值的研究应当经得起未来的检验。第四个类比则聚焦于可扩展性，随着计算能力的持续提升，研究工作是否具备良好的扩展规律，将决定其长期影响力。&lt;/p&gt;
&lt;p&gt;整体而言，演讲通过这些精妙的类比，鼓励研究者在探索中保持开放心态，关注研究的长远价值与实际应用，而非仅仅追求短期的指标提升。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;研究即随机梯度下降&lt;/strong&gt;：何恺明将研究过程比作 SGD 在非凸损失函数中的优化过程。研究充满不确定性和噪声，每一次实验就像一步梯度更新。大学习率意味着敢于跳出舒适区探索全新方向，小学习率则意味着在有前景的方向上精细打磨。这个类比提醒研究者，既要有勇气进行大胆探索，也要有耐心做深入研究，二者的平衡至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;寻找&amp;quot;惊喜&amp;quot;而非期望&lt;/strong&gt;：机器学习模型以最大化期望收益为目标，但研究的突破往往来自于那些违反直觉、挑战常识的意外发现。这些&amp;quot;惊喜&amp;quot;可能最初只是偶然观察到的异常现象，但经过严谨验证后，可能颠覆现有理论，成为未来新范式的基石。这一类比鼓励研究者重视实验中的异常结果，而非简单忽略或丢弃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;未来作为测试集&lt;/strong&gt;：在机器学习中，模型的真正能力体现在从未见过的测试数据上的表现。类似地，研究成果的真正价值取决于其对未来的影响力，而非仅仅在当前基准上的分数。研究者需要警惕对现有评估体系的&amp;quot;过拟合&amp;quot;——即过度针对特定数据集或指标进行优化，而忽视了方法的普适性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;研究的可扩展性&lt;/strong&gt;：随着算力和数据规模的持续增长，研究方法是否具备良好的扩展规律（Scaling Law）变得愈发重要。一个在小规模上表现优异但无法扩展的方法，其长期价值可能有限。理解并利用扩展规律，有助于研究者将工作聚焦于那些能够随技术发展而持续受益的方向，从而在快速演进的领域中保持竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;探索与利用的平衡&lt;/strong&gt;：贯穿整个演讲的一个隐含主题是研究中&amp;quot;探索&amp;quot;与&amp;quot;利用&amp;quot;的平衡问题。这与强化学习中的经典难题一脉相承——何时应该探索全新的未知方向，何时应该深入利用已知的有效路径。何恺明的四个类比从不同角度回应了这一根本问题，为研究者提供了多维度的思考框架。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/QtfAJ-I5KZfWSBmCwEam3A"&gt;何恺明 NeurIPS 2024 Talk 分享&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;何恺明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024年12月27日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;会议&lt;/td&gt;
 &lt;td&gt;NeurIPS 2024 NewInML Workshop&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;论文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://people.csail.mit.edu/kaiming/neurips2024workshop/neurips2024_newinml_kaiming.pdf"&gt;PDF链接&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深度神经网络的优势与应用</title><link>https://linguista.cn/curated/henrinotes-2025/deep-neural-networks-advantages-applications/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025/deep-neural-networks-advantages-applications/</guid><description>&lt;h1 id="深度神经网络的优势与应用"&gt;深度神经网络的优势与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文探讨了深度神经网络相较于三层浅层网络的核心优势。虽然万能逼近定理证明了三层网络理论上可逼近任何连续函数，但深度网络在特征提取效率、参数利用和泛化能力上具有显著优势。通过分层特征学习、参数共享和稀疏连接等机制，深度网络能够更高效地处理复杂数据结构，在现代人工智能应用中发挥着不可替代的作用。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了三层神经网络的理论基础——万能逼近定理，同时指出了理论在实际应用中的局限性。浅层网络虽然理论上完备，但为了逼近高维复杂函数往往需要指数级增长的神经元数量，这在计算上是不可行的。此外，浅层网络在优化过程中容易陷入局部最优，且缺乏分层特征表示能力。&lt;/p&gt;
&lt;p&gt;接着，文章详细介绍了深度神经网络的三大核心优势。首先是分层特征提取机制，网络能够从低层的简单模式（如边缘、纹理）逐层抽象到高层的语义概念。其次是参数共享与稀疏连接设计，这大幅降低了模型复杂度和计算需求。最后是深度网络更强的表达能力，研究表明其可以用更少的参数实现比浅层网络更高的逼近能力。&lt;/p&gt;
&lt;p&gt;文章还介绍了深度神经网络的训练优化技术，包括反向传播、激活函数改进、批归一化、正则化技术以及预训练与迁移学习等。在实际应用层面，深度学习已在图像处理、自然语言处理和强化学习等领域取得突破性进展，成为推动人工智能发展的关键技术。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;万能逼近定理&lt;/strong&gt;：这是神经网络理论的基石，证明了具有单个隐藏层的前馈神经网络在合适的条件下可以逼近任何连续函数。然而，这并不意味着三层网络就是最优选择，因为定理仅保证了存在性，并未考虑实现这种逼近所需的计算资源和训练难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分层特征提取&lt;/strong&gt;：深度网络的核心优势在于其能够自动学习多层次的特征表示。低层捕获简单模式如边缘和纹理，中层识别局部结构和形状，高层则理解全局概念和语义信息。这种层级抽象机制使得深度网络能够高效处理复杂的视觉和语言数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参数共享与稀疏连接&lt;/strong&gt;：这是卷积神经网络等架构的关键设计理念。通过在整个输入空间共享同一组权重，并限制神经元只与局部邻域连接，模型可以用更少的参数处理高维数据，这不仅降低了计算复杂度，也提高了模型的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;反向传播与优化技巧&lt;/strong&gt;：深度网络的训练依赖于反向传播算法和梯度下降优化。为了解决深层网络训练中的梯度消失和过拟合问题，现代深度学习发展了多种技术，包括ReLU激活函数、批归一化、Dropout正则化以及预训练与迁移学习等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;泛化能力&lt;/strong&gt;：深度网络通过学习更具普适性的特征表示，能够在未见过的数据上保持良好性能。这种泛化能力源于深度架构能够捕获数据中的本质规律而非仅仅记忆训练样本，这也是深度学习在实际应用中取得成功的关键因素。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D47cZT7DvATC1VGwWjUFgw"&gt;一般来说，三层神经网络可以逼近任何一个非线性函数，为什么还需要深度神经网络？为什么深度学习模型能够自动提取多层次特征？｜深度学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;深度学习&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-04&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>稀疏贝叶斯学习的理论与应用</title><link>https://linguista.cn/curated/henrinotes-2025/sparse-bayesian-learning-theory-application/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025/sparse-bayesian-learning-theory-application/</guid><description>&lt;h1 id="稀疏贝叶斯学习的理论与应用"&gt;稀疏贝叶斯学习的理论与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;稀疏贝叶斯学习（SBL）是一种结合稀疏性与贝叶斯统计的学习方法，广泛应用于信号处理和机器学习领域。本文从贝叶斯统计与稀疏表示的理论基础出发，详细阐述了SBL模型的构建与优化过程，包括边缘似然最大化和EM算法的具体步骤，并通过实验对比了SBL与Ridge回归在不同噪声条件下的表现，验证了SBL在高噪声环境中更强的稳定性和适应性。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了稀疏贝叶斯学习的基本定义与核心特点。SBL与传统稀疏方法（如L1正则化）的关键区别在于，它不需要预先指定稀疏度参数，而是通过贝叶斯推理自适应地发现数据中的稀疏结构，这使得模型在应用中具有更高的灵活性。&lt;/p&gt;
&lt;p&gt;在理论基础部分，文章系统回顾了贝叶斯统计的核心要素——先验分布、似然函数、后验分布和边缘似然，并介绍了稀疏表示的基本概念，包括Lasso、L1和L2正则化等常见方法，为后续的模型构建奠定了理论基础。&lt;/p&gt;
&lt;p&gt;模型构建与优化是文章的核心内容。文章以线性回归模型为框架，采用零均值高斯分布作为参数的稀疏先验分布，通过独立的精度参数控制每个参数的方差。在优化过程中，采用期望最大化（EM）算法，通过E步骤计算参数的后验分布，M步骤更新超参数和噪声方差，逐步最大化边缘似然。&lt;/p&gt;
&lt;p&gt;实验部分通过生成高维稀疏数据，对比了SBL与Ridge回归在不同噪声水平下的性能表现，使用MSE、MAE、最大误差和R²分数等多维指标进行评估，结果表明SBL在高噪声条件下展现出更强的稳定性和抗噪能力。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;稀疏贝叶斯学习（SBL）&lt;/strong&gt;：一种将贝叶斯推理与稀疏性约束相结合的学习框架。其核心优势在于无需手动设定稀疏度参数，而是通过贝叶斯推理自动识别数据中的稀疏结构。模型通过精度参数的自适应调节实现稀疏性——当某个精度参数值很大时，对应的参数方差趋近于零，从而自然地实现参数的&amp;quot;裁剪&amp;quot;，得到稀疏解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边缘似然最大化&lt;/strong&gt;：SBL模型优化的核心策略。边缘似然是在对模型参数积分后得到的数据概率，它综合考虑了模型复杂度和数据拟合度，天然具备奥卡姆剃刀效应。通过最大化边缘似然来估计精度参数和噪声方差，避免了过拟合风险，使模型能够在数据拟合与结构简洁之间取得平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;期望最大化（EM）算法&lt;/strong&gt;：SBL模型参数估计的迭代优化方法。E步骤在当前超参数下计算模型参数的后验分布，获得均值和协方差；M步骤利用后验统计量更新精度参数和噪声方差。两个步骤交替迭代直至收敛，逐步逼近边缘似然的最大值，是SBL实现自适应稀疏学习的关键算法机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;稀疏先验分布&lt;/strong&gt;：SBL模型中对参数施加的先验假设，通常采用零均值高斯分布，并为每个参数引入独立的精度（逆方差）超参数。这种层级化的先验设计使模型能够对不同参数施加不同强度的约束，精度越大则参数越可能为零，从而在贝叶斯框架下自然地引导出稀疏解，而非通过硬性的正则化惩罚项实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SBL与Ridge回归的对比实验&lt;/strong&gt;：实验在不同噪声水平下对比了两种方法的性能。在低噪声条件下，两者表现相当；但在高噪声环境中，Ridge回归的解释方差（R²分数）波动较大，而SBL保持相对稳定，体现了贝叶斯方法在不确定性建模方面的天然优势，以及SBL通过自适应稀疏机制过滤噪声干扰的能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D21HuJN6qq8QBjv86V5BTA"&gt;稀疏贝叶斯学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>苦涩的教训</title><link>https://linguista.cn/essays/bitter-lesson/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0800</pubDate><guid>https://linguista.cn/essays/bitter-lesson/</guid><description>&lt;p&gt;本文是里奇·萨顿（Richard Sutton）2019年著名文章《&lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;The Bitter Lesson（苦涩的教训）&lt;/a&gt;》。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ts3.tc.mm.bing.net/th/id/OIP-C.5-o1C9ggTpwDMynLnfilkgHaE8?cb=12&amp;amp;rs=1&amp;amp;pid=ImgDetMain&amp;amp;o=7&amp;amp;rm=3" alt=""&gt;&lt;/p&gt;</description></item><item><title>阅读收藏</title><link>https://linguista.cn/bookmarks/reading_list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://linguista.cn/bookmarks/reading_list/</guid><description>&lt;p&gt;本页为我在阅读过程中所收藏的网页目录。&lt;/p&gt;</description></item></channel></rss>