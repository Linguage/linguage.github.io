<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>强化学习 on Linguista</title><link>https://linguista.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 强化学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 27 Sep 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenAI的氛围编程到氛围研究：AI研究新范式</title><link>https://linguista.cn/curated/henrinotes-2025_p2/openai-vibe-coding-research-new-paradigm/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/openai-vibe-coding-research-new-paradigm/</guid><description>&lt;h1 id="openai的氛围编程到氛围研究ai研究新范式"&gt;OpenAI的氛围编程到氛围研究：AI研究新范式&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本期播客中，OpenAI首席科学家Jakub Pachocki与首席研究官Mark Chen与a16z深入对话，全面解析GPT-5的技术突破、AI自动化研究的愿景，以及在极高压力下推动前沿AI发展的实践经验。他们详细阐述了从传统的即时应答模型向具备深度推理能力的新范式的转变，分享了强化学习持续突破的经验，探讨了&amp;quot;氛围编程&amp;quot;如何重塑开发方式，并揭示了打造世界级AI研究团队的文化密码。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;对谈首先聚焦GPT-5的核心创新，其最大突破在于将&amp;quot;推理&amp;quot;能力引入主流AI产品，使模型能够针对复杂问题进行深度思考而非即时应答。这种变革标志着AI评测体系从传统基准测试转向更具经济相关性的指标，如自动发现新知识的能力。团队分享了数学和物理领域专家使用模型解决数月难题的真实案例，展现了AI在硬科学领域的突破性进展。&lt;/p&gt;
&lt;p&gt;关于AI自动化研究的未来蓝图，OpenAI的终极目标是打造能够自主进行实验、发现并推进新理论的&amp;quot;自动化研究者&amp;quot;。这一目标驱动着方法论从单一预训练向强化学习深度引导转型，重点突破&amp;quot;长远推理&amp;quot;与&amp;quot;记忆保持&amp;quot;两大核心能力。在编程领域，&amp;ldquo;氛围编程&amp;quot;已成为新一代开发者的默认方式，AI根据问题难度自适应响应，快速搭建原型而非手动实现每一行代码。&lt;/p&gt;
&lt;p&gt;在研究团队建设方面，两位负责人强调了&amp;quot;氛围研究&amp;quot;精神的重要性，核心包括坚定信念、持续追问和对失败的容忍。他们分享了如何发现和培养&amp;quot;洞穴探险者&amp;quot;型天才，如何在产品需求与基础研究之间保持平衡，以及如何动态分配算力资源以在前沿突破和产品化应用之间找到最佳平衡点。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;自动化研究者&lt;/strong&gt;：OpenAI的终极愿景，指能够自主进行科学研究、设计实验、发现新理论并推进技术边界的AI系统。这要求AI具备长远推理、记忆保持和自我修正能力，不仅能在数学、编程竞赛中达到专业水平，更要迈向自主发现新领域的阶段。当前GPT-5已在硬科学领域展现出自动生成新知识结构的初步能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;氛围编程&lt;/strong&gt;：新一代开发者借助AI自动生成代码与建议、快速搭建原型的主流工作方式。与传统的逐行手动实现不同，氛围编程强调利用AI的智能来加速开发流程，模型会根据问题难度自动分配计算资源和等待时间，在简单问题快速响应与复杂问题深度思考之间找到最佳平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习持续突破&lt;/strong&gt;：尽管业界曾担心强化学习会面临泛化不足、模态坍塌等瓶颈，但OpenAI凭借丰富语言环境下的持续实践屡次打破这些预期。奖励建模正从手工微调向更接近人类学习的自动化范式演化，显著降低了使用门槛。未来范式将更加注重人类样本效率和直观易用的奖励机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;长远推理与记忆保持&lt;/strong&gt;：下一代AI的核心能力，要求模型能够持续操作数小时乃至更久的任务，在多步骤规划与调整决策的复杂场景下保持稳定高效。这涉及在&amp;quot;稳定性&amp;quot;与&amp;quot;深度&amp;quot;之间的精细权衡，步骤越多后续推理精度可能下降，但单一步骤又难以突破自主创造的边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;洞穴探险者型研究者&lt;/strong&gt;：OpenAI青睐的人才类型，他们可能不是社交媒体活跃分子或频繁发表论文者，但善于独立解决极难问题，倾向于攻克&amp;quot;常人不认为可解&amp;quot;的挑战。这类研究者往往具备跨学科背景，能够在物理、金融、计算机科学等领域间建立创新连接。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=KSgPNVmZ8jQ"&gt;From Vibe Coding to Vibe Researching OpenAI&amp;rsquo;s Mark Chen and Jakub Pachocki&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;a16z（对谈嘉宾：Jakub Pachocki、Mark Chen，主持人：Anjney Midha、Sarah Wang）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;YouTube a16z播客&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>DeepSeek-R1如何用强化学习激发大模型推理能力</title><link>https://linguista.cn/curated/henrinotes-2025_p2/deepseek-r1-reinforcement-learning-reasoning/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/deepseek-r1-reinforcement-learning-reasoning/</guid><description>&lt;h1 id="deepseek-r1如何用强化学习激发大模型推理能力"&gt;DeepSeek-R1如何用强化学习激发大模型推理能力&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1项目通过强化学习（Reinforcement Learning, RL）在极少人类标注、无需人工推理示范的前提下，让大规模语言模型（LLMs）自主进化出复杂的推理能力。其强化学习机制不仅使模型在数学、代码、STEM等可验证领域远超传统有监督学习模型，还促使模型形成自反思、验证与动态策略适应等高级推理行为。该框架揭示，大模型原生具备高度可塑的推理潜力，其能力关键在于难题驱动、验证器和充足算力支撑下的持续自我优化。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先探讨了大模型推理能力的瓶颈与强化学习的动机。传统LLM虽然能在大规模参数与高效数据驱动下涌现出数学、逻辑推理及代码等能力，但这些能力的进一步提升受两大主要限制：大规模人类标注高度依赖人力，导致成本和规模受限，并会引入人为思维路径的固有限制，抑制模型自发探索超越人类的策略。&lt;/p&gt;
&lt;p&gt;DeepSeek-R1提出了用强化学习挖掘模型自进化能力的方案——仅以最终答案与正确性为信号，完全不限定中间推理过程，让模型自主发展推理路径。实验基于DeepSeek-V3 Base，利用GRPO（群组相对策略优化）作为核心RL算法，只用最终答案的正确与否作为奖励信号。&lt;/p&gt;
&lt;p&gt;在强化学习训练过程中，模型逐步学会生成更长、更严谨的推理过程，内容包含自我验证、反思、方案再试验等。此进化完全非人类指导，模型通过与&amp;quot;地面真值&amp;quot;对比，自主修正推理方式。训练初期AIME 2024竞赛成绩仅15.6%，RL收敛后提升至77.9%，用自洽推理采样更可达86.7%，远超所有人类参赛平均分。&lt;/p&gt;
&lt;p&gt;DeepSeek-R1在Zero基础上，由于原生模型会出现中英文混杂、文风难读等实际问题，因此在多轮RL基础上融入部分人类标注初始样本，结合RL与有监督微调，并引入拒绝采样和&amp;quot;奖励模型&amp;quot;用于对齐人类偏好和安全性。训练流程包括冷启动数据强化、多阶段RL、拒绝采样与SFT以及最终偏好对齐等多个阶段。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GRPO（群组相对策略优化）&lt;/strong&gt;：这是DeepSeek-R1使用的核心强化学习算法。它通过群组采样—多路输出对齐机制，鼓励模型尝试更多解题方案，自动发展如反思验证、备用策略生成等能力。相比传统策略优化方法，GRPO能够在无需人工标注中间推理步骤的情况下，仅依靠最终答案的正确性来引导模型优化其推理策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自反思（Self-Reflection）&lt;/strong&gt;：模型在推理过程中生成自我检查（如&amp;quot;wait&amp;quot;），反复迭代以修正潜在错误，并非简单线性输出。这一过程源于奖励信号的诱导，而非人工预设。在RL训练中，模型逐步学会生成更长、更严谨的推理过程，包含自我验证、反思、方案再试验等高级元认知行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态策略适应（Dynamic Strategy Adaptation）&lt;/strong&gt;：面对不同问题，模型能灵活分配计算资源——易题给少量Token、难题就大规模生成长链推理。模型会探索多路径、取自一致答案以自动校验正确性。这种能力不是人工设计的，而是通过强化学习自然涌现的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多阶段训练流水线&lt;/strong&gt;：DeepSeek-R1采用结构化分解的训练流程，包括极简模板加RL以最大释放模型原生推理潜力、少量人工&amp;quot;冷启动&amp;quot;矫正输出风格、奖励模型引导对齐人类偏好和安全防控。这种分层设计既保证了推理能力的深度发展，又确保了输出质量和可用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;奖励黑客问题（Reward Hacking）&lt;/strong&gt;：这是强化学习面临的重大挑战。部分场景难设计可靠奖励函数，例如写作类输出很难自动判优劣，传统模型奖励很容易被策略模型&amp;quot;钻空子&amp;quot;，导致奖励信号被攻击而训练失效。DeepSeek-R1团队认为需要构建更智能泛化奖励模型与RL环境来应对这一问题。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;DeepSeek-AI团队（代表作者：Daya Guo等）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>理解推理型大型语言模型的构建与优化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</guid><description>&lt;h1 id="理解推理型大型语言模型的构建与优化"&gt;理解推理型大型语言模型的构建与优化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文由AI专家Sebastian Raschka撰写，系统介绍了推理型大型语言模型的核心概念与构建方法。文章详细分析了推理模型的定义、优势与劣势，并以DeepSeek R1为例，阐述了纯强化学习、监督微调、模型蒸馏等四种主要的训练优化方法。作者还探讨了低预算下开发推理模型的可行性，为研究者和开发者提供了实用的技术路线参考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从LLM领域的发展趋势切入，指出2024年以来专门化应用方向的快速发展。推理模型作为一类能够通过多步中间步骤解决复杂任务的特殊模型，在数学证明、逻辑谜题和高级编程等场景中具有重要价值，但在简单任务中可能因&amp;quot;过度思考&amp;quot;而降低效率。&lt;/p&gt;
&lt;p&gt;DeepSeek R1系列模型作为典型案例，展示了三种不同的训练范式：R1-Zero采用纯强化学习，无需监督微调即可自动生成推理步骤；R1结合了监督微调与强化学习，引入一致性奖励机制；R1-Distill则通过模型蒸馏技术，将大型模型的推理能力迁移到较小的模型中。&lt;/p&gt;
&lt;p&gt;构建推理模型的四种主要方法各具特色：推理时扩展通过增加计算资源提高性能；纯强化学习展示了无需监督数据的可能性；监督微调结合强化学习能够充分发挥两者优势；模型蒸馏则在效率和成本方面具有显著优势。对于资源有限的开发者，Sky-T1和TinyZero等项目证明了低预算开发推理模型的可行性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理模型&lt;/strong&gt;：指需要通过复杂、多步生成中间步骤来回答问题的语言模型。这类模型能够解决需要逻辑推理的任务，如数学计算、编程挑战等，但在简单任务中可能导致效率低下和成本增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纯强化学习&lt;/strong&gt;：DeepSeek R1-Zero证明了推理能力可以通过纯强化学习而无需监督微调来实现。模型通过准确性和格式奖励自动生成推理步骤，这为理解AI推理能力的本质提供了新的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型蒸馏&lt;/strong&gt;：通过将大型模型生成的推理数据用于训练较小的模型，以提高推理能力。这种方法在效率和成本方面具有优势，使得资源有限的团队也能开发出性能可观的推理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推理时扩展&lt;/strong&gt;：通过增加推理时的计算资源来提高模型性能，包括链式思考提示、投票和搜索策略等。这种方法不需要重新训练模型，但会增加推理时间和成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;旅程学习&lt;/strong&gt;：一种新的训练策略，通过在训练数据中包含错误的解决方案路径，让模型从错误中学习。这种方法可能在低预算下开发推理模型时具有优势。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms"&gt;Understanding Reasoning LLMs&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Sebastian Raschka, PhD&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Andrej Karpathy 盛赞 DeepSeek R1 强化学习推动 AI 模型进化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</guid><description>&lt;h1 id="andrej-karpathy-盛赞-deepseek-r1强化学习推动-ai-模型进化"&gt;Andrej Karpathy 盛赞 DeepSeek R1：强化学习推动 AI 模型进化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于 Andrej Karpathy 的最新视频内容，详细介绍了 DeepSeek R1 在强化学习方面的技术创新。Karpathy 指出，DeepSeek R1 通过强化学习微调，在解决数学问题等方面表现出色，并且能够发现人类思考问题的逻辑和策略。文章还探讨了强化学习与人类反馈（RLHF）的优势与局限性，并展望了大语言模型在多模态能力和智能体方面的未来发展。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;Andrej Karpathy 作为 OpenAI 早期成员、前特斯拉 AI 总监，在其最新视频中对 DeepSeek R1 给予了高度评价。他回顾了从神经网络起源到最新大模型的发展历程，特别强调了 DeepSeek R1 在强化学习（RL）方面的技术创新。DeepSeek R1 是一个通过强化学习微调的大语言模型，其性能与 OpenAI 的模型不相上下，在解决数学问题时表现出色。&lt;/p&gt;
&lt;p&gt;Karpathy 认为，强化学习是大语言模型训练中的新兴阶段，目前仍处于起步状态。强化学习的优势在于能够突破人类的限制，发现人类未曾意识到的策略，例如 AlphaGo 在围棋中发明的创新走法。然而，强化学习也存在挑战，它非常擅长发现&amp;quot;欺骗&amp;quot;模型的方法，这可能会阻碍其在某些领域的应用。&lt;/p&gt;
&lt;p&gt;关于强化学习与人类反馈，Karpathy 指出，从人类反馈中进行强化学习（RLHF）能够提升模型性能，尤其是在那些无法验证的领域，如创意写作等。但 RLHF 的主要问题是基于人类反馈的强化学习可能会产生误导，因为人类反馈是一个有损模拟，无法完美反映真实人类的判断。&lt;/p&gt;
&lt;p&gt;展望未来，Karpathy 预测未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现多模态能力。此外，大语言模型将能够执行长期任务，人类将成为这些智能体任务的监督者。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：强化学习是大语言模型训练中的新兴阶段，通过奖励机制优化模型行为。DeepSeek R1 通过强化学习微调，在解决数学问题时表现出色，并且能够发现人类思考问题的逻辑和策略，如&amp;quot;思维链&amp;quot;（CoT）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLHF&lt;/strong&gt;：从人类反馈中进行强化学习能够提升模型性能，尤其是在那些无法验证的领域。人类标注者可以通过简单的排序任务来提供高质量的反馈，但人类反馈是一个有损模拟，无法完美反映真实人类的判断，可能会导致误导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多模态能力&lt;/strong&gt;：未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现真正的多模态能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/thTwdVgc4lfYRj6WWpKBwA"&gt;Andrej Karpathy 最新视频盛赞 DeepSeek：R1 正在发现人类思考的逻辑并进行复现&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;郑佳美&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年02月06日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>以DeepSeek R1为例学习推理型大语言模型</title><link>https://linguista.cn/curated/henrinotes-2025-p1/reasoning-llm-deepseek-r1-training-methods/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/reasoning-llm-deepseek-r1-training-methods/</guid><description>&lt;h1 id="以deepseek-r1为例学习推理型大语言模型"&gt;以DeepSeek R1为例学习推理型大语言模型&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文以DeepSeek R1为核心案例，系统梳理了推理型大语言模型的概念、适用场景与优劣势。文章详细解读了DeepSeek R1三个版本（R1-Zero、R1、R1-Distill）的训练流程，归纳了构建推理模型的四种主要方法，并介绍了Sky-T1和TinyZero等低成本推理模型的探索实践，为研究者和开发者理解与构建推理模型提供了全面指引。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从推理型大语言模型的定义出发，指出其核心特征是能够通过多步推理回答复杂问题，区别于常规LLM在于包含中间推理步骤和&amp;quot;思考&amp;quot;过程。作者首先分析了推理模型的适用场景——主要针对解谜、高级数学和编程挑战等需要复杂中间推理的任务，同时也坦诚指出其资源消耗大、输出冗长和&amp;quot;过度思考&amp;quot;等局限。&lt;/p&gt;
&lt;p&gt;文章的核心内容围绕DeepSeek R1的训练流程展开，详细介绍了三个版本的演进路径：R1-Zero通过纯强化学习涌现推理能力，R1在此基础上叠加监督微调和多轮RL训练以获得更强性能，R1-Distill则通过蒸馏将能力迁移到更小模型。这一递进式的技术路线清晰展示了推理模型的构建逻辑。&lt;/p&gt;
&lt;p&gt;在方法论层面，文章系统归纳了推理阶段扩展、纯强化学习、监督微调+强化学习、纯监督微调与蒸馏四种主要构建路径，并对比了各自的适用场景和优劣。此外，文章还介绍了Sky-T1（450美元训练成本）和TinyZero（不到30美元）等低成本方案，展示了推理模型平民化的可能性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理型大语言模型（Reasoning LLMs）&lt;/strong&gt;：指能够通过多步推理回答复杂问题的模型，与常规LLM的关键区别在于包含中间推理步骤，展现出&amp;quot;思考&amp;quot;过程。这类模型主要适用于解谜、数学证明、代码生成等需要复杂推理链的任务，而对摘要、翻译等简单任务可能并无优势，甚至因&amp;quot;过度思考&amp;quot;而产生错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeepSeek R1的三阶段训练流程&lt;/strong&gt;：R1-Zero基于671B参数的DeepSeek-V3通过纯RL训练，使用准确性奖励和格式奖励使模型涌现推理能力；R1在此基础上增加SFT和多轮RL，引入语言一致性奖励，性能显著提升；R1-Distill则利用R1生成的SFT数据微调Llama和Qwen等小模型，以更低成本获得推理能力。这三个版本体现了从探索到优化再到普及的技术路线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四种推理模型构建方法&lt;/strong&gt;：推理阶段扩展通过增加推理时计算资源提升输出质量，无需额外训练但增加推理成本；纯RL可涌现推理能力但性能有限，更适合理论研究；SFT+RL是性能最强的方案，DeepSeek R1即为典型代表；SFT+蒸馏适用于资源有限场景，通过大模型数据微调小模型实现能力迁移，但无法达到最前沿性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习中的奖励机制设计&lt;/strong&gt;：DeepSeek R1训练中采用了准确性奖励（如LeetCode编译器判定编程答案正确性）和格式奖励（LLM评估器判断回答格式），以及语言一致性奖励（防止模型在回答中切换语言）。这种多维度奖励设计是推理能力涌现和提升的关键，展示了RL在LLM训练中的精细化应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;低成本推理模型的可行性&lt;/strong&gt;：Sky-T1仅用1.7万条SFT数据和450美元训练出性能与o1相当的32B模型，TinyZero用不到30美元通过纯RL训练出3B参数的推理模型并展现出自我验证能力。这些探索表明，推理模型的构建门槛正在快速降低，为资源有限的研究者开辟了新路径。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/DG8-bENYNji8qg4SwexEmg"&gt;以DeepSeek R1为例学习推理型大语言模型&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;—&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-02-08&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>强化学习升温 白宫发布强硬AI政策及其他动态</title><link>https://linguista.cn/curated/henrinotes-2025_p2/reinforcement-learning-white-house-ai-policy/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/reinforcement-learning-white-house-ai-policy/</guid><description>&lt;h1 id="强化学习升温-白宫发布强硬ai政策及其他动态"&gt;强化学习升温 白宫发布强硬AI政策及其他动态&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文综合报道了AI领域的最新重要动态。DeepSeek-R1的开源发布标志着中国在生成式AI领域的显著进展，其性能媲美OpenAI o1但成本仅1/30。强化学习技术正成为提升大型语言模型推理能力的关键路径。OpenAI推出Operator代理进入消费市场，白宫签署AI行政令减少监管限制，合成数据微调技术取得新突破。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DeepSeek事件的开源冲击&lt;/strong&gt;：中国在生成式AI领域迅速追赶美国，DeepSeek-R1以开源形式发布，性能与OpenAI o1相当但成本仅为1/30。这标志着开源权重模型正在商品化基础模型层，降低了应用开发门槛。更重要的是，DeepSeek团队通过算法优化在较低性能的H800 GPU上训练出高性能模型，证明算力并非AI进步的唯一途径，算法创新同样关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习的崛起&lt;/strong&gt;：强化学习通过奖励或惩罚模型特定行为来训练，与监督学习不同，它不依赖于已知的&amp;quot;真实值&amp;quot;。DeepSeek-R1和Kimi k1.5都采用强化学习提升推理能力，使模型学会检查答案、优化输出长度等策略。强化学习在解决复杂数学和编程任务时展现出巨大潜力，正成为训练大型语言模型的重要技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI代理进入消费市场&lt;/strong&gt;：OpenAI推出Operator，允许用户通过ChatGPT执行购物、购票等简单网络任务。Operator基于新的Computer-Using Agent模型，在WebVoyager和OSWorld基准测试中表现出色。这标志着AI代理开始进入消费市场，可能成为下一代产品的模板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;政策环境变化&lt;/strong&gt;：特朗普总统签署AI行政令，要求180天内制定AI行动计划。政策要点包括撤销阻碍AI发展的监管、要求AI系统&amp;quot;无意识形态偏见&amp;quot;、优先支持符合国家竞争力的AI公司。新政策减少了AI开发的官僚监管，为创新提供更宽松环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;合成数据技术进展&lt;/strong&gt;：Cohere团队提出&amp;quot;主动继承&amp;quot;技术，通过选择理想特性的合成数据进行微调。实验表明，选择最低毒性的合成数据可显著降低模型毒性，同时保持或提升某些基准测试性能。这为合成数据训练提供了新方法，帮助模型减少负面特性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;开源模型商品化&lt;/strong&gt;：DeepSeek-R1的开源发布标志着基础模型层正在被商品化。开源权重降低了使用门槛，使应用开发者能够以更低成本构建产品。这种趋势可能重塑AI行业格局，使竞争焦点从基础模型转向应用层创新。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习的独特价值&lt;/strong&gt;：与传统监督学习依赖标注数据不同，强化学习通过奖励机制让模型自主学习策略。在推理任务中，强化学习使模型能够学会检查答案、分步思考等高级策略，这是单纯扩大模型规模难以实现的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法创新vs算力堆叠&lt;/strong&gt;：DeepSeek团队通过算法优化在H800 GPU上训练出高性能模型，证明AI进步不仅依赖算力堆叠。算法效率提升、训练方法优化同样是推动AI发展的重要路径，这为算力受限的团队提供了突破机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI代理的计算机交互能力&lt;/strong&gt;：Operator代表的Computer-Using Agent模型能够直接与网页元素交互，而非依赖API。这种能力使AI代理能够执行更广泛的任务，为AI应用打开了新的可能性空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;合成数据的选择性训练&lt;/strong&gt;：主动继承技术通过筛选合成数据的理想特性进行微调，能够在保持性能的同时减少模型毒性。这种方法解决了合成数据训练中的一个关键问题——如何控制模型输出质量。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.deeplearning.ai/the-batch/issue-286/"&gt;Reinforcement Learning Heats Up, White House Orders Muscular AI Policy, and more&amp;hellip;&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;DeepLearning.AI&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>