<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>强化学习 on Linguista</title><link>https://linguista.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 强化学习 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>Scott Wu谈收购Windsurf——过程、交易与理由</title><link>https://linguista.cn/rosetta/chat-notes/scott-wu-on-acquiring-windsurf/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/scott-wu-on-acquiring-windsurf/</guid><description>&lt;h1 id="scott-wu谈收购windsurf过程交易与理由"&gt;Scott Wu谈收购Windsurf——过程、交易与理由&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;Cognition CEO Scott Wu接受20VC主持人Harry Stebbings专访，详述72小时闪电收购Windsurf的决策过程与商业逻辑，并深入探讨AI Agents如何重塑软件工程、工程师角色转型、AI行业人才争夺、价值链分布以及强化学习作为技术发展隐形引擎的深远影响。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/Xz060_Kn0F0?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="Scott Wu谈收购Windsurf——过程、交易与理由"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;闪电收购——指Cognition在72小时内完成对Windsurf的并购，体现AI时代企业决策与执行的极致速度&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI Agents——能够独立接管软件开发全生命周期的智能代理，将工程师从编码中解放并推动角色向架构师和产品经理转型&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jevons悖论——效率提升反而导致资源消耗总量增加，此处指工程师效率提升10倍将催生10倍代码增长&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习(RL)——被Scott Wu视为AI发展的隐形引擎，是推动模型能力跃升的关键技术路径&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;差异化价值——AI产业链中价值并非固定归属于某一层，而是累积在能建立真正差异化优势的环节&lt;/strong&gt;：&lt;/p&gt;
&lt;h3 id="访谈介绍"&gt;访谈介绍&lt;/h3&gt;
&lt;p&gt;本访谈录记录了20VC主持人Harry Stebbings与Cognition公司CEO Scott Wu的深度对话。访谈的核心围绕Cognition近期完成的重磅交易——在72小时内成功收购Windsurf公司展开。Scott Wu详细回顾了这次快速收购的决策过程、背后的商业逻辑，以及双方在团队、产品与市场方面的互补性。&lt;/p&gt;
&lt;p&gt;对话并未止步于单一的商业事件。基于此次收购，访谈进一步延伸至对当前AI行业的宏观洞察。内容涵盖了AI领域激烈的人才竞争、价值在产业链各环节的分布与归属、基础模型与上层应用之间的竞合关系等核心议题。Scott Wu分享了他对AI技术发展趋势的看法，特别是对强化学习（RL）潜力的强调，以及他对行业估值泡沫论的反驳。&lt;/p&gt;
&lt;p&gt;此外，访谈还深入探讨了AI对软件工程未来的颠覆性影响，包括AI代理（Agents）如何重塑开发流程、工程师的角色将如何演变，以及Cognition对下一代人机交互界面的构想。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id="一cognition-72小时闪电收购windsurfai时代的速度与远见"&gt;一：Cognition 72小时闪电收购Windsurf：AI时代的速度与远见&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;
Scott Wu，Cognition的联合创始人兼CEO，在近期的一场访谈中，详细披露了其公司如何在短短72小时内完成对Windsurf的年度收购案。这宗交易不仅是AI领域的一大新闻，更折射出当前技术变革时代下，企业决策、价值判断与执行速度的新范式。本文将深入解析这场“闪电战”的幕后逻辑，以及它对AI行业并购所带来的启示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心要点一：机会的捕捉与价值的重估&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;突如其来的契机：&lt;/strong&gt; Scott Wu透露，Cognition与外界几乎同时在周五得知Windsurf的潜在拆分信息。迅速识别出Windsurf作为“宝藏”而非“空壳”的潜力，是Cognition能够迅速行动的关键。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google是否错失良机：&lt;/strong&gt; 访谈中提出Google是否“错失了金矿”的问题。Scott Wu的回答暗示，大型企业在组织重构时，确实可能忽略某些“被遗留”的资产的真实价值。他强调，Windsurf拥有出色的产品、完整的客户群、全部代码、专有IP以及一支强大的团队，这些都是其被Cognition看重的核心价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;互补性是核心驱动：&lt;/strong&gt; Cognition专注于核心工程与产品团队，而Windsurf则在市场推广、营销、财务和运营方面具备优势。这种天然的互补性，使得Scott Wu认为这是一次“非常自然的合作”，极大地加速了决策过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心要点二：AI时代下的极速并购逻辑&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>下半场——人工智能从解决问题转向定义问题</title><link>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</guid><description>&lt;h1 id="下半场人工智能从解决问题转向定义问题"&gt;下半场——人工智能从解决问题转向定义问题&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文提出人工智能正处于中场休息阶段。上半场的核心是开发新的训练方法和模型，而下半场的焦点将从解决问题转向定义问题，评估将变得比训练更重要。作者通过强化学习的视角，阐述了语言预训练、规模化和推理行动三大要素如何构成一套通用配方，使得RL终于具备了泛化能力，从而彻底改变了AI研究的游戏规则。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习泛化&lt;/strong&gt;：指RL通过语言预训练获得先验知识，结合推理行动机制，能够跨任务迁移而非局限于单一环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReAct范式&lt;/strong&gt;：将推理作为一种特殊行动加入RL环境的行动空间，使智能体在决策前进行语言化思考，从而提升泛化能力&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI评估转向&lt;/strong&gt;：下半场的核心命题不再是能否训练模型解决某个问题，而是应该训练AI做什么以及如何衡量真正的进展&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配方标准化&lt;/strong&gt;：大规模语言预训练加规模化加推理行动的组合已将基准提升过程工业化，使得针对特定任务的方法创新价值大幅降低&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;先验知识&lt;/strong&gt;：在RL三要素中，语言预训练提供的先验被证明可能比算法和环境更为关键，这颠覆了传统RL研究的优先级排序&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心观点：我们正处于人工智能的中场休息。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文连接：&lt;a href="https://ysymyth.github.io/The-Second-Half/"&gt;The Second Half&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;作者：&lt;a href="https://ysymyth.github.io/"&gt;Shunyu Yao&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;几十年来，人工智能的发展主要围绕着开发新的训练方法和模型。这确实取得了成效：从在国际象棋和围棋比赛中击败世界冠军，到在SAT和律师资格考试中超越大多数人类，再到赢得IMO和IOI金牌。在这些载入史册的里程碑——深蓝（DeepBlue）、AlphaGo、GPT-4以及o系列模型——背后，是人工智能方法的根本性创新：搜索、深度强化学习（deep RL）、规模化（scaling）和推理（reasoning）。随着时间的推移，一切都在变得更好。&lt;/p&gt;
&lt;p&gt;那么，现在突然发生了什么变化？&lt;/p&gt;
&lt;p&gt;三个词概括：强化学习（RL）终于奏效了。更准确地说：强化学习终于具备了泛化能力。在经历了数次重大的弯路和一系列里程碑式的积累之后，我们终于找到了一种行之有效的配方，能够利用语言和推理来解决各种各样的强化学习任务。哪怕就在一年前，如果你告诉大多数人工智能研究者，单一的配方就能处理软件工程、创意写作、IMO级别的数学、键鼠操作以及长篇问答——他们会嘲笑你在痴人说梦。这些任务中的每一项都极其困难，许多研究者整个博士生涯都只专注于其中一个狭窄的领域。&lt;/p&gt;
&lt;p&gt;然而，这确实发生了。&lt;/p&gt;
&lt;p&gt;那么接下来会发生什么？人工智能的下半场——从现在开始——将把焦点从&lt;strong&gt;解决问题&lt;/strong&gt;转向&lt;strong&gt;定义问题&lt;/strong&gt;。在这个新时代，&lt;strong&gt;评估&lt;/strong&gt;变得比&lt;strong&gt;训练&lt;/strong&gt;更重要。我们不再仅仅问：“我们能否训练一个模型来解决X问题？”，而是问：“我们应该训练人工智能去做什么？以及我们如何衡量真正的进展？” 要在下半场取得成功，我们需要及时转变思维模式和技能组合，也许更接近于一个产品经理。&lt;/p&gt;
&lt;h2 id="上半场"&gt;上半场&lt;/h2&gt;
&lt;p&gt;要理解上半场，看看它的赢家就知道了。你认为迄今为止最具影响力的人工智能论文是哪些？&lt;/p&gt;
&lt;p&gt;我试着回答了斯坦福224N课程里的这个问题，答案并不令人意外：Transformer、AlexNet、GPT-3等等。这些论文有什么共同点？它们提出了一些根本性的突破，用以训练出更好的模型。而且，它们都通过在某些基准（benchmarks）上展示出（显著的）改进而成功发表。&lt;/p&gt;
&lt;p&gt;不过，还有一个潜在的共同点：这些“赢家”都是&lt;strong&gt;训练方法或模型&lt;/strong&gt;，而不是&lt;strong&gt;基准或任务&lt;/strong&gt;。即使是 arguably 最具影响力的基准 ImageNet，其引用量也不到 AlexNet 的三分之一。方法与基准的对比在其他地方甚至更为悬殊——例如，Transformer 的主要基准是 WMT’14，其研讨会报告的引用量约为1300次，而 Transformer 的引用量超过了16万次。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/first_half.png" alt="上半场"&gt;&lt;/p&gt;
&lt;p&gt;这描绘了上半场的游戏规则：专注于构建新的模型和方法，而评估和基准是次要的（尽管对于论文发表体系的运作是必要的）。&lt;/p&gt;
&lt;p&gt;为什么会这样？一个重要原因是，在人工智能的上半场，&lt;strong&gt;方法比任务更难、更令人兴奋&lt;/strong&gt;。从零开始创建一个新的算法或模型架构——想想反向传播算法、卷积网络（AlexNet）或 GPT-3 中使用的 Transformer 等突破——需要非凡的洞察力和工程能力。相比之下，为人工智能定义任务通常感觉更直接：我们只需将人类已经在做的任务（如翻译、图像识别或下棋）转化为基准。这并不需要太多的洞察力，甚至工程量也不大。&lt;/p&gt;
&lt;p&gt;方法也往往比单个任务更通用、适用范围更广，这使得它们尤为宝贵。例如，Transformer 架构最终推动了计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）等许多领域的进步——远远超出了它最初证明自己的那个单一数据集（WMT’14 翻译）。一个优秀的、简洁且通用的新方法可以在许多不同的基准上取得进展（hillclimb），因此其影响往往超越单个任务。&lt;/p&gt;
&lt;p&gt;这场游戏已经持续了几十年，催生了改变世界的想法和突破，这些都体现在各个领域基准性能的不断提升上。那为什么游戏规则会发生改变呢？因为这些想法和突破的积累，在&lt;strong&gt;创造一个解决任务的有效配方&lt;/strong&gt;方面，产生了质的变化。&lt;/p&gt;
&lt;h2 id="配方"&gt;配方&lt;/h2&gt;
&lt;p&gt;这个配方是什么？不出所料，它的成分包括：大规模语言预训练、规模（数据和计算能力），以及推理和行动（reasoning and acting）的思想。这些听起来可能像是你在旧金山每天都能听到的流行词，但为什么称它们为“配方”？&lt;/p&gt;
&lt;p&gt;我们可以通过**强化学习（RL）**的视角来理解这一点。RL 常被认为是人工智能的“终局之战”——毕竟，理论上 RL 保证能赢得游戏，而经验上也难以想象任何超人系统（如 AlphaGo）没有 RL 的参与。&lt;/p&gt;
&lt;p&gt;在 RL 中，有三个关键组成部分：&lt;strong&gt;算法（algorithm）、环境（environment）和先验（priors）&lt;/strong&gt;。很长一段时间里，RL 研究者主要关注算法（例如 REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO……）——即智能体学习方式的智力核心——而将环境和先验视为固定的或最简化的。例如，Sutton 和 Barto 的经典教科书几乎全是关于算法，而很少涉及环境或先验。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/rl_book.png" alt="RL 教科书"&gt;&lt;/p&gt;
&lt;p&gt;然而，在深度强化学习时代，环境在经验上变得非常重要：一个算法的性能往往高度依赖于其开发和测试的环境。如果你忽略环境，你可能会构建出一个只在玩具环境中表现出色的“最优”算法。那么，为什么我们不先弄清楚我们真正想要解决的环境，然后再找到最适合该环境的算法呢？&lt;/p&gt;
&lt;p&gt;这正是 OpenAI 最初的计划。它构建了 &lt;a href="https://openai.com/index/openai-gym-beta/"&gt;gym&lt;/a&gt;，一个包含各种游戏的标准 RL 环境，然后是 &lt;a href="https://openai.com/index/universe/"&gt;World of Bits 和 Universe 项目&lt;/a&gt;，试图将互联网或计算机变成一个游戏。这计划听起来不错，对吧？一旦我们将所有数字世界都变成环境，用智能 RL 算法解决它，我们就拥有了数字通用人工智能（digital AGI）。&lt;/p&gt;</description></item><item><title>OpenAI的氛围编程到氛围研究：AI研究新范式</title><link>https://linguista.cn/curated/henrinotes-2025_p2/openai-vibe-coding-research-new-paradigm/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/openai-vibe-coding-research-new-paradigm/</guid><description>&lt;h1 id="openai的氛围编程到氛围研究ai研究新范式"&gt;OpenAI的氛围编程到氛围研究：AI研究新范式&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本期播客中，OpenAI首席科学家Jakub Pachocki与首席研究官Mark Chen与a16z深入对话，全面解析GPT-5的技术突破、AI自动化研究的愿景，以及在极高压力下推动前沿AI发展的实践经验。他们详细阐述了从传统的即时应答模型向具备深度推理能力的新范式的转变，分享了强化学习持续突破的经验，探讨了&amp;quot;氛围编程&amp;quot;如何重塑开发方式，并揭示了打造世界级AI研究团队的文化密码。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;对谈首先聚焦GPT-5的核心创新，其最大突破在于将&amp;quot;推理&amp;quot;能力引入主流AI产品，使模型能够针对复杂问题进行深度思考而非即时应答。这种变革标志着AI评测体系从传统基准测试转向更具经济相关性的指标，如自动发现新知识的能力。团队分享了数学和物理领域专家使用模型解决数月难题的真实案例，展现了AI在硬科学领域的突破性进展。&lt;/p&gt;
&lt;p&gt;关于AI自动化研究的未来蓝图，OpenAI的终极目标是打造能够自主进行实验、发现并推进新理论的&amp;quot;自动化研究者&amp;quot;。这一目标驱动着方法论从单一预训练向强化学习深度引导转型，重点突破&amp;quot;长远推理&amp;quot;与&amp;quot;记忆保持&amp;quot;两大核心能力。在编程领域，&amp;ldquo;氛围编程&amp;quot;已成为新一代开发者的默认方式，AI根据问题难度自适应响应，快速搭建原型而非手动实现每一行代码。&lt;/p&gt;
&lt;p&gt;在研究团队建设方面，两位负责人强调了&amp;quot;氛围研究&amp;quot;精神的重要性，核心包括坚定信念、持续追问和对失败的容忍。他们分享了如何发现和培养&amp;quot;洞穴探险者&amp;quot;型天才，如何在产品需求与基础研究之间保持平衡，以及如何动态分配算力资源以在前沿突破和产品化应用之间找到最佳平衡点。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;自动化研究者&lt;/strong&gt;：OpenAI的终极愿景，指能够自主进行科学研究、设计实验、发现新理论并推进技术边界的AI系统。这要求AI具备长远推理、记忆保持和自我修正能力，不仅能在数学、编程竞赛中达到专业水平，更要迈向自主发现新领域的阶段。当前GPT-5已在硬科学领域展现出自动生成新知识结构的初步能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;氛围编程&lt;/strong&gt;：新一代开发者借助AI自动生成代码与建议、快速搭建原型的主流工作方式。与传统的逐行手动实现不同，氛围编程强调利用AI的智能来加速开发流程，模型会根据问题难度自动分配计算资源和等待时间，在简单问题快速响应与复杂问题深度思考之间找到最佳平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习持续突破&lt;/strong&gt;：尽管业界曾担心强化学习会面临泛化不足、模态坍塌等瓶颈，但OpenAI凭借丰富语言环境下的持续实践屡次打破这些预期。奖励建模正从手工微调向更接近人类学习的自动化范式演化，显著降低了使用门槛。未来范式将更加注重人类样本效率和直观易用的奖励机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;长远推理与记忆保持&lt;/strong&gt;：下一代AI的核心能力，要求模型能够持续操作数小时乃至更久的任务，在多步骤规划与调整决策的复杂场景下保持稳定高效。这涉及在&amp;quot;稳定性&amp;quot;与&amp;quot;深度&amp;quot;之间的精细权衡，步骤越多后续推理精度可能下降，但单一步骤又难以突破自主创造的边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;洞穴探险者型研究者&lt;/strong&gt;：OpenAI青睐的人才类型，他们可能不是社交媒体活跃分子或频繁发表论文者，但善于独立解决极难问题，倾向于攻克&amp;quot;常人不认为可解&amp;quot;的挑战。这类研究者往往具备跨学科背景，能够在物理、金融、计算机科学等领域间建立创新连接。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=KSgPNVmZ8jQ"&gt;From Vibe Coding to Vibe Researching OpenAI&amp;rsquo;s Mark Chen and Jakub Pachocki&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;a16z（对谈嘉宾：Jakub Pachocki、Mark Chen，主持人：Anjney Midha、Sarah Wang）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;YouTube a16z播客&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>DeepSeek-R1如何用强化学习激发大模型推理能力</title><link>https://linguista.cn/curated/henrinotes-2025_p2/deepseek-r1-reinforcement-learning-reasoning/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/deepseek-r1-reinforcement-learning-reasoning/</guid><description>&lt;h1 id="deepseek-r1如何用强化学习激发大模型推理能力"&gt;DeepSeek-R1如何用强化学习激发大模型推理能力&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;DeepSeek-R1项目通过强化学习（Reinforcement Learning, RL）在极少人类标注、无需人工推理示范的前提下，让大规模语言模型（LLMs）自主进化出复杂的推理能力。其强化学习机制不仅使模型在数学、代码、STEM等可验证领域远超传统有监督学习模型，还促使模型形成自反思、验证与动态策略适应等高级推理行为。该框架揭示，大模型原生具备高度可塑的推理潜力，其能力关键在于难题驱动、验证器和充足算力支撑下的持续自我优化。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先探讨了大模型推理能力的瓶颈与强化学习的动机。传统LLM虽然能在大规模参数与高效数据驱动下涌现出数学、逻辑推理及代码等能力，但这些能力的进一步提升受两大主要限制：大规模人类标注高度依赖人力，导致成本和规模受限，并会引入人为思维路径的固有限制，抑制模型自发探索超越人类的策略。&lt;/p&gt;
&lt;p&gt;DeepSeek-R1提出了用强化学习挖掘模型自进化能力的方案——仅以最终答案与正确性为信号，完全不限定中间推理过程，让模型自主发展推理路径。实验基于DeepSeek-V3 Base，利用GRPO（群组相对策略优化）作为核心RL算法，只用最终答案的正确与否作为奖励信号。&lt;/p&gt;
&lt;p&gt;在强化学习训练过程中，模型逐步学会生成更长、更严谨的推理过程，内容包含自我验证、反思、方案再试验等。此进化完全非人类指导，模型通过与&amp;quot;地面真值&amp;quot;对比，自主修正推理方式。训练初期AIME 2024竞赛成绩仅15.6%，RL收敛后提升至77.9%，用自洽推理采样更可达86.7%，远超所有人类参赛平均分。&lt;/p&gt;
&lt;p&gt;DeepSeek-R1在Zero基础上，由于原生模型会出现中英文混杂、文风难读等实际问题，因此在多轮RL基础上融入部分人类标注初始样本，结合RL与有监督微调，并引入拒绝采样和&amp;quot;奖励模型&amp;quot;用于对齐人类偏好和安全性。训练流程包括冷启动数据强化、多阶段RL、拒绝采样与SFT以及最终偏好对齐等多个阶段。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GRPO（群组相对策略优化）&lt;/strong&gt;：这是DeepSeek-R1使用的核心强化学习算法。它通过群组采样—多路输出对齐机制，鼓励模型尝试更多解题方案，自动发展如反思验证、备用策略生成等能力。相比传统策略优化方法，GRPO能够在无需人工标注中间推理步骤的情况下，仅依靠最终答案的正确性来引导模型优化其推理策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自反思（Self-Reflection）&lt;/strong&gt;：模型在推理过程中生成自我检查（如&amp;quot;wait&amp;quot;），反复迭代以修正潜在错误，并非简单线性输出。这一过程源于奖励信号的诱导，而非人工预设。在RL训练中，模型逐步学会生成更长、更严谨的推理过程，包含自我验证、反思、方案再试验等高级元认知行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;动态策略适应（Dynamic Strategy Adaptation）&lt;/strong&gt;：面对不同问题，模型能灵活分配计算资源——易题给少量Token、难题就大规模生成长链推理。模型会探索多路径、取自一致答案以自动校验正确性。这种能力不是人工设计的，而是通过强化学习自然涌现的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多阶段训练流水线&lt;/strong&gt;：DeepSeek-R1采用结构化分解的训练流程，包括极简模板加RL以最大释放模型原生推理潜力、少量人工&amp;quot;冷启动&amp;quot;矫正输出风格、奖励模型引导对齐人类偏好和安全防控。这种分层设计既保证了推理能力的深度发展，又确保了输出质量和可用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;奖励黑客问题（Reward Hacking）&lt;/strong&gt;：这是强化学习面临的重大挑战。部分场景难设计可靠奖励函数，例如写作类输出很难自动判优劣，传统模型奖励很容易被策略模型&amp;quot;钻空子&amp;quot;，导致奖励信号被攻击而训练失效。DeepSeek-R1团队认为需要构建更智能泛化奖励模型与RL环境来应对这一问题。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;DeepSeek-AI团队（代表作者：Daya Guo等）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>模型为何思考</title><link>https://linguista.cn/rosetta/technology/why-models-think/</link><pubDate>Thu, 01 May 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/why-models-think/</guid><description>&lt;h1 id="模型为何思考"&gt;模型为何思考&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统回顾了大语言模型在推理时如何通过额外计算提升性能的最新进展。从心理学双系统理论类比出发，探讨了思维链prompting、并行采样、序列修订等关键技术，分析了将计算视为资源和潜在变量建模两种理论视角，并梳理了从早期CoT到o1、R1等推理模型的发展脉络。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理时计算（Test-time Compute）&lt;/strong&gt;：指模型在推理阶段而非训练阶段投入额外计算资源，通过更多的思考步骤来提升输出质量&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维链（Chain-of-Thought）&lt;/strong&gt;：让模型在给出最终答案前生成中间推理步骤的方法，类似人类逐步解题的过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;并行采样（Parallel Sampling）&lt;/strong&gt;：同时生成多个候选输出，再通过评分函数或多数投票选择最优结果的解码策略&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;序列修订（Sequential Revision）&lt;/strong&gt;：让模型对已有输出进行反思和迭代修正，逐步改进响应质量的方法&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;潜在变量建模（Latent Variable Modeling）&lt;/strong&gt;：将思考过程视为隐藏变量，通过边缘化所有可能的推理路径来建模最终答案分布的概率框架&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文链接：&lt;a href="https://lilianweng.github.io/posts/2025-05-01-thinking/"&gt;Why We Think&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;日期：2025年5月1日&lt;/li&gt;
&lt;li&gt;预计阅读时间：40分钟&lt;/li&gt;
&lt;li&gt;作者：Lilian Weng&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特别感谢 &lt;a href="https://scholar.google.com/citations?user=itSa94cAAAAJ&amp;amp;hl=en"&gt;John Schulman&lt;/a&gt; 为本文提供了大量极其宝贵的反馈和直接编辑。&lt;/p&gt;
&lt;p&gt;推理时计算 (&lt;a href="https://arxiv.org/abs/1603.08983"&gt;Graves et al. 2016&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/1705.04146"&gt;Ling, et al. 2017&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/2110.14168"&gt;Cobbe et al. 2021&lt;/a&gt;) 和思维链 (CoT) (&lt;a href="https://arxiv.org/abs/2201.11903"&gt;Wei et al. 2022&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/2112.00114"&gt;Nye et al. 2021&lt;/a&gt;) 显著提升了模型性能，同时也引发了许多研究问题。本文旨在回顾关于如何有效利用推理时计算（即“思考时间”）及其益处的最新进展。&lt;/p&gt;
&lt;h1 id="动机"&gt;动机&lt;/h1&gt;
&lt;p&gt;让模型进行更长时间的思考，可以从几个不同方面进行阐述。&lt;/p&gt;
&lt;h2 id="心理学类比"&gt;心理学类比&lt;/h2&gt;
&lt;p&gt;核心思想与人类的思考方式息息相关。我们人类无法立即给出&lt;code&gt;“12345 乘以 56789 是多少？”&lt;/code&gt;这个问题的答案。相反，对于复杂问题，我们自然会花时间思考和分析，然后才能得出结果。在 &lt;a href="https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555"&gt;《思考，快与慢》(Kahneman, 2013)&lt;/a&gt; 一书中，丹尼尔·卡尼曼 (Daniel Kahneman) 借助 &lt;a href="https://en.wikipedia.org/wiki/Dual_process_theory"&gt;双系统理论&lt;/a&gt; 的视角，将人类思维划分为两种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;快速思维（系统 1）&lt;/em&gt; 运作迅速且自动化，由直觉和情感驱动，几乎不需要努力。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;慢速思维（系统 2）&lt;/em&gt; 需要深思熟虑、逻辑推理和显著的认知努力。这种思维模式消耗更多的脑力，需要有意识的参与。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于系统 1 思维快速且容易，它常常成为主要的决策驱动因素，但代价是牺牲了准确性和逻辑性。它自然地依赖我们大脑的思维捷径（即启发式），并可能导致错误和偏见。通过有意识地放慢速度，花更多时间反思、改进和分析，我们可以运用系统 2 思维来挑战我们的直觉，做出更理性的选择。&lt;/p&gt;
&lt;h2 id="计算作为一种资源"&gt;计算作为一种资源&lt;/h2&gt;
&lt;p&gt;深度学习的一种观点认为，神经网络可以根据其在一次前向传播中可访问的计算量和存储量来表征；如果我们使用梯度下降优化它们来解决问题，优化过程将找出如何利用这些资源——它们将弄清楚如何将这些资源组织成用于计算和信息存储的电路。从这个角度来看，如果我们设计一个能够在推理时进行更多计算的架构或系统，并训练它有效利用这一资源，它将表现得更好。&lt;/p&gt;</description></item><item><title>理解推理型大型语言模型的构建与优化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</guid><description>&lt;h1 id="理解推理型大型语言模型的构建与优化"&gt;理解推理型大型语言模型的构建与优化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文由AI专家Sebastian Raschka撰写，系统介绍了推理型大型语言模型的核心概念与构建方法。文章详细分析了推理模型的定义、优势与劣势，并以DeepSeek R1为例，阐述了纯强化学习、监督微调、模型蒸馏等四种主要的训练优化方法。作者还探讨了低预算下开发推理模型的可行性，为研究者和开发者提供了实用的技术路线参考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从LLM领域的发展趋势切入，指出2024年以来专门化应用方向的快速发展。推理模型作为一类能够通过多步中间步骤解决复杂任务的特殊模型，在数学证明、逻辑谜题和高级编程等场景中具有重要价值，但在简单任务中可能因&amp;quot;过度思考&amp;quot;而降低效率。&lt;/p&gt;
&lt;p&gt;DeepSeek R1系列模型作为典型案例，展示了三种不同的训练范式：R1-Zero采用纯强化学习，无需监督微调即可自动生成推理步骤；R1结合了监督微调与强化学习，引入一致性奖励机制；R1-Distill则通过模型蒸馏技术，将大型模型的推理能力迁移到较小的模型中。&lt;/p&gt;
&lt;p&gt;构建推理模型的四种主要方法各具特色：推理时扩展通过增加计算资源提高性能；纯强化学习展示了无需监督数据的可能性；监督微调结合强化学习能够充分发挥两者优势；模型蒸馏则在效率和成本方面具有显著优势。对于资源有限的开发者，Sky-T1和TinyZero等项目证明了低预算开发推理模型的可行性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理模型&lt;/strong&gt;：指需要通过复杂、多步生成中间步骤来回答问题的语言模型。这类模型能够解决需要逻辑推理的任务，如数学计算、编程挑战等，但在简单任务中可能导致效率低下和成本增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纯强化学习&lt;/strong&gt;：DeepSeek R1-Zero证明了推理能力可以通过纯强化学习而无需监督微调来实现。模型通过准确性和格式奖励自动生成推理步骤，这为理解AI推理能力的本质提供了新的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型蒸馏&lt;/strong&gt;：通过将大型模型生成的推理数据用于训练较小的模型，以提高推理能力。这种方法在效率和成本方面具有优势，使得资源有限的团队也能开发出性能可观的推理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推理时扩展&lt;/strong&gt;：通过增加推理时的计算资源来提高模型性能，包括链式思考提示、投票和搜索策略等。这种方法不需要重新训练模型，但会增加推理时间和成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;旅程学习&lt;/strong&gt;：一种新的训练策略，通过在训练数据中包含错误的解决方案路径，让模型从错误中学习。这种方法可能在低预算下开发推理模型时具有优势。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms"&gt;Understanding Reasoning LLMs&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Sebastian Raschka, PhD&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Andrej Karpathy 盛赞 DeepSeek R1 强化学习推动 AI 模型进化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</guid><description>&lt;h1 id="andrej-karpathy-盛赞-deepseek-r1强化学习推动-ai-模型进化"&gt;Andrej Karpathy 盛赞 DeepSeek R1：强化学习推动 AI 模型进化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于 Andrej Karpathy 的最新视频内容，详细介绍了 DeepSeek R1 在强化学习方面的技术创新。Karpathy 指出，DeepSeek R1 通过强化学习微调，在解决数学问题等方面表现出色，并且能够发现人类思考问题的逻辑和策略。文章还探讨了强化学习与人类反馈（RLHF）的优势与局限性，并展望了大语言模型在多模态能力和智能体方面的未来发展。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;Andrej Karpathy 作为 OpenAI 早期成员、前特斯拉 AI 总监，在其最新视频中对 DeepSeek R1 给予了高度评价。他回顾了从神经网络起源到最新大模型的发展历程，特别强调了 DeepSeek R1 在强化学习（RL）方面的技术创新。DeepSeek R1 是一个通过强化学习微调的大语言模型，其性能与 OpenAI 的模型不相上下，在解决数学问题时表现出色。&lt;/p&gt;
&lt;p&gt;Karpathy 认为，强化学习是大语言模型训练中的新兴阶段，目前仍处于起步状态。强化学习的优势在于能够突破人类的限制，发现人类未曾意识到的策略，例如 AlphaGo 在围棋中发明的创新走法。然而，强化学习也存在挑战，它非常擅长发现&amp;quot;欺骗&amp;quot;模型的方法，这可能会阻碍其在某些领域的应用。&lt;/p&gt;
&lt;p&gt;关于强化学习与人类反馈，Karpathy 指出，从人类反馈中进行强化学习（RLHF）能够提升模型性能，尤其是在那些无法验证的领域，如创意写作等。但 RLHF 的主要问题是基于人类反馈的强化学习可能会产生误导，因为人类反馈是一个有损模拟，无法完美反映真实人类的判断。&lt;/p&gt;
&lt;p&gt;展望未来，Karpathy 预测未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现多模态能力。此外，大语言模型将能够执行长期任务，人类将成为这些智能体任务的监督者。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：强化学习是大语言模型训练中的新兴阶段，通过奖励机制优化模型行为。DeepSeek R1 通过强化学习微调，在解决数学问题时表现出色，并且能够发现人类思考问题的逻辑和策略，如&amp;quot;思维链&amp;quot;（CoT）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLHF&lt;/strong&gt;：从人类反馈中进行强化学习能够提升模型性能，尤其是在那些无法验证的领域。人类标注者可以通过简单的排序任务来提供高质量的反馈，但人类反馈是一个有损模拟，无法完美反映真实人类的判断，可能会导致误导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多模态能力&lt;/strong&gt;：未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现真正的多模态能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/thTwdVgc4lfYRj6WWpKBwA"&gt;Andrej Karpathy 最新视频盛赞 DeepSeek：R1 正在发现人类思考的逻辑并进行复现&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;郑佳美&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年02月06日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>以DeepSeek R1为例学习推理型大语言模型</title><link>https://linguista.cn/curated/henrinotes-2025-p1/reasoning-llm-deepseek-r1-training-methods/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/reasoning-llm-deepseek-r1-training-methods/</guid><description>&lt;h1 id="以deepseek-r1为例学习推理型大语言模型"&gt;以DeepSeek R1为例学习推理型大语言模型&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文以DeepSeek R1为核心案例，系统梳理了推理型大语言模型的概念、适用场景与优劣势。文章详细解读了DeepSeek R1三个版本（R1-Zero、R1、R1-Distill）的训练流程，归纳了构建推理模型的四种主要方法，并介绍了Sky-T1和TinyZero等低成本推理模型的探索实践，为研究者和开发者理解与构建推理模型提供了全面指引。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从推理型大语言模型的定义出发，指出其核心特征是能够通过多步推理回答复杂问题，区别于常规LLM在于包含中间推理步骤和&amp;quot;思考&amp;quot;过程。作者首先分析了推理模型的适用场景——主要针对解谜、高级数学和编程挑战等需要复杂中间推理的任务，同时也坦诚指出其资源消耗大、输出冗长和&amp;quot;过度思考&amp;quot;等局限。&lt;/p&gt;
&lt;p&gt;文章的核心内容围绕DeepSeek R1的训练流程展开，详细介绍了三个版本的演进路径：R1-Zero通过纯强化学习涌现推理能力，R1在此基础上叠加监督微调和多轮RL训练以获得更强性能，R1-Distill则通过蒸馏将能力迁移到更小模型。这一递进式的技术路线清晰展示了推理模型的构建逻辑。&lt;/p&gt;
&lt;p&gt;在方法论层面，文章系统归纳了推理阶段扩展、纯强化学习、监督微调+强化学习、纯监督微调与蒸馏四种主要构建路径，并对比了各自的适用场景和优劣。此外，文章还介绍了Sky-T1（450美元训练成本）和TinyZero（不到30美元）等低成本方案，展示了推理模型平民化的可能性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理型大语言模型（Reasoning LLMs）&lt;/strong&gt;：指能够通过多步推理回答复杂问题的模型，与常规LLM的关键区别在于包含中间推理步骤，展现出&amp;quot;思考&amp;quot;过程。这类模型主要适用于解谜、数学证明、代码生成等需要复杂推理链的任务，而对摘要、翻译等简单任务可能并无优势，甚至因&amp;quot;过度思考&amp;quot;而产生错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeepSeek R1的三阶段训练流程&lt;/strong&gt;：R1-Zero基于671B参数的DeepSeek-V3通过纯RL训练，使用准确性奖励和格式奖励使模型涌现推理能力；R1在此基础上增加SFT和多轮RL，引入语言一致性奖励，性能显著提升；R1-Distill则利用R1生成的SFT数据微调Llama和Qwen等小模型，以更低成本获得推理能力。这三个版本体现了从探索到优化再到普及的技术路线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四种推理模型构建方法&lt;/strong&gt;：推理阶段扩展通过增加推理时计算资源提升输出质量，无需额外训练但增加推理成本；纯RL可涌现推理能力但性能有限，更适合理论研究；SFT+RL是性能最强的方案，DeepSeek R1即为典型代表；SFT+蒸馏适用于资源有限场景，通过大模型数据微调小模型实现能力迁移，但无法达到最前沿性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习中的奖励机制设计&lt;/strong&gt;：DeepSeek R1训练中采用了准确性奖励（如LeetCode编译器判定编程答案正确性）和格式奖励（LLM评估器判断回答格式），以及语言一致性奖励（防止模型在回答中切换语言）。这种多维度奖励设计是推理能力涌现和提升的关键，展示了RL在LLM训练中的精细化应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;低成本推理模型的可行性&lt;/strong&gt;：Sky-T1仅用1.7万条SFT数据和450美元训练出性能与o1相当的32B模型，TinyZero用不到30美元通过纯RL训练出3B参数的推理模型并展现出自我验证能力。这些探索表明，推理模型的构建门槛正在快速降低，为资源有限的研究者开辟了新路径。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/DG8-bENYNji8qg4SwexEmg"&gt;以DeepSeek R1为例学习推理型大语言模型&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;—&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-02-08&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>强化学习升温 白宫发布强硬AI政策及其他动态</title><link>https://linguista.cn/curated/henrinotes-2025_p2/reinforcement-learning-white-house-ai-policy/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/reinforcement-learning-white-house-ai-policy/</guid><description>&lt;h1 id="强化学习升温-白宫发布强硬ai政策及其他动态"&gt;强化学习升温 白宫发布强硬AI政策及其他动态&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文综合报道了AI领域的最新重要动态。DeepSeek-R1的开源发布标志着中国在生成式AI领域的显著进展，其性能媲美OpenAI o1但成本仅1/30。强化学习技术正成为提升大型语言模型推理能力的关键路径。OpenAI推出Operator代理进入消费市场，白宫签署AI行政令减少监管限制，合成数据微调技术取得新突破。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DeepSeek事件的开源冲击&lt;/strong&gt;：中国在生成式AI领域迅速追赶美国，DeepSeek-R1以开源形式发布，性能与OpenAI o1相当但成本仅为1/30。这标志着开源权重模型正在商品化基础模型层，降低了应用开发门槛。更重要的是，DeepSeek团队通过算法优化在较低性能的H800 GPU上训练出高性能模型，证明算力并非AI进步的唯一途径，算法创新同样关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习的崛起&lt;/strong&gt;：强化学习通过奖励或惩罚模型特定行为来训练，与监督学习不同，它不依赖于已知的&amp;quot;真实值&amp;quot;。DeepSeek-R1和Kimi k1.5都采用强化学习提升推理能力，使模型学会检查答案、优化输出长度等策略。强化学习在解决复杂数学和编程任务时展现出巨大潜力，正成为训练大型语言模型的重要技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI代理进入消费市场&lt;/strong&gt;：OpenAI推出Operator，允许用户通过ChatGPT执行购物、购票等简单网络任务。Operator基于新的Computer-Using Agent模型，在WebVoyager和OSWorld基准测试中表现出色。这标志着AI代理开始进入消费市场，可能成为下一代产品的模板。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;政策环境变化&lt;/strong&gt;：特朗普总统签署AI行政令，要求180天内制定AI行动计划。政策要点包括撤销阻碍AI发展的监管、要求AI系统&amp;quot;无意识形态偏见&amp;quot;、优先支持符合国家竞争力的AI公司。新政策减少了AI开发的官僚监管，为创新提供更宽松环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;合成数据技术进展&lt;/strong&gt;：Cohere团队提出&amp;quot;主动继承&amp;quot;技术，通过选择理想特性的合成数据进行微调。实验表明，选择最低毒性的合成数据可显著降低模型毒性，同时保持或提升某些基准测试性能。这为合成数据训练提供了新方法，帮助模型减少负面特性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;开源模型商品化&lt;/strong&gt;：DeepSeek-R1的开源发布标志着基础模型层正在被商品化。开源权重降低了使用门槛，使应用开发者能够以更低成本构建产品。这种趋势可能重塑AI行业格局，使竞争焦点从基础模型转向应用层创新。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;强化学习的独特价值&lt;/strong&gt;：与传统监督学习依赖标注数据不同，强化学习通过奖励机制让模型自主学习策略。在推理任务中，强化学习使模型能够学会检查答案、分步思考等高级策略，这是单纯扩大模型规模难以实现的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算法创新vs算力堆叠&lt;/strong&gt;：DeepSeek团队通过算法优化在H800 GPU上训练出高性能模型，证明AI进步不仅依赖算力堆叠。算法效率提升、训练方法优化同样是推动AI发展的重要路径，这为算力受限的团队提供了突破机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI代理的计算机交互能力&lt;/strong&gt;：Operator代表的Computer-Using Agent模型能够直接与网页元素交互，而非依赖API。这种能力使AI代理能够执行更广泛的任务，为AI应用打开了新的可能性空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;合成数据的选择性训练&lt;/strong&gt;：主动继承技术通过筛选合成数据的理想特性进行微调，能够在保持性能的同时减少模型毒性。这种方法解决了合成数据训练中的一个关键问题——如何控制模型输出质量。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.deeplearning.ai/the-batch/issue-286/"&gt;Reinforcement Learning Heats Up, White House Orders Muscular AI Policy, and more&amp;hellip;&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;DeepLearning.AI&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>DeepSeek——中国AI初创企业的崛起与全球影响</title><link>https://linguista.cn/rosetta/chat-notes/deepseek-china-ai-startup-rise-and-global-impact/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/deepseek-china-ai-startup-rise-and-global-impact/</guid><description>&lt;h1 id="deepseek中国ai初创企业的崛起与全球影响"&gt;DeepSeek——中国AI初创企业的崛起与全球影响&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;DeepSeek是中国幻方量化旗下的AI初创团队，以极低成本训练出性能比肩OpenAI的大语言模型V3和推理模型R1，其MoE架构、MLA注意力机制和GRPO强化学习算法等工程创新引发全球关注，对闭源与开源AI公司、GPU市场及创业生态产生深远影响，同时也面临数据蒸馏、成本透明度和安全性等争议。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/b_OpjUz7zN8?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="DeepSeek——中国AI初创企业的崛起与全球影响"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MoE混合专家结构&lt;/strong&gt;：将输入数据动态分配给不同专家子网络处理，仅激活部分参数，从而大幅提升计算效率并降低训练与推理成本&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MLA多头潜在注意力机制&lt;/strong&gt;：通过压缩KV缓存减少内存占用，优化Transformer架构在大规模参数下的推理效率&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GRPO群体相对策略优化&lt;/strong&gt;：一种无需人类反馈的强化学习算法，去掉传统PPO中的Critic模块，让模型通过自我对弈直接优化策略，显著降低训练开销&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据蒸馏&lt;/strong&gt;：通过调用大模型API获取输出结果，再用这些结果训练小模型的技术手段，在AI行业中存在知识产权争议&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;开源AI生态&lt;/strong&gt;：DeepSeek将模型权重和技术细节完全公开，降低了AI技术的使用门槛，推动更多开发者和企业参与AI应用创新&lt;/p&gt;
&lt;p&gt;DeepSeek：中国AI初创企业的崛起与全球影响&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原文标题：硅谷视角深聊：DeepSeek的颠覆、冲击、争议和误解 - YouTube&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;链接： &lt;a href="https://www.youtube.com/watch?v=b_OpjUz7zN8&amp;amp;list=WL"&gt;https://www.youtube.com/watch?v=b_OpjUz7zN8&amp;list=WL&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文章类别&lt;/strong&gt;：博客/视频文稿&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;内容整理&lt;/strong&gt;：&lt;/p&gt;
&lt;h3 id="文章框架"&gt;文章框架&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 引言：DeepSeek的崛起与全球关注
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── DeepSeek的技术创新
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 模型架构创新：MoE和MLA
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 训练方法创新：GRPO算法
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 推理模型优化：R1-Zero和R1
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── DeepSeek对全球AI行业的影响
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 对闭源AI公司的冲击：OpenAI和Anthropic
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 对开源AI公司的冲击：Meta等
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 对GPU计算市场的影响：Nvidia
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 对AI创业生态的影响
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── DeepSeek的争议与误解
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 数据蒸馏与模型抄袭争议
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 成本估算的准确性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 安全性能问题
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── DeepSeek的商业模式与未来
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 与Magic Square的关系
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 商业化与盈利模式
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└── 结语：DeepSeek的未来展望
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="文章标签"&gt;文章标签&lt;/h3&gt;
&lt;p&gt;#DeepSeek ， #AI创新 ， #全球影响 ， #技术争议&lt;/p&gt;</description></item><item><title>强化学习基础与贝尔曼方程详解</title><link>https://linguista.cn/curated/henrinotes_2025_p3/reinforcement-learning-bellman-equation-fundamentals/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/reinforcement-learning-bellman-equation-fundamentals/</guid><description>&lt;h1 id="强化学习基础与贝尔曼方程详解"&gt;强化学习基础与贝尔曼方程详解&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于西湖大学赵世钰老师《强化学习的数学原理》课程，系统性地介绍了强化学习的核心概念体系。从策略、奖励、回报等基本要素出发，逐步构建马尔可夫决策过程的理论框架，重点阐述了贝尔曼方程的数学原理及其在状态值和动作值计算中的应用，为深入理解强化学习提供了清晰的数学基础。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先建立了强化学习的基本概念体系，通过具体示例说明了策略的概率分布特性、奖励的数值含义以及回报的累积计算方式。作者强调，奖励是依赖于当前状态和动作的标量值，可正可负，而回报则是沿轨迹收集的所有奖励之和，用于评估策略优劣。文章还引入了Episode的概念，将完整轨迹定义为一次回合。&lt;/p&gt;
&lt;p&gt;在理论基础部分，文章详细解析了马尔可夫决策过程的三大核心要素：马尔可夫属性的无记忆性特征、决策过程的策略定义、以及涉及状态转移概率和奖励概率的完整过程描述。这部分内容为后续的贝尔曼方程推导奠定了数学基础。&lt;/p&gt;
&lt;p&gt;文章的核心内容围绕贝尔曼方程展开，介绍了理查德·贝尔曼在1950年代提出的最优性原理。作者通过状态价值函数和动作价值函数两种形式，详细阐述了如何通过未来可能价值来计算当前价值。文章特别强调了回报与状态值的本质区别：回报针对单个轨迹，而状态值是对多个轨迹求回报后的平均值。&lt;/p&gt;
&lt;p&gt;在应用层面，文章讲解了动作值的定义和计算方法，阐明了状态值与动作值之间的数学关系，即状态值等于不同动作对应的动作值的加权平均。最后通过Bellman最优性方程，引入了最优策略的存在性和唯一性证明，完整构建了从基础概念到高级理论的强化学习知识体系。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;策略（Policy）&lt;/strong&gt;：策略定义了智能体在给定状态下应采取各种动作的概率分布。文章通过表格示例展示了在S1状态下执行a2、a3动作各为0.5的概率配置，并提供了相应的编程实现代码。策略的核心作用是将状态映射到动作空间，是强化学习中决策机制的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bootstrap概念&lt;/strong&gt;：这是贝尔曼方程的核心思想，描述了所有状态之间值的相互依赖关系。通过bootstrap，当前状态的价值可以通过未来状态的价值来估计，这种递归性质使得强化学习能够通过迭代计算逐步优化策略。文章在策略评价和最优值计算中都强调了这一概念的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;状态值与动作值的关系&lt;/strong&gt;：状态值V(s)表示从某个状态出发的期望回报，而动作值Q(s,a)则表示从某个状态执行特定动作的期望回报。两者的关键关系在于：状态值等于该状态下所有可能动作的动作值按策略概率加权平均。这一关系为策略改进和最优策略求解提供了数学基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最优性原理&lt;/strong&gt;：贝尔曼提出的这一原理指出，如果一个策略在每个子问题上都是最优的，那么它对整个问题就是全局最优的。这一原理不仅具有深刻的数学美感，更为动态规划和强化学习算法提供了理论支撑，是理解最优策略存在性和唯一性的关键。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/I1TjXMwSInlW7CFe1SXlGw"&gt;强化学习：概念和贝尔曼方程&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;赵世钰（西湖大学）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;《强化学习的数学原理》课程整理&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>