<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>彩票假说 on Linguista</title><link>https://linguista.cn/tags/%E5%BD%A9%E7%A5%A8%E5%81%87%E8%AF%B4/</link><description>Recent content in 彩票假说 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 18 Aug 2025 08:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%BD%A9%E7%A5%A8%E5%81%87%E8%AF%B4/index.xml" rel="self" type="application/rss+xml"/><item><title>AI研究人员如何意外发现他们对学习的固有认知是错误的</title><link>https://linguista.cn/info/htmlcards/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</link><pubDate>Mon, 18 Aug 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</guid><description>本文深入探讨了机器学习领域关于“偏差-方差权衡”的铁律如何被2019年的“双重下降”现象打破。文章阐述了研究人员如何通过扩大模型规模，意外发现大型神经网络不仅能避免过拟合，反而能涌现出惊人的新能力。这一发现挑战了三百年的统计理论，引发了以 OpenAI 为首的“越大越好”行业变革，并最终由“彩票假说”从数学上解释了为何海量冗余参数是通向简单高效解决方案的必经之路。</description></item><item><title>AI研究人员如何意外发现他们对学习的固有认知是错误的</title><link>https://linguista.cn/rosetta/technology/lottery-ticket-hypothesis-why-large-neural-networks-work/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/lottery-ticket-hypothesis-why-large-neural-networks-work/</guid><description>&lt;h1 id="ai研究人员如何意外发现他们对学习的固有认知是错误的"&gt;AI研究人员如何意外发现他们对学习的固有认知是错误的&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;传统机器学习理论认为过大的模型必然导致过拟合，但研究人员发现大规模神经网络反而表现更优，出现了双重下降现象。彩票假说为此提供了解释——大型网络的成功并非源于学习复杂解决方案，而是因为拥有更多随机初始化的子网络，从而大幅提升找到简单优雅解决方案的概率。这一发现调和了经典理论与实践之间的矛盾，也重新定义了我们对智能本身的理解。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;偏差-方差权衡&lt;/strong&gt;：统计学习中的经典原则，指模型过于简单会欠拟合、过于复杂会过拟合，需在两者之间寻找最优平衡点，该理论有三百年数学基础支撑&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双重下降&lt;/strong&gt;：一种违反传统U型误差曲线的现象，模型在过拟合后继续扩大规模，测试误差会先升后降，出现第二次性能提升，由Mikhail Belkin等人记录发现&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票假说&lt;/strong&gt;：由MIT的Jonathan Frankle和Michael Carbin提出，认为大型神经网络中存在能匹配整体性能的微小子网络，大网络的优势在于提供了更多随机初始化的彩票，使找到最优简单解的概率大幅增加&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;过参数化&lt;/strong&gt;：指模型参数数量远超训练数据所需的现象，传统理论认为这必然导致记忆而非学习，但实践证明过参数化反而有助于网络搜索简单解决方案&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经网络剪枝&lt;/strong&gt;：训练完成后移除冗余权重的技术，研究发现可以在不损失准确性的情况下剥离高达96%的参数，证明网络中大量参数实际上是冗余的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作者: NearlyRight&lt;/li&gt;
&lt;li&gt;日期： &lt;em&gt;2025年8月18日&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;大型网络的成功不是通过学习复杂的解决方案，而是通过提供更多机会来寻找简单的解决方案。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;阅读时间：7分钟&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票假说解释了为什么大规模神经网络能够成功，尽管数百年的理论曾预测它们会失败&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;五年前，如果有人建议AI研究人员训练拥有数万亿参数的神经网络，那只会招致怜悯的目光。这违反了机器学习中最基本的规则：如果模型过大，它就会变成一台高级复印机，只会记忆训练数据，却学不到任何有用的东西。&lt;/p&gt;
&lt;p&gt;这不仅仅是惯例，它是一条数学定律，有三百年统计理论作为支撑。每本教科书都展示着同一条不可避免的曲线：小模型欠拟合，最优模型泛化，大模型灾难性过拟合。毋庸置疑。&lt;/p&gt;
&lt;p&gt;然而今天，那些“不可能”的大规模模型驱动着ChatGPT，解码着蛋白质，并引发了一场价值数千亿美元的全球军备竞赛。改变的不仅仅是计算能力，更是我们对学习本身的理解。这场变革背后的故事揭示了AI领域最大的突破是如何源于研究人员敢于无视本领域基础假设的勇气。&lt;/p&gt;
&lt;h2 id="机器学习的铁律"&gt;机器学习的铁律&lt;/h2&gt;
&lt;p&gt;三百多年来&lt;a href="#ZgotmplZ"&gt;1&lt;/a&gt;，一条原则主导着所有学习系统：偏差-方差权衡。其数学优雅，逻辑无可辩驳。构建一个过于简单的模型，它会错过关键模式。构建一个过于复杂的模型，它会记忆噪声而非信号。&lt;/p&gt;
&lt;p&gt;想象一个学生学习算术。向他们展示成千上万道带答案的加法题，他们可能有两种学习方式。聪明的做法：掌握进位和位值的基本算法。愚蠢的做法：记忆每一个例子。第二种策略能在家庭作业中取得满分，但在考试中却完全失败。&lt;/p&gt;
&lt;p&gt;神经网络似乎尤其容易陷入这种记忆陷阱。拥有数百万参数的它们，可以轻易存储整个数据集。传统理论预测，这些过参数化网络会表现得像那个死记硬背的学生——在训练数据上表现完美，但在新数据上却毫无希望。&lt;/p&gt;
&lt;p&gt;这种理解塑造了整个领域。研究人员痴迷于架构技巧、正则化技术和数学约束，以从小而精心控制的模型中挤出性能。扩大规模被认为是昂贵而愚蠢的做法。&lt;/p&gt;
&lt;p&gt;该领域最受尊敬的声音强化了这一正统观念。“模型越大，过拟合越严重”成了口头禅。会议论文侧重于效率，而非规模。认为仅仅增加更多参数就能解决问题的想法，在学术界看来简直是异端。&lt;/p&gt;
&lt;h2 id="打破规则的异端们"&gt;打破规则的异端们&lt;/h2&gt;
&lt;p&gt;2019年，一群研究人员犯下了终极“罪行”：他们无视警告，继续扩大模型规模。他们没有在网络达到完美训练准确率时——理论上尖叫“危险”的节点——停止，而是进一步深入“禁区”。&lt;/p&gt;
&lt;p&gt;接下来发生的事情彻底颠覆了三百年来的学习理论。&lt;/p&gt;
&lt;p&gt;模型并没有崩溃。在最初它们看似记忆了训练数据而出现短暂的性能下滑后，令人惊叹的事情发生了。性能开始再次提升。而且是戏剧性地提升。&lt;/p&gt;
&lt;p&gt;这种现象被称为“双重下降”——首先是模型过拟合时预期的误差上升，然后是它们以某种方式完全超越过拟合后的意外第二次下降。记录这一发现的米哈伊尔·贝尔金（Mikhail Belkin）及其同事指出，这“与源自偏差-方差分析的传统智慧相悖”。&lt;/p&gt;
&lt;p&gt;其影响在AI研究领域掀起波澜。OpenAI随后的工作表明，这些益处横跨多个数量级。更大的模型不仅仅是积累更多事实——它们正在发展出质性新能力，包括仅从示例中学习任务的能力。&lt;/p&gt;
&lt;p&gt;突然间，整个领域都转向了。谷歌、微软、Meta和OpenAI投入数十亿美元来构建越来越大的模型。理论曾禁止的“越大越好”哲学，成了行业“北极星”。&lt;/p&gt;
&lt;p&gt;但一个问题困扰着每一位研究人员：这一切为什么会奏效？&lt;/p&gt;
&lt;h2 id="拯救学习理论的彩票"&gt;拯救学习理论的彩票&lt;/h2&gt;
&lt;p&gt;答案出乎意料地来自一个角落：一项关于神经网络彩票券的研究。2018年，麻省理工学院的乔纳森·弗兰克尔（Jonathan Frankle）和迈克尔·卡宾（Michael Carbin）正在研究剪枝——训练后移除不必要的权重。他们的发现将为规模悖论提供优雅的解决方案。&lt;/p&gt;
&lt;p&gt;他们在每个大型网络中都发现了“中奖彩票”——微小的子网络，它们能够与整个网络的性能相匹配。他们可以在不损失准确性的情况下剥离96%的参数。绝大多数成功网络的参数实际上都是冗余。&lt;/p&gt;
&lt;p&gt;但关键的洞察在于：这些“中奖”子网络只有在它们原始的随机初始权重下才能成功。改变初始值，同样的稀疏架构就会完全失败。&lt;/p&gt;
&lt;p&gt;彩票假说逐渐明确：大型网络成功不是通过学习复杂的解决方案，而是通过提供更多机会来找到简单的解决方案。每组权重都代表着一张不同的彩票——一个具有随机初始化的潜在优雅解决方案。大多数彩票会输，但拥有数十亿张彩票，中奖变得不可避免。&lt;/p&gt;
&lt;p&gt;在训练过程中，网络并不搜索完美的架构。它已经包含了无数个小型网络，每个都有不同的初始条件。训练变成了一场大规模的彩票抽奖，其中初始条件最佳的小型网络脱颖而出，而数十亿其他网络则逐渐消逝。&lt;/p&gt;
&lt;p&gt;这一发现将经验成功与经典理论调和。大型模型不是在记忆——它们是在广阔的参数空间中寻找优雅的简单解决方案。奥卡姆剃刀原理毫发无损地幸存下来：最简单的解释仍然是最好的。规模只是成为了一种更复杂的工具，用于找到那些简单的解释。&lt;/p&gt;
&lt;h2 id="智能的真正面貌"&gt;智能的真正面貌&lt;/h2&gt;
&lt;p&gt;其影响超越了人工智能范畴。如果学习意味着找到解释数据的最简单模型，并且更大的搜索空间能够实现更简单的解决方案，那么这重新定义了智能本身。&lt;/p&gt;
&lt;p&gt;考虑一下你的大脑：860亿个神经元，数万亿个连接，无论从何种标准衡量，都属于大规模过参数化。然而你擅长从有限的例子中学习并泛化到新情况。彩票假说认为这种神经元的丰度也服务于同样的目的——为任何问题提供大量潜在的简单解决方案。&lt;/p&gt;
&lt;p&gt;智能不是关于记忆信息——它关于寻找解释复杂现象的优雅模式。规模提供了进行这种搜索所需的计算空间，而不是存储复杂解决方案的地方。&lt;/p&gt;
&lt;p&gt;这一发现也阐明了科学进步。几十年来，研究人员避免扩大规模，因为理论认为这行不通。突破来自经验勇气——检验假设而不是接受它们。&lt;/p&gt;
&lt;p&gt;这种模式在整个科学领域回响。大陆漂移学说曾被驳斥，直到板块构造论提供了其机制。量子力学曾看似荒谬，直到实验证据变得确凿无疑。最重要的发现往往需要我们超越公认理论的边界。&lt;/p&gt;
&lt;p&gt;然而，彩票假说并没有推翻经典学习理论——它揭示了这些原则的运作方式比想象的要复杂得多。简单的解决方案仍然是最优的；我们只是发现了一种更好的方法来找到它们。&lt;/p&gt;
&lt;p&gt;对于AI发展而言，这种理解既带来了希望，也暗示了局限性。规模化之所以有效，是因为更大的模型提供了更多彩票，更多机会找到最优解决方案。但这种机制也意味着自然的限制。随着网络在寻找最小解决方案方面变得越来越成功，额外的规模将导致收益递减。&lt;/p&gt;
&lt;p&gt;这与专家们对当前方法局限性的担忧相符。扬·勒昆（Yann LeCun）认为，无论规模多大，基本的架构限制都可能阻碍语言模型实现真正的理解。彩票机制解释了当前的成功，同时也暗示了未来的挑战。&lt;/p&gt;
&lt;h2 id="优雅的惊喜"&gt;优雅的惊喜&lt;/h2&gt;
&lt;p&gt;彻底改变AI的意外发现提供了一个深刻的教训：宇宙常常为那些敢于挑战传统智慧边界的人保留着优雅的惊喜。有时最深刻的见解并非来自推翻既定原则，而是来自发现它们的运作方式比我们想象的要复杂得多。&lt;/p&gt;
&lt;p&gt;进化本身也遵循类似的原则，探索广阔的遗传可能性空间以寻找优雅的生存解决方案。最成功的生物并非最复杂的，而是适应最有效率的。&lt;/p&gt;
&lt;p&gt;曾被认为是学习理论危机的，最终却证明了其正确性。偏差-方差权衡幸存下来，但我们了解到它通过比任何人怀疑的都更为微妙的机制运作。大型神经网络并非通过打破规则而成功——它们通过在我们从未想到的水平上运用这些规则而成功。&lt;/p&gt;
&lt;p&gt;那些敢于超越理论舒适区进行规模化研究的研究人员，不仅推动了AI发展，也提醒我们，经验现实有时蕴含着理论尚未掌握的智慧。在一个建立在数学确定性之上的领域，最重要的发现却来自拥抱不确定性本身。&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;300年的时间范围指的是支撑现代偏差-方差分析的数学基础原则，而非当代术语。贝叶斯定理（1763年）建立了用证据更新信念的数学框架，而拉普拉斯在统计推断方面的早期工作（18世纪80年代-19世纪10年代）则正式确立了模型必须平衡拟合度与简洁性以避免虚假结论的原则。这些早期统计见解——即过于复杂的解释往往捕捉的是噪声而非信号——构成了我们现在所说的偏差-方差权衡的数学基石。具体的现代公式是在20世纪后期几十年中出现的，但其核心原则已经指导统计推理数百年。&lt;a href="#ZgotmplZ"&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Title: &lt;a href="https://nearlyright.com/how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/"&gt;How AI researchers accidentally discovered that everything they thought about learning was wrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Author: NearlyRight&lt;/li&gt;
&lt;li&gt;Date: 18 Aug, 2025&lt;/li&gt;
&lt;li&gt;URL:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#AI&lt;/p&gt;</description></item></channel></rss>