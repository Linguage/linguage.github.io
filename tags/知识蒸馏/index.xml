<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>知识蒸馏 on Linguista</title><link>https://linguista.cn/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/</link><description>Recent content in 知识蒸馏 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 11 Feb 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/index.xml" rel="self" type="application/rss+xml"/><item><title>知识蒸馏技术的发展历程与应用</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/knowledge-distillation-deepseek-hinton/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/knowledge-distillation-deepseek-hinton/</guid><description>&lt;h1 id="知识蒸馏技术的发展历程与应用"&gt;知识蒸馏技术的发展历程与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;知识蒸馏技术由诺贝尔物理学奖得主Geoffrey Hinton于2015年提出，是一种通过让小模型学习大模型知识来实现模型压缩的有效方法。本文从技术起源、发展历程、核心创新到最新应用实践，全面解析知识蒸馏技术的前世今生，重点介绍DeepSeek如何通过知识蒸馏技术提升模型性能。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;知识蒸馏技术起源于机器学习领域对模型压缩的需求。2006年，研究者发现集成学习虽然能显著提高预测准确率，但多个模型集成会导致模型体积过大、运行速度缓慢的问题。康奈尔大学Rich Caruana教授率先提出模型压缩方法，通过训练小模型来模仿大模型的行为，并使用MUNGE算法生成人工数据进行训练，成功将模型运行速度提升1000倍且几乎不损失准确率。&lt;/p&gt;
&lt;p&gt;2015年，Hinton在谷歌实验室正式提出知识蒸馏概念。他的核心创新在于引入了软目标和温度系数，认为知识传递不应仅限于最终的硬目标结果，还应包含模型输出的中间概率分布信息。通过调整温度参数，可以使小模型学习到大模型的暗知识，从而提高泛化能力。这一方法通过KL散度损失函数来衡量学生模型与教师模型输出分布的差异。&lt;/p&gt;
&lt;p&gt;近年来，知识蒸馏技术不断演进创新。2015年Romero等人提出中间层蒸馏方法，认为模型的中间层特征同样包含重要知识。2017年Yim等人发展出关系蒸馏，通过FSP矩阵捕捉网络层间关系。此外还出现了对抗性蒸馏、多教师蒸馏等多种变体。2025年，DeepSeek通过R1模型输出思维链和结果，在较小开源模型上进行监督微调，使蒸馏模型不仅学到结果还学到推理过程，性能优于同参数模型。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;硬目标与软目标&lt;/strong&gt;：硬目标指传统的one-hot编码标签，只有正确类别的概率为1，其他均为0；软目标是经过温度系数调整后的概率分布，包含了模型对所有类别的相对置信度。Hinton认为软目标包含了暗知识，能够反映类别间的相似性关系，比硬目标提供更多信息量，有助于学生模型学习更泛化的特征表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;温度系数&lt;/strong&gt;：知识蒸馏中的关键超参数，用于控制模型输出概率分布的平滑程度。当温度T=1时，输出保持原始softmax分布；当T&amp;gt;1时，分布变得更平滑，各类别概率差异缩小；当T&amp;lt;1时，分布变得更尖锐。在蒸馏过程中，通常使用较高的温度来产生软目标，使学生模型能够学到更精细的类别关系信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KL散度损失&lt;/strong&gt;：用于衡量两个概率分布差异的指标，在知识蒸馏中用于计算学生模型与教师模型输出分布的距离。蒸馏损失通常由两部分组成：蒸馏损失（学生与教师软目标间的KL散度）和学生损失（学生与真实标签的交叉熵），通过加权平衡两者来优化学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中间层蒸馏&lt;/strong&gt;：由Romero等人提出的蒸馏方法，认为不仅输出层包含知识，模型的中间隐藏层特征同样承载重要信息。该方法通过让学生模型的中间层特征逼近教师模型的对应层，实现更全面的知识传递。这种方法特别适用于深度网络，能够传递模型的层次化特征表示能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维链蒸馏&lt;/strong&gt;：DeepSeek实践的蒸馏方法，通过让大模型输出完整的推理过程（思维链）和最终答案，然后用这些数据监督训练小模型。与传统结果蒸馏不同，思维链蒸馏使小模型不仅学到输入输出映射关系，还能学到部分推理能力，在复杂任务中表现更好。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/srgCxtlkjj15JQ60wUjjzw"&gt;知识蒸馏：由诺奖得主Hinton提出，9年后被DeepSeek带火，究竟是什么？&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>