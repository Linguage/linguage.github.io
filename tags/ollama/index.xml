<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on Linguista</title><link>https://linguista.cn/tags/ollama/</link><description>Recent content in Ollama on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 03 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>本地运行大型语言模型实战指南</title><link>https://linguista.cn/curated/summary-2025-p1/local-llm-setup-guide/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/summary-2025-p1/local-llm-setup-guide/</guid><description>&lt;h1 id="本地运行大型语言模型实战指南"&gt;本地运行大型语言模型实战指南&lt;/h1&gt;
&lt;h2 id="摀要"&gt;摀要&lt;/h2&gt;
&lt;p&gt;本文是作者 Abishek Muthian 分享的本地运行大型语言模型的实践经验总结。文章详细介绍了硬件配置要求、核心工具选择（Ollama、Open WebUI、llamafile 等）、模型获取与管理策略，以及在图像生成、代码补全、笔记查询等场景的具体应用。作者强调了本地运行 LLMs 在数据隐私和响应延迟方面的优势，并指出这一切离不开开源项目和免费模型的支持。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章以实用的技术分享为主线，首先阐述了本地运行 LLMs 的意义和价值所在。作者明确指出，在开始之前需要感谢那些为 LLMs 训练提供基础数据的无名艺术家、编码者和作家，他们的工作往往没有得到应有的认可。&lt;/p&gt;
&lt;p&gt;在硬件要求部分，作者公开了自己的设备配置：搭载 Linux 系统的笔记本电脑，配备 i9 处理器（32 线程）、4090 GPU（16GB 显存）和 96GB RAM。同时他也说明，较小的模型可以在旧 GPU 或 CPU 上运行，只是在速度和准确性上会有所妥协。这为不同预算条件的读者提供了参考基准。&lt;/p&gt;
&lt;p&gt;工具推荐是文章的核心内容，作者详细介绍了 Ollama 这一中间件工具，它提供了 Python 和 JavaScript 库，可以方便地在 Docker 中运行。Open WebUI 作为前端界面，提供了熟悉的聊天体验，支持文本和图像输入。此外还提到了 llamafile 这种单执行文件方式，以及 AUTOMATIC1111、Fooocus、ComfyUI 等图像生成工具。&lt;/p&gt;
&lt;p&gt;模型管理方面，作者使用 Ollama 模型页面下载最新的 LLMs，并通过 RSS 在 Thunderbird 上跟踪模型更新。对于图像生成模型，则使用 CivitAI 平台获取特定风格的模型。文章最后强调了本地运行 LLMs 的核心优势：完全的数据控制权和更低的响应延迟。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;：这是一个专门为运行 LLMs 设计的中间件工具，提供了便捷的 Python 和 JavaScript 库支持。作者推荐在 Docker 环境中使用 Ollama，这样可以更好地隔离和管理运行环境，简化部署流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt;：作为本地 LLMs 的前端界面，它提供了类似 ChatGPT 的聊天体验，支持文本和图像输入，并与 Ollama 后端无缝通信。这种架构分离了模型运行和用户界面，使得整个系统更加模块化和易于维护。&lt;/p&gt;</description></item></channel></rss>