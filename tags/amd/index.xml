<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AMD on Linguista</title><link>https://linguista.cn/tags/amd/</link><description>Recent content in AMD on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 02 Jul 2025 08:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/amd/index.xml" rel="self" type="application/rss+xml"/><item><title>解锁AI潜力：吴恩达博士与苏姿丰博士的见解</title><link>https://linguista.cn/infos/htmlcards/unlocking-ai-potential-andrew-ng-lisa-su/</link><pubDate>Wed, 02 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/infos/htmlcards/unlocking-ai-potential-andrew-ng-lisa-su/</guid><description>本文深入探讨了人工智能的未来发展趋势，重点阐述了开放AI生态系统的战略价值以及AI技术栈各层级的关键变革。通过分析吴恩达博士与苏姿丰博士的对话，文章揭示了从基础设施优化到开发者工具演进的完整路径，展示了AI如何降低开发门槛并重塑软件工程范式。此外，内容还展望了未来职业形态的转变，强调人机协作将成为创新的核心驱动力。</description></item><item><title>MI300X、H100与H200训练性能对比：CUDA护城河依然坚固</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</guid><description>&lt;h1 id="mi300xh100与h200训练性能对比cuda护城河依然坚固"&gt;MI300X、H100与H200训练性能对比：CUDA护城河依然坚固&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于SemiAnalysis团队为期五个月的独立基准测试研究，全面对比了AMD MI300X与NVIDIA H100/H200在AI模型训练场景下的实际性能表现。测试涵盖GEMM计算性能、HBM内存带宽、单节点与多节点训练吞吐量、集体通信效率等核心指标。研究结果表明，尽管MI300X在纸面规格上具有一定优势，但由于软件栈的诸多问题，其实际训练性能显著落后于NVIDIA产品。NVIDIA的CUDA生态系统护城河依然坚固，而AMD在开箱即用体验、软件稳定性、性能优化等方面仍需大量改进工作。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先介绍了研究的背景和方法论，SemiAnalysis团队与NVIDIA和AMD进行了深度互动，进行了为期五个月的独立测试，旨在反映真实用户在实际使用中可能遇到的情况。研究不仅关注纯性能指标，还特别重视用户体验和可用性，这是评估AI基础设施产品的重要维度。&lt;/p&gt;
&lt;p&gt;文章的核心发现部分揭示了AMD和NVIDIA产品在实际应用中的巨大差距。尽管MI300X的总拥有成本较低，但在按TCO计算的训练性能方面，使用AMD公共稳定版软件的表现不如H100/H200。关键问题在于AMD的软件体验存在诸多缺陷，导致MI300X的实际训练性能远低于其纸面规格。在BF16精度下，H100/H200的GEMM性能约为720 TFLOP/s，而MI300X仅为约620 TFLOP/s，比其市场宣传的1307 TFLOP/s低得多，比H100/H200慢14%。在FP8精度下，差距更为明显，H100/H200达到约1280 TFLOP/s，MI300X仅为约990 TFLOP/s，慢22%。&lt;/p&gt;
&lt;p&gt;文章深入分析了性能差距的技术原因，从GEMM性能、集体通信操作、网络拓扑等多个维度进行了详细对比。特别值得关注的是，研究指出了当前流行的GEMM基准测试方法存在严重缺陷，许多测试未正确清除L2缓存，且仅取最大性能值而非迭代过程中的中位数或平均值。在多节点训练场景下，H100的性能优势更加明显，这主要得益于NVIDIA的NVLink拓扑、InfiniBand SHARP技术以及高度优化的NCCL集体通信库。&lt;/p&gt;
&lt;p&gt;最后，文章对AMD提出了具体的改进建议，包括增加软件工程资源投入、改进GEMM库的启发式模型、减少对环境标志的依赖、加强内部测试流程等。作者指出，AMD许多库是基于NVIDIA开源或生态系统库分叉而来，这种策略长期来看不利于建立独立健康的软件生态系统。文章还提供了不同网络配置下GPU集群的成本和性能分析，为读者提供了全面的TCO视角。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GEMM性能与Transformer训练&lt;/strong&gt;：通用矩阵乘法是衡量Transformer架构模型训练性能的核心指标，因为现代深度学习模型的计算负载中，矩阵乘法操作占据了主导地位。研究显示，AMD MI300X在BF16和FP8精度下的GEMM性能均显著低于NVIDIA H100/H200，这是导致其实际训练性能落后于纸面规格的关键原因之一。值得注意的是，AMD的torch.matmul和F.Linear API在性能上存在差异，这是因为底层使用了不同的GEMM库，这种不一致性增加了用户的优化负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;集体通信操作与多节点训练&lt;/strong&gt;：在大规模分布式训练中，all_reduce、all_gather、reduce_scatter等集体通信操作的效率直接影响整体训练性能。NVIDIA的NCCL库经过多年优化，在单节点和多节点环境下都表现出色，特别是在配合NVLink和InfiniBand SHARP技术时，能够实现近乎线性的扩展性能。相比之下，AMD的RCCL库性能明显不足，在多节点场景下的差距尤为突出，这严重限制了MI300X在大规模训练任务中的竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUDA生态系统护城河&lt;/strong&gt;：NVIDIA的真正优势不仅体现在硬件性能上，更重要的是其软件生态系统的完整性。从开箱即用的PyTorch体验、高度优化的cuBLASLt库、智能的算法启发式模型，到完善的开发工具和文档，NVIDIA为用户提供了无缝的开发和部署体验。反观AMD，用户需要处理大量环境标志配置、手动调优、依赖库从源代码构建等复杂操作，这种体验差异构成了CUDA生态系统的深层次护城河。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总拥有成本与实际训练性能&lt;/strong&gt;：虽然MI300X的硬件价格较低，总拥有成本具有优势，但如果按TCO计算的实际训练性能来评估，使用AMD公共稳定版软件的性价比并不如NVIDIA产品。文章强调，即使使用AMD的定制开发构建能够改善性能，但等到这些构建成熟可用时，NVIDIA的下一代产品可能已经上市。这揭示了AI基础设施评估中一个重要原则：不能仅看硬件规格和价格，必须综合考虑软件成熟度、开发效率、时间成本等因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;软件质量与用户体验&lt;/strong&gt;：本文反复强调的一个核心观点是，AI训练硬件的竞争本质上是软件生态系统的竞争。AMD存在大量内部测试不足导致的软件问题，如PyTorch原生Flash Attention内核性能极低、注意力层与torch.compile兼容性问题、PYTORCH_TUNABLE_OPS标志导致的内存泄漏和程序崩溃等。这些看似细微的软件缺陷累积起来，会严重影响用户体验，降低开发效率，最终影响产品的市场竞争力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/#executive-recommendation-to-amd"&gt;MI300X vs H100 vs H200 Benchmark Part 1: Training – CUDA Moat Still Alive&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;SemiAnalysis&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-22&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>