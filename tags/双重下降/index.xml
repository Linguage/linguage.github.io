<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>双重下降 on Linguista</title><link>https://linguista.cn/tags/%E5%8F%8C%E9%87%8D%E4%B8%8B%E9%99%8D/</link><description>Recent content in 双重下降 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 18 Aug 2025 08:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%8F%8C%E9%87%8D%E4%B8%8B%E9%99%8D/index.xml" rel="self" type="application/rss+xml"/><item><title>AI研究人员如何意外发现他们对学习的固有认知是错误的</title><link>https://linguista.cn/static/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</link><pubDate>Mon, 18 Aug 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/wechat-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</guid><description>本文深入探讨了机器学习领域关于“偏差-方差权衡”的铁律如何被2019年的“双重下降”现象打破。文章阐述了研究人员如何通过扩大模型规模，意外发现大型神经网络不仅能避免过拟合，反而能涌现出惊人的新能力。这一发现挑战了三百年的统计理论，引发了以 OpenAI 为首的“越大越好”行业变革，并最终由“彩票假说”从数学上解释了为何海量冗余参数是通向简单高效解决方案的必经之路。</description></item></channel></rss>