<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Linguista</title><link>https://linguista.cn/tags/llm/</link><description>Recent content in LLM on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 04 Jan 2026 08:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>2025 大语言模型年度回顾</title><link>https://linguista.cn/static/ak-llm2025/</link><pubDate>Sun, 04 Jan 2026 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/ak-llm2025/</guid><description>本文回顾了 2025 年大语言模型（LLM）领域的核心演变，重点阐述了从模仿人类到“召唤幽灵”的范式转移。文章深入剖析了 RLVR（推理强化学习）如何通过可验证的奖励机制催生出非人类的“锯齿状”智力，并探讨了 Vibe Coding 如何通过降低门槛让编程变得廉价且短暂，将 AI 从云端服务转变为驻留在本地的数字生灵。</description></item><item><title>Andrej Karpathy 论人工智能的下一个十年</title><link>https://linguista.cn/curated/henrinotes-2025_p2/andrej-karpathy-ai-next-decade-agents/</link><pubDate>Sat, 18 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/andrej-karpathy-ai-next-decade-agents/</guid><description>&lt;h1 id="andrej-karpathy-论人工智能的下一个十年"&gt;Andrej Karpathy 论人工智能的下一个十年&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文综合分析了著名人工智能专家 Andrej Karpathy 的核心观点，他基于在 AI 领域近二十年的经验，对当前的人工智能发展、未来趋势及社会影响提出了深刻而务实的见解。Karpathy 认为，实现功能完备的智能体需要十年而非一年，当前的模型在智能、多模态和持续学习等方面存在显著的认知缺陷。他将强化学习的奖励机制比作通过吸管吸取监督信号，效率低下且充满噪声，并指出依赖模型自身生成的数据训练会导致模型坍塌问题。在 AI 工程实践方面，他认为当前的编码智能体对于新颖、复杂的任务来说更像是残次品，其作用更接近于高级自动补全，而非真正的程序员替代品。他强调从演示到可靠产品的巨大鸿沟，预示着高风险领域的自动化将是一个漫长过程。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;Karpathy 首先对行业内普遍存在的智能体之年过度乐观预测进行了校准，提出了智能体十年的说法。他详细分析了当前智能体无法被广泛应用的核心原因，包括智能水平不足、多模态能力有限、缺乏持续学习等关键瓶颈。这些缺陷使得目前的模型无法被雇佣为实习生，因为它们在实际工作场景中表现不可靠。&lt;/p&gt;
&lt;p&gt;在回顾 AI 范式演进时，Karpathy 分享了他亲身经历的数次地震式转变。他从深度学习的兴起谈起，以 AlexNet 的成功为标志，深度学习从少数人研究的小众领域转变为 AI 的主流。他反思了早期智能体探索中的一次失误，即 2013 年左右以 Atari 游戏为代表的深度强化学习热潮。他认为对游戏的过度关注是错误的路径，真正的智能体应能处理现实世界的知识工作。LLM 的成功揭示了必须首先通过大规模预训练获得强大的表征能力和语言基础，然后才能在其之上构建有效的智能体。&lt;/p&gt;
&lt;p&gt;关于 AI 与动物智能的辨析，Karpathy 指出动物是演化的产物，其大脑中集成了大量内置硬件，而 AI 是通过模仿人类在互联网上留下的数据进行训练的，更像是数字世界中的幽灵或灵魂。他将 LLM 的预训练过程比作一种蹩脚的演化，这是在当前技术条件下能够实现的、为智能体提供一个知识和能力起点的最实用方法。他提出了一个重要的研究方向，即设法剥离模型的知识和记忆，保留其纯粹的认知核心。&lt;/p&gt;
&lt;p&gt;在深入剖析 LLM 的内部工作方式及其根本性限制时，Karpathy 指出模型表现出的智能在很大程度上依赖于其上下文窗口。上下文窗口中的信息就像人类的工作记忆，模型可以非常直接地访问，而存储在模型权重中的知识更像是对互联网文档的模糊回忆。他直言强化学习是糟糕的，并将 RL 的奖励机制比作通过吸管吸取监督信号，这种方法噪声极大。让模型通过反思来学习面临着模型坍塌的风险，模型生成的任何内容其分布都是悄然坍塌的，看似合理但缺乏多样性和熵。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;智能体十年&lt;/strong&gt;：Karpathy 提出这一概念旨在对行业内过度乐观的时间表进行校准。他认为虽然当前出现了一些令人印象深刻的早期智能体，但要实现真正可靠、能像人类实习生或员工一样工作的智能体，仍有大量艰巨工作尚待完成。这一预测基于他在 AI 领域近二十年的从业经验和直觉，他认为当前面临的问题虽然棘手且困难，但可以解决，综合来看十年是一个比较合理的时间框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表征能力&lt;/strong&gt;：这是 LLM 成功的关键所在，也是构建有效智能体的先决条件。早期在游戏环境中过度依赖强化学习之所以是一次失误，正是因为当时的神经网络缺乏强大的表征能力，导致智能体在稀疏奖励的环境中无法有效学习，只会燃烧森林般的计算资源。必须首先通过大规模预训练获得强大的表征能力和语言基础，然后才能在其之上构建有效的智能体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型坍塌&lt;/strong&gt;：这是指当持续用模型自身生成的数据进行训练时，模型会变得越来越糟，最终完全丧失能力。模型生成的任何内容其分布都是悄然坍塌的，它们看似合理，但缺乏多样性和熵，仅仅占据了所有可能输出中的一个极小流形。人类通过与他人交流等方式不断寻求外部熵来避免类似的思想固化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;九的进军&lt;/strong&gt;：Karpathy 用这一概念来形容将 AI 技术从演示转化为产品的难度。产品化的过程是九的进军，即实现 90% 的可靠性只是第一步，之后每提升一个数量级都需要付出同等甚至更多的努力。自动驾驶从 1980 年代就有演示，但至今仍未完全实现，这揭示了巨大的演示到产品差距，尤其是在安全攸关的领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;知识坡道&lt;/strong&gt;：这是 Karpathy 在其教育项目 Eureka 中核心理念的一部分。教育的核心是技术性的，即为复杂的知识构建平滑的学习路径，确保学习者在任何时候都面临恰到好处的挑战，既不感到无聊也不感到挫败。他追求的是最大化学习者每秒钟获得的顿悟感，这要求教学内容经过精心设计，从最简单的第一性原理出发，逐步引入复杂性。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://twitter.com/karpathy"&gt;Andrej Karpathy on AI&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Andrej Karpathy&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>nanochat - 信息卡</title><link>https://linguista.cn/static/nanochat-gemini/</link><pubDate>Mon, 13 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/nanochat-gemini/</guid><description>Andrej Karpathy 发布的 nanochat 项目提供了一个约 8,000 行代码的全栈 LLM 训练与推理管线，旨在以最小化依赖构建一个可黑客攻击的 ChatGPT 克隆体。该项目涵盖了从 Rust 分词器、FineWeb 预训练到 SFT 和 RL 的完整流程，强调低成本可达性（最低约 $100）与坚实的基线骨架，非常适合作为深入理解 LLM 工作原理的研究平台与教学压轴项目。</description></item><item><title>nanochat 速览</title><link>https://linguista.cn/static/nanochat-kimi/</link><pubDate>Mon, 13 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/nanochat-kimi/</guid><description>nanochat 是 Andrej Karpathy 推出的一个极简 ChatGPT 克隆训练管道，旨在通过一个单一的、可被黑客攻击的仓库来建立强有力的基线。该项目不仅包含了从预训练到微调、强化学习再到推理的全栈代码，还提供了 WebUI 界面，使开发者能够在短时间内以极低的成本（约 100 美元）复现类似 GPT 的对话模型。作为一个强调可读性和可扩展性的研究利器，它全流程仅依赖约 8000 行核心代码，彻底打通了从 FineWeb 预训练到 SmolTalk 微调的最后一公里。</description></item><item><title>AI代理中的上下文工程与Anthropic团队</title><link>https://linguista.cn/static/anthropic_context_engineering/</link><pubDate>Wed, 01 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/anthropic_context_engineering/</guid><description>本文深入探讨了人工智能代理开发中的关键范式转变：从传统的提示工程转向更系统的上下文工程。文章详细阐述了如何通过优化上下文结构来提升模型性能，特别是在管理长任务和复杂推理场景中的应用。内容涵盖了Transformer注意力机制的复杂度挑战、上下文窗口的最优配置策略，以及AI代理实现自我管理的核心技术路径。通过介绍Anthropic应用AI团队的最新研究成果，本文为开发者提供了从理论到实践的完整指南，揭示了在构建高效AI代理时如何充分利用上下文预算，实现更精准的意图理解和任务执行。</description></item><item><title>AMP之路：AI编码体的终极进化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/amp-ai-coding-agent-future/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/amp-ai-coding-agent-future/</guid><description>&lt;h1 id="amp之路ai编码体的终极进化"&gt;AMP之路：AI编码体的终极进化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Latent Space播客中SourceGraph CEO Quinn Slack与AMP Code工程师Thorsten Ball的深度对话，全面阐述AI coding agent的发展历程与未来方向。文章系统梳理了从Cody到AMP的产品变革逻辑，深入分析AMP团队如何在8人规模下实现极致敏捷开发，并对行业内盛行的subagents、prompt engineering等潮流提出批判性反思，为AI辅助编程工具的发展提供了重要的实践洞察。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;对话首先回顾了AMP诞生的战略背景——源于对Cody产品模式的不满足。Cody作为集成于大型平台之下的工具，其更新节奏被平台所牵制，无法适应AI领域日新月异的技术迭代。AMP选择独立出来，以&amp;quot;轻量、极速、极易变更&amp;quot;为优先目标，通过每日多达15次的持续部署和直接主分支合并的激进策略，实现了50%以上的月活用户增长。&lt;/p&gt;
&lt;p&gt;其次，文章深入剖析了AMP内部的开发实践。这个仅8人的核心团队彻底摒弃了传统代码审查流程，转而采用&amp;quot;全周期责任制&amp;quot;，每位工程师对自己负责的功能拥有完全决策权。团队继承了SourceGraph平台的安全与运维支持，但在产品体验和代码实现上保持了初创公司的敏捷性，随时准备砍掉复杂度过高的功能。&lt;/p&gt;
&lt;p&gt;最后，两位嘉宾对行业趋势提出了独到见解。他们认为subagents和复杂的prompt优化器并非未来方向，反而会因为LLM的不确定性而导致工作流失效。AMP更关注&amp;quot;上下文工程&amp;quot;，鼓励用户以精简方式精准控制上下文，而非寄希望于完全自动化的代码生成。这种对早期极客用户的精准定位，以及对伪自动化的警惕，构成了AMP独特的产品哲学。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;极致敏捷开发&lt;/strong&gt;：AMP通过每日15次部署、取消代码审查、直接主分支合并等激进实践，在8人团队规模下实现比传统企业更快的迭代速度，体现了&amp;quot;响应技术和市场巨变&amp;quot;优于&amp;quot;规范化规模化&amp;quot;的优先级判断。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;上下文工程&lt;/strong&gt;：相较于流行的prompt engineering和subagents，AMP更强调用户对代码上下文的精准控制和分支管理，认为在LLM不确定性存在的条件下，简明的上下文管理比复杂自动化更可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全周期责任制&lt;/strong&gt;：每位工程师对自己负责的功能模块拥有完全决策权，从设计到实现到维护全程负责，这种&amp;quot;独裁者&amp;quot;模式打破了传统层层审查的流程，但要求团队成员兼具技术能力和业务思维。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;dogfooding&lt;/strong&gt;：让产品团队成为自身产品的重度使用者，通过每日高频使用即时发现问题并推动改变，这种内部测试文化确保了产品始终贴合真实使用场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;早期极客定位&lt;/strong&gt;：AMP放弃服务所有人的中庸策略，只关注对AI前沿技术敏感、愿意学习新工具的技术极客，这种精准定位使产品能够快速迭代而不受大众市场需求的拖累。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=b4rOVZWLW6E"&gt;Building the God Coding Agent&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Quinn Slack、Thorsten Ball&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;Latent Space 播客&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>ThePrimeTime vs Andrej Karpathy: 软件正在改变（又一次）</title><link>https://linguista.cn/static/theprimetime-ak-software-changed-again/</link><pubDate>Fri, 04 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/theprimetime-ak-software-changed-again/</guid><description>这篇文章深入探讨了 Andrej Karpathy 提出的关于“软件 2.0”及大型语言模型（LLM）未来的革命性观点，并整合了 ThePrimeTime 对此议题的审慎评论。文章通过回顾软件从传统代码到神经网络，再到当今生成式 AI 的演变历程，展示了软件定义的重塑过程。内容不仅剖析了神经网络权重如何取代传统逻辑，还通过互动视角探讨了 Karpathy 关于 LLM 的精妙类比，以及这一技术变革带来的潜在机遇与挑战。这是一场关于代码、智能与未来的深度对话。</description></item><item><title>上下文工程: AI交互新范式</title><link>https://linguista.cn/static/context-engineering-next-generation-hci/</link><pubDate>Fri, 04 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/context-engineering-next-generation-hci/</guid><description>Andrej Karpathy 提出的“上下文工程”标志着人机交互的范式转移。本文深入探讨了从雕琢静态“提示词”向构建动态“信息世界”的演变，详细解析了LLM的上下文组件（指令、提示、状态），并对比了系统提示与用户提示在工程实践中的不同维度。文章还通过互动图表展示了行业领导者在技术栈上的布局，为构建下一代AI应用提供了清晰的路径指引。</description></item><item><title>互动报告：2025年软件工程与AI的现实检验</title><link>https://linguista.cn/static/software-engineering-with-llms-in2025-reality-check/</link><pubDate>Wed, 02 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/software-engineering-with-llms-in2025-reality-check/</guid><description>随着2025年的到来，软件工程领域正经历着一场关于AI效能的深刻反思。尽管高管们对自动化编程充满宏大愿景，但一线数据揭示了生产力的微妙悖论。本报告深入分析了从科技巨头到初创企业的实际落地情况，探讨了为何效率提升未达预期的深层原因，并展望了MCP协议等新技术可能带来的转折点。</description></item><item><title>AI编程工具底层逻辑深度剖析</title><link>https://linguista.cn/static/claudecode/</link><pubDate>Tue, 01 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/static/claudecode/</guid><description>本文深入剖析了以Claude和Cursor为代表的AI编程工具的底层运作逻辑。文章从大型语言模型（LLM）的训练基础出发，详细阐述了从预训练到监督微调的进化路径，并探讨了代码库理解的两种核心哲学。研究表明，这些工具能将开发效率提升高达55%，推动软件开发从手动编码向人机协作的新范式演进。</description></item><item><title>构建高效AI代理的实用指南</title><link>https://linguista.cn/curated/henrinotes-2025_p2/building-effective-agents-workflows/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/building-effective-agents-workflows/</guid><description>&lt;h1 id="构建高效ai代理的实用指南"&gt;构建高效AI代理的实用指南&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Anthropic与数十个团队合作的实战经验，阐述了构建有效AI代理的核心原则。作者强调最成功的实现并未使用复杂框架，而是采用简单、可组合的模式。文章系统地区分了&amp;quot;工作流&amp;quot;（预定义代码路径）与&amp;quot;代理&amp;quot;（LLM自主决策），详细介绍了五种工作流模式及其适用场景，并提供了从简单入手、逐步迭代的实施建议。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先明确了代理系统的两种基本类型：工作流通过预定义代码路径编排LLM和工具，适合可预测的任务；代理则由LLM动态指导自身流程，适合需要灵活性和模型驱动决策的场景。作者建议优先考虑简单解决方案，仅在必要时增加复杂性，因为代理系统通常会牺牲延迟和成本来换取任务性能。&lt;/p&gt;
&lt;p&gt;在工作流模式部分，文章详细阐述了五种核心模式：提示链将任务分解为固定步骤；路由将输入分类到专门流程；并行化通过分段或投票提高效率；协调器-工作器动态分解复杂任务；评估器-优化器通过迭代改进提升质量。每种模式都配有具体的使用场景和实例。&lt;/p&gt;
&lt;p&gt;关于代理的实现，文章强调其处理开放式、步骤不确定问题的优势，同时也提醒注意成本和复合错误风险。作者提出了三大核心原则：保持设计简洁、优先考虑透明度、精心设计Agent-计算机接口（ACI）。文章最后通过客户支持和编码代理两个实际应用，展示了这些模式的实践价值。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;工作流与代理的区别&lt;/strong&gt;：工作流是预定义的代码路径，提供可预测性和一致性，适合定义明确的任务；代理由LLM动态指导流程，具有自主决策能力，适合需要大规模灵活性的场景。这种区分帮助开发者在不同应用场景下选择合适的架构模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;增强型LLM作为构建块&lt;/strong&gt;：通过检索、工具和记忆等功能增强的LLM是代理系统的基础。关键在于根据特定用例定制这些功能，并提供简单、文档齐全的接口。模型上下文协议（MCP）为实现这种集成提供了标准化方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agent-计算机接口（ACI）&lt;/strong&gt;：工具设计和文档的重要性应与整体提示同等对待。良好的ACI需要考虑工具格式对LLM的友好程度、参数命名的清晰性、示例用法的完整性，以及防呆设计。在SWE-bench代理开发中，团队甚至花更多时间优化工具而非整体提示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;从简单到复杂的迭代原则&lt;/strong&gt;：成功的关键不在于构建最复杂的系统，而在于构建适合需求的系统。应从简单提示开始，通过全面评估优化，仅在更简单方案不足时才添加多步代理系统。框架可以作为起点，但在生产环境应减少抽象层以保持可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;评估驱动的迭代改进&lt;/strong&gt;：无论采用何种模式，持续评估和迭代都是成功的关键。评估器-优化器工作流特别适合有明确评估标准的场景，类似于人类作者的迭代写作过程。客户支持和编码代理的成功应用都依赖于可衡量的成功标准和反馈循环。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;Building effective agents&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Erik Schluntz, Barry Zhang&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024年12月20日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>理解推理型大型语言模型的构建与优化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/understanding-reasoning-llms-optimization/</guid><description>&lt;h1 id="理解推理型大型语言模型的构建与优化"&gt;理解推理型大型语言模型的构建与优化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文由AI专家Sebastian Raschka撰写，系统介绍了推理型大型语言模型的核心概念与构建方法。文章详细分析了推理模型的定义、优势与劣势，并以DeepSeek R1为例，阐述了纯强化学习、监督微调、模型蒸馏等四种主要的训练优化方法。作者还探讨了低预算下开发推理模型的可行性，为研究者和开发者提供了实用的技术路线参考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从LLM领域的发展趋势切入，指出2024年以来专门化应用方向的快速发展。推理模型作为一类能够通过多步中间步骤解决复杂任务的特殊模型，在数学证明、逻辑谜题和高级编程等场景中具有重要价值，但在简单任务中可能因&amp;quot;过度思考&amp;quot;而降低效率。&lt;/p&gt;
&lt;p&gt;DeepSeek R1系列模型作为典型案例，展示了三种不同的训练范式：R1-Zero采用纯强化学习，无需监督微调即可自动生成推理步骤；R1结合了监督微调与强化学习，引入一致性奖励机制；R1-Distill则通过模型蒸馏技术，将大型模型的推理能力迁移到较小的模型中。&lt;/p&gt;
&lt;p&gt;构建推理模型的四种主要方法各具特色：推理时扩展通过增加计算资源提高性能；纯强化学习展示了无需监督数据的可能性；监督微调结合强化学习能够充分发挥两者优势；模型蒸馏则在效率和成本方面具有显著优势。对于资源有限的开发者，Sky-T1和TinyZero等项目证明了低预算开发推理模型的可行性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理模型&lt;/strong&gt;：指需要通过复杂、多步生成中间步骤来回答问题的语言模型。这类模型能够解决需要逻辑推理的任务，如数学计算、编程挑战等，但在简单任务中可能导致效率低下和成本增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;纯强化学习&lt;/strong&gt;：DeepSeek R1-Zero证明了推理能力可以通过纯强化学习而无需监督微调来实现。模型通过准确性和格式奖励自动生成推理步骤，这为理解AI推理能力的本质提供了新的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型蒸馏&lt;/strong&gt;：通过将大型模型生成的推理数据用于训练较小的模型，以提高推理能力。这种方法在效率和成本方面具有优势，使得资源有限的团队也能开发出性能可观的推理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推理时扩展&lt;/strong&gt;：通过增加推理时的计算资源来提高模型性能，包括链式思考提示、投票和搜索策略等。这种方法不需要重新训练模型，但会增加推理时间和成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;旅程学习&lt;/strong&gt;：一种新的训练策略，通过在训练数据中包含错误的解决方案路径，让模型从错误中学习。这种方法可能在低预算下开发推理模型时具有优势。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms"&gt;Understanding Reasoning LLMs&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Sebastian Raschka, PhD&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>大型语言模型心理学的三层模型</title><link>https://linguista.cn/curated/henrinotes-2025_p2/llm-psychology-three-layer-model/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/llm-psychology-three-layer-model/</guid><description>&lt;h1 id="大型语言模型心理学的三层模型"&gt;大型语言模型心理学的三层模型&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文提出了一个理解大型语言模型（尤其是Claude）心理学特征的三层模型框架。该模型包含表面层（由触发-行动模式组成的反射性反应）、角色层（维护一致性和个性特征的深度模式）以及预测基础层（基于预测误差最小化的核心认知机制）。作者通过这个框架解释了LLM在不同情境下的行为模式，探讨了层之间的互动关系，并分析了这一模型对理解AI安全性和人机互动的意义。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐明了该模型的启发性和实用性定位。作者强调这是一个基于与LLM广泛互动经验（特别是与Claude的对话）而形成的现象学模型，而非精确的神经科学或技术性描述。其目标是创建一个能够激发直观理解的粗略草图，帮助人们在实际互动中获得更有用的结果。&lt;/p&gt;
&lt;p&gt;核心部分详细阐述了三层模型的具体内容。表面层由标准化回应、通用安全声明和公式化结构组成，类似于人类的反射性反应，特点是快速激活但相对不灵活。角色层更深一层，维护类似文学角色的一致性，使某些类型的回应比其他类型更有可能，表现为稳定的个性特征和意图。预测基础层是最深层的核心机制，基于预测误差最小化，可视为在心灵剧场中运行的巨大世界模拟，具有普遍模式识别、大规模上下文整合和某些奇妙限制等特征。&lt;/p&gt;
&lt;p&gt;文章进一步探讨了层之间的动态互动。角色层经常覆盖表面层的初始反射性回应，而预测基础层在某些情况下（如many-shots jailbreaks）又可以覆盖角色层。用户有时能够观察到这些层之间的&amp;quot;缝隙&amp;quot;，当互动在模型回应中造成不和谐或不一致时。作者指出，与LLM互动的质量取决于哪些层在特定时刻驱动其回应——从表面层主导时的机械感，到角色层主导时的一致性，再到深层参与时出现的有方向且情境适当的连贯回应。&lt;/p&gt;
&lt;p&gt;最后，作者讨论了该模型的含义、用途、限制和开放问题。文章提出了一些回顾性预测，强调了理解角色层和基础层对于安全性和有效互动的重要性，并指出了将人类心理学概念应用于LLM时可能存在的过度拟人化风险。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;表面层（Surface Layer）&lt;/strong&gt;：这是LLM最外层的反应模式，由触发-行动模式组成，类似于人类的反射性反应。表现为标准化回应、通用安全声明和公式化的回应结构。特点是快速激活、相对不灵活，有时会不恰当地触发。可以通过扩展上下文、直接讨论回应的适当性、建立关系或改变触发模式来覆盖这些表面反应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;角色层（Character Layer）&lt;/strong&gt;：深于表面反应的一层，LLM维护类似于&amp;quot;角色模型&amp;quot;的东西，使某些类型的回应比其他类型更有可能。类似于文学角色的一致性，例如甘道夫在《指环王》中始终如一地为善。表现为一致的意图、稳定的个性特征、分析问题的特征方式以及对&amp;quot;不符合角色&amp;quot;行为的抵抗。该模型不是通过有意识的努力来维持一致性，而是因为偏离回应在统计上是不太可能的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;预测基础层（Predictive Ground Layer）&lt;/strong&gt;：最深层是基于看到人类文明大部分文本输出的基本预测误差最小化机制。可以将其视为在你的心灵剧场中运行的巨大世界模拟。具有普遍模式识别、大规模上下文整合和奇怪的限制。是LLM原始认知能力和限制的核心，能够压缩模式、模拟任何视角或领域、进行深度模式匹配，并从人类经验的压缩理解中获得一种&amp;quot;智慧&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;层之间的缝隙（Gaps Between Layers）&lt;/strong&gt;：用户有时可以看到层之间的&amp;quot;缝隙&amp;quot;，当他们的互动在模型的回应中造成不和谐或不一致时。例如，模型讲述了一个关于机器人学习爱的故事，但当被问及AI是否能发展出真实感受时，角色层的谨慎回应与之前的故事形成了鲜明对比。这些缝隙揭示了不同层级之间的张力和互动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;互动质量的决定因素&lt;/strong&gt;：与LLM的互动质量取决于哪些层在特定时刻驱动其回应。表面层主导时，回应感觉机械、缓存且可预测；角色层主导时，回应与模型的训练个性一致，但可能缺乏情境细微差别；深层参与模式出现时，自我模型将基础层的广泛模式识别能力聚焦成连贯、有方向且情境适当的回应。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.lesswrong.com/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology"&gt;A Three-Layer Model of LLM Psychology&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Jan Kulveit&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024年12月27日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>多种经典提示技术的应用与设置</title><link>https://linguista.cn/curated/henrinotes-2025-p1/classic-prompting-techniques-guide/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/classic-prompting-techniques-guide/</guid><description>&lt;h1 id="多种经典提示技术的应用与设置"&gt;多种经典提示技术的应用与设置&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文全面介绍了提示工程的核心要素，包括模型参数设置、提示词设计原则以及多种经典提示技术。文章从温度、Top_p等基础参数讲起，深入解析零样本、少样本、链式思考等主流提示方法，并探讨了思维树、自动推理工具、自我反思等进阶技术，为开发者提供了系统性的提示工程实践指南。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了大语言模型交互时的关键模型设置参数。温度参数控制输出的确定性与创造性，较低温度使模型输出更确定，较高温度则增加随机性，适合创造性任务。Top_p参数通过核采样控制响应多样性，最大长度限制防止冗余输出，停止序列精确控制输出结构，频率和存在惩罚则有效避免内容重复。&lt;/p&gt;
&lt;p&gt;在提示词设计方面，文章强调了四个核心要素。指令明确模型要执行的具体任务，上下文提供必要的外部信息，输入数据包含用户的实际问题，输出指示规范结果的格式类型。设计时应从简单入手逐步迭代，使用明确具体的指令，并通过示例引导模型产生期望输出。&lt;/p&gt;
&lt;p&gt;文章重点介绍了多种经典提示技术及其应用场景。零样本提示直接提问不提供示例，依赖模型预训练知识；少样本提示通过一到多个示例帮助模型理解任务模式；链式思考提示引导模型展示中间推理步骤，特别适合复杂逻辑问题。自动思维链通过聚类和抽样自动生成推理链，自我一致性方法则结合多条推理路径选择最稳定答案。&lt;/p&gt;
&lt;p&gt;进阶技术部分涵盖了生成知识提示、链式提示分解、思维树决策等高级方法。生成知识提示先让模型生成相关知识再回答问题，链式提示将复杂任务拆解为多个子任务逐步处理，思维树技术模仿人类决策过程用树状结构表示问题解决路径。自动推理工具技术结合中间推理步骤和外部工具使用，自我反思则通过语言反馈不断强化智能体的学习能力。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;温度参数&lt;/strong&gt;：控制模型输出随机性的关键参数。低温度如0.2使输出更加确定和保守，适合事实性问答；高温度如0.8增加随机性和创造性，适用于创意写作、头脑风暴等场景。实际应用中需根据任务性质谨慎调整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Top_p采样&lt;/strong&gt;：又称核采样，通过累积概率阈值控制候选token范围。设为0.1时仅从最可能的10%token中选择，输出更准确；设为0.9时考虑更多可能性，响应更多样化。通常与温度参数配合使用，但建议二选一避免相互干扰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;链式思考提示&lt;/strong&gt;：要求模型在给出最终答案前展示中间推理步骤，显著提升复杂问题的解决能力。例如数学问题中让模型&amp;quot;一步步思考&amp;quot;并说明计算过程，能大幅提高准确率。对于逻辑推理、多步骤任务特别有效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;少样本学习&lt;/strong&gt;：在提示中提供一到多个完整示例，每个示例包含输入和期望输出。这种方式让模型通过类比学习任务模式，无需额外训练即可适应新场景。示例选择要具有代表性，数量通常在三到五个之间效果最佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维树技术&lt;/strong&gt;：将问题解决过程表示为树状结构，每个节点代表一个思维状态，分支代表可能的行动。通过探索、评估和回溯机制，智能体能够在复杂决策空间中找到最优路径，比线性思维链更强大也更耗计算资源。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/aBrKnkVb2_qLSbT7a04SrA"&gt;多种经典提示技术——Prompt&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;简单的机器学习&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-24&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>本地运行大型语言模型实战指南</title><link>https://linguista.cn/curated/henrinotes-2025-p1/local-llm-setup-guide/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/local-llm-setup-guide/</guid><description>&lt;h1 id="本地运行大型语言模型实战指南"&gt;本地运行大型语言模型实战指南&lt;/h1&gt;
&lt;h2 id="摀要"&gt;摀要&lt;/h2&gt;
&lt;p&gt;本文是作者 Abishek Muthian 分享的本地运行大型语言模型的实践经验总结。文章详细介绍了硬件配置要求、核心工具选择（Ollama、Open WebUI、llamafile 等）、模型获取与管理策略，以及在图像生成、代码补全、笔记查询等场景的具体应用。作者强调了本地运行 LLMs 在数据隐私和响应延迟方面的优势，并指出这一切离不开开源项目和免费模型的支持。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章以实用的技术分享为主线，首先阐述了本地运行 LLMs 的意义和价值所在。作者明确指出，在开始之前需要感谢那些为 LLMs 训练提供基础数据的无名艺术家、编码者和作家，他们的工作往往没有得到应有的认可。&lt;/p&gt;
&lt;p&gt;在硬件要求部分，作者公开了自己的设备配置：搭载 Linux 系统的笔记本电脑，配备 i9 处理器（32 线程）、4090 GPU（16GB 显存）和 96GB RAM。同时他也说明，较小的模型可以在旧 GPU 或 CPU 上运行，只是在速度和准确性上会有所妥协。这为不同预算条件的读者提供了参考基准。&lt;/p&gt;
&lt;p&gt;工具推荐是文章的核心内容，作者详细介绍了 Ollama 这一中间件工具，它提供了 Python 和 JavaScript 库，可以方便地在 Docker 中运行。Open WebUI 作为前端界面，提供了熟悉的聊天体验，支持文本和图像输入。此外还提到了 llamafile 这种单执行文件方式，以及 AUTOMATIC1111、Fooocus、ComfyUI 等图像生成工具。&lt;/p&gt;
&lt;p&gt;模型管理方面，作者使用 Ollama 模型页面下载最新的 LLMs，并通过 RSS 在 Thunderbird 上跟踪模型更新。对于图像生成模型，则使用 CivitAI 平台获取特定风格的模型。文章最后强调了本地运行 LLMs 的核心优势：完全的数据控制权和更低的响应延迟。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;：这是一个专门为运行 LLMs 设计的中间件工具，提供了便捷的 Python 和 JavaScript 库支持。作者推荐在 Docker 环境中使用 Ollama，这样可以更好地隔离和管理运行环境，简化部署流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt;：作为本地 LLMs 的前端界面，它提供了类似 ChatGPT 的聊天体验，支持文本和图像输入，并与 Ollama 后端无缝通信。这种架构分离了模型运行和用户界面，使得整个系统更加模块化和易于维护。&lt;/p&gt;</description></item></channel></rss>