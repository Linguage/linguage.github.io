<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>混合专家 on Linguista</title><link>https://linguista.cn/tags/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/</link><description>Recent content in 混合专家 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 10 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/index.xml" rel="self" type="application/rss+xml"/><item><title>重新思考 MoE 技术及其在大模型中的应用</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/moe-technology-rethinking-large-language-models/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/moe-technology-rethinking-large-language-models/</guid><description>&lt;h1 id="重新思考-moe-技术及其在大模型中的应用"&gt;重新思考 MoE 技术及其在大模型中的应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统性地梳理了混合专家技术在大语言模型中的演进历程。作者从内部世界模型对齐的视角出发，深入分析了GShard、DeepSeek MoE、DeepSeek-V3等架构的技术特点，探讨了RPC-Attention和MoDE等创新方法，并最终从认知框架层面重新解读了MoE的本质——一种基于先验知识的跨范畴采样策略。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章开篇提出了一个关键观点：MoE大模型如果不能在内部世界模型上实现对齐和互换，其输出将呈现人格分裂的状态。作者将内部世界模型定义为&amp;quot;现实的共享统计模型&amp;quot;或&amp;quot;以概率为表征的丰富范畴&amp;quot;，这为后续的技术分析奠定了理论基础。&lt;/p&gt;
&lt;p&gt;在架构演进方面，文章详细剖析了四个关键发展阶段。GShard作为早期的MoE实现，通过轻量级标注API和XLA编译器扩展，实现了万亿级参数的支持。DeepSeek MoE针对GShard存在的知识混杂和冗余问题，创新性地提出了细粒度专家分段和共享专家隔离策略。DeepSeek-V3则进一步整合了MLA注意力机制和无辅助损失的负载均衡策略，在6710亿总参数中每个token仅激活370亿参数，实现了训练与推理的高效平衡。&lt;/p&gt;
&lt;p&gt;技术创新部分，文章介绍了RPC-Attention通过核PCA框架推导自注意力，提供了对数据污染具有鲁棒性的解决方案。MoDE则将MoE思想扩展到扩散模型，通过噪声调节路由和专家缓存机制，将推理成本降低了90%。&lt;/p&gt;
&lt;p&gt;最后，作者从认知科学角度重新诠释了MoE的本质，将其视为一种分布式采样策略，这一策略可以基于人类先验知识，在高维语言概率空间中实现跨范畴的智能推理。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;内部世界模型对齐&lt;/strong&gt;：这是作者对MoE技术提出的核心要求。内部世界模型指模型对现实的共享统计表征，当不同的专家模块在这个表征上实现对齐时，模型才能产生连贯一致的输出，而非人格分裂的结果。这一概念将技术问题提升到了认知科学的高度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;细粒度专家分段&lt;/strong&gt;：DeepSeek MoE的核心创新之一。通过在保持参数量不变的情况下，将FFN中间隐藏维度拆分为更细粒度的专家，使模型能够激活更多专家并实现更灵活的组合。这种设计让多样化的知识能够更精确地分配到专门的专家中，提升专精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;共享专家隔离&lt;/strong&gt;：针对知识冗余问题的解决方案。将某些专家设计为始终激活的共享专家，专门捕获跨上下文的共享知识，从而减少其他路由专家之间的冗余，提高整体参数效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跨范畴采样&lt;/strong&gt;：作者对MoE本质的哲学解读。MoE中的&amp;quot;专家&amp;quot;本质上是&amp;quot;特定范畴&amp;quot;的形象化表达，模型推理的过程是在高维语言概率空间的子空间中进行采样，而类比推理则是跨范畴的采样过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;噪声条件路由&lt;/strong&gt;：MoDE架构的关键机制。将路由器设计为噪声条件化的，使得在推理前可以预先计算每个噪声水平使用的专家，从而去除路由器，仅保留选定专家，显著提升网络效率。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/p7C9zXZRY5iE0Q-d8GRkaQ"&gt;重新思考 MoE&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>