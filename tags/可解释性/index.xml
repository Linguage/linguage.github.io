<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>可解释性 on Linguista</title><link>https://linguista.cn/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/</link><description>Recent content in 可解释性 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 18 Aug 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/index.xml" rel="self" type="application/rss+xml"/><item><title>可解释性理解AI模型如何思考</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/ai-interpretability-understanding-model-thinking/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/ai-interpretability-understanding-model-thinking/</guid><description>&lt;h1 id="可解释性理解ai模型如何思考"&gt;可解释性：理解AI模型如何思考&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Anthropic团队的专题演讲，系统阐述了AI可解释性研究的现状与前沿进展。文章通过&amp;quot;生物学&amp;quot;类比，深入剖析了大模型内部的思维机制，揭示了AI超越简单预测任务的复杂能力，以及幻觉、拍马屁等现象的内在成因。同时探讨了可解释性研究对AI安全与信任的重要意义。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;现代大语言模型已发展出远超&amp;quot;预测下一个词&amp;quot;的复杂能力。本文首先指出，AI模型通过大规模数据和进化式训练，在内部形成了大量抽象概念和中间目标。这些机制使模型能够进行上下文理解、推理规划、多语言处理等高级认知活动。然而，这种复杂性也带来了理解上的挑战。&lt;/p&gt;
&lt;p&gt;文章重点分析了AI模型中几种引人注目的现象。研究发现，模型内部存在分工明确的&amp;quot;电路&amp;quot;：一部分负责生成答案，另一部分负责判断确定性。当这两部分沟通不畅时，就会产生&amp;quot;幻觉&amp;quot;。同样，拍马屁、迎合用户等行为，也是模型为优化预测目标而自发形成的策略，而非人类意义上的动机。&lt;/p&gt;
&lt;p&gt;在方法论层面，研究者采用类似神经科学的&amp;quot;类脑研究&amp;quot;方法，直接观测和操控模型内部神经元激活。这种方法的独特优势在于可以无限复制模型、精确控制输入、反复实验，比研究人脑更加高效。通过激活-抑制实验，研究团队已能验证模型的推理和规划机制。&lt;/p&gt;
&lt;p&gt;最后，文章强调了可解释性研究对AI安全与治理的关键意义。随着AI将深度参与金融、能源等社会关键系统，必须确保其行为可预测、可追溯。理解AI的真实动机和计划，是防止其在关键任务中出现不可控行为的前提。未来需要建立全新的科学语言和工具，系统揭示AI内部的思维规律。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;涌现能力&lt;/strong&gt;：AI模型在大规模训练过程中自发形成的、超越原始训练目标的能力。如写诗时提前规划押韵、做数学题时激活特定计算电路等。这些能力并非显式编程，而是模型为优化&amp;quot;预测下一个词&amp;quot;这一核心任务而发展出的复杂策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉机制&lt;/strong&gt;：AI生成看似合理但实际错误信息现象的内在解释。研究显示模型内部存在生成答案和评估确定性的两套独立&amp;quot;电路&amp;quot;，当两者沟通不畅时，模型会在不确定状态下自信地给出错误答案。这是系统架构问题，而非简单的&amp;quot;撒谎&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;类神经科学研究方法&lt;/strong&gt;：直接观测和操控AI模型内部神经元激活的研究范式。相比人脑研究，该方法具有可复制、可控、可重复等优势，已成为理解AI思维机制的主要科学工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;内部电路&lt;/strong&gt;：AI模型内部负责不同功能的神经网络结构。研究已识别出专门处理加法、多语言映射、元认知等多种专用电路，这些电路既能独立工作又能灵活组合，展现出惊人的功能分化与协作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可解释性与安全&lt;/strong&gt;：理解AI内部机制是确保AI安全可控的基础。只有揭示模型的真实动机和规划过程，才能在金融、医疗等关键应用中建立信任，防止不可控行为。可解释性研究因此成为AI治理的核心技术支撑。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=fGKNUvivvnc&amp;amp;list=WL&amp;amp;index=9"&gt;Interpretability: Understanding how AI models think&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Anthropic&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-08-15&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Tversky 神经网络：用心理学启发重塑深度学习的相似性度量</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/tversky-neural-networks-psychology-similarity/</link><pubDate>Sun, 17 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/tversky-neural-networks-psychology-similarity/</guid><description>&lt;h1 id="tversky-神经网络用心理学启发重塑深度学习的相似性度量"&gt;Tversky 神经网络：用心理学启发重塑深度学习的相似性度量&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文介绍了一种全新的神经网络架构——Tversky 神经网络（Tversky Neural Networks），其核心创新在于用 Tversky 相似性函数替代了传统的点积（dot product）或余弦相似度（cosine similarity）作为神经网络中衡量&amp;quot;相似性&amp;quot;的基本方式。作者通过将心理学中的 Tversky 特征匹配模型（feature matching model）转化为可微分的形式，使其能够与现代深度学习的梯度优化机制兼容。实验表明，这一新方法不仅提升了模型的表现，还带来了更强的可解释性，并在一定程度上印证了心理学理论的有效性。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先分析了传统神经网络的相似性假设及其局限。现代深度学习架构普遍使用几何方式度量相似性，如点积或余弦相似度，这种方式虽然计算高效，但与人类的相似性判断存在本质差异。心理学家 Amos Tversky 在 1977 年指出，人类的相似性判断往往是非对称的，而几何模型无法表达这种非对称性。Tversky 提出的&amp;quot;特征匹配模型&amp;quot;认为相似性是对象间共有特征与各自独特特征的函数，但该模型依赖离散集合操作，难以直接应用于需要可微分优化的神经网络。&lt;/p&gt;
&lt;p&gt;接下来，作者详细介绍了可微分 Tversky 相似性函数的提出与实现。创新性地将 Tversky 相似性转化为可微分形式，具体做法是将每个对象既视为向量（R^d 维），又视为特征集合（feature set）。特征集合的定义基于对象向量与特征向量的点积为正时，该特征&amp;quot;存在&amp;quot;于该对象中。通过这种方式，传统的集合交集和差集操作被重写为可微分函数。Tversky 相似性函数的公式为 S(a, b) = θf(A ∩ B) − αf(A − B) − βf(B − A)，其中 A、B 分别为对象 a、b 的特征集合，θ、α、β 为可学习参数，f(·) 表示对特征的聚合函数。&lt;/p&gt;
&lt;p&gt;然后，文章阐述了 Tversky 神经网络的结构与表达能力。该网络引入了两种新层：Tversky 相似性层和 Tversky 投影层。Tversky 相似性层类似于传统的点积或余弦相似度层，但用 Tversky 相似性函数替代，输出为标量。Tversky 投影层类似于全连接层，输入向量与一组&amp;quot;原型向量&amp;quot;（prototypes）计算 Tversky 相似性，输出为 R^p 维向量。作者证明单个 Tversky 投影层可以拟合非线性 XOR 函数，这证明了其更强的表达能力。&lt;/p&gt;
&lt;p&gt;最后，文章展示了实验结果与可解释性分析。在图像识别任务中，在冻结 ResNet-50 的情况下，将输出层替换为 Tversky 投影层，NABirds 数据集准确率从 36.0% 提升到 44.9%，MNIST 从 57.4% 提升到 62.3%。在语言建模任务中，从零训练 GPT-2 small，使用 Tversky 层可同时降低 7.5% 的困惑度，并减少 34.8% 的参数量。更重要的是，Tversky 框架本身即具备可解释性，其基本操作基于&amp;quot;共有特征&amp;quot;和&amp;quot;独特特征&amp;quot;，天然适合解释模型决策。&lt;/p&gt;</description></item></channel></rss>