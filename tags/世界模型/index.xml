<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>世界模型 on Linguista</title><link>https://linguista.cn/tags/%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B/</link><description>Recent content in 世界模型 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 03 Oct 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>Yann LeCun自监督学习与世界模型：AI未来的深度解析</title><link>https://linguista.cn/curated/henrinotes-2025-p1/yann-lecun-self-supervised-learning-world-models/</link><pubDate>Fri, 03 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/yann-lecun-self-supervised-learning-world-models/</guid><description>&lt;h1 id="yann-lecun自监督学习与世界模型ai未来的深度解析"&gt;Yann LeCun自监督学习与世界模型：AI未来的深度解析&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Yann LeCun在哈佛CMSA的讲座内容，系统梳理了其对当前AI技术瓶颈的分析及未来发展方向的主张。LeCun指出，尽管大语言模型在文本处理上表现突出，但在物理世界理解、常识推理和复杂规划方面仍远逊于人类甚至动物。他提出通过自监督学习、联合嵌入预测架构（JEPA）和世界模型来突破这些限制，强调AI需要从纯文本驱动转向多模态真实世界信号学习，并构建能够进行分层规划和零样本推理的认知架构。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;LeCun首先剖析了当前AI技术的核心局限。大语言模型虽然在特定任务上超越人类表现，但在日常通用智能方面——如灵巧操作、复杂推理与长期规划——仍无法与人类儿童或动物相比。人类和动物能够在极少数据下展现出惊人学习效率，这得益于他们与生俱来的世界模型和自监督学习能力。相比之下，当前主流的监督学习需要巨量标注数据，强化学习在复杂场景下效率极低，而文本驱动的自回归生成架构容易产生幻觉且缺乏全局规划能力。&lt;/p&gt;
&lt;p&gt;针对这些瓶颈，LeCun提出了基于JEPA和能量函数的新范式。传统生成模型试图直接预测未来的每个像素，这在高维连续信号面前几乎不可能，因为未来充满不确定性，简单的平均预测会导致模糊失真。JEPA的核心创新在于，它只在抽象表征空间中预测未来状态，将感知输入和输出都编码为低维表征，从而规避了高维输出的不可控问题。配合能量函数，系统能够灵活表达输入输出间的兼容性，通过优化搜索找到最佳匹配的预测结果，这种方式天然支持零样本推理，类似人类的理性决策过程。&lt;/p&gt;
&lt;p&gt;在实现路径上，LeCun强调需要构建分层的世界模型和规划系统。人类在规划复杂任务时采用分层策略，从宏观目标逐步分解到具体操作，这要求AI具备高度解耦、可组合的抽象表征。目前基于DINO等自监督学习方法已经在构建通用表征方面取得进展，而新一代视频理解模型甚至能自动检测物理不可能事件，显示出基本的常识推理能力。LeCun团队开发的机器人世界模型能够在未知环境下自主规划行动序列达成目标，展现出从纯文本AI向具备物理世界理解能力的通用智能进化的可能性。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;自监督学习&lt;/strong&gt;：这是LeCun认为未来AI突破的关键技术方向。与需要大量标注数据的监督学习不同，自监督学习让系统通过主动感知世界、发现其中的结构和关联来学习，更接近人类和动物的学习方式。当前技术路线主要包括对比学习和正则化方法，后者通过限制低能量分布体积来避免表征空间坍缩。实验表明，用自监督学习预训练的表征进行下游任务微调，性能已全面超越传统监督学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;JEPA（联合嵌入预测架构）&lt;/strong&gt;：这是LeCun提出的解决AI推理极限的核心架构。传统生成模型在像素空间直接预测未来几乎不可能，因为高维连续信号充满不确定性。JEPA的突破在于，它只在抽象表征空间中预测未来状态，将输入输出都编码为低维表征后再进行推理比较，一举规避了高维输出的不可控和模糊问题。配合能量函数，JEPA能够通过全局优化完成复杂推理，天然支持零样本迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;：这是人类和动物能够进行常识推理和高效规划的基础。婴儿很早就理解了物体的持存、支持、坠落等物理规则，建立起能够预测动作后果的世界模型。AI要达到类似的通用智能，也需要构建分层的世界模型——用多层抽象来理解现实，每一层屏蔽细节、聚焦对预测最有用的信息。科学的本质就是在寻找这些能用于预测的高效表征，而AI的发展同样需要这样的分层建模能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;能量函数&lt;/strong&gt;：这是JEPA框架中用于衡量输入输出兼容性的关键机制。给定输入，能量函数能够评估怎样的输出才是合理的，搜索输出的过程就变成了找使能量最低的输出。这种方式类似于人类的理性推理过程，能够支持零样本推理和更复杂的动态规划，比传统的单向预测或分类器更加灵活和强大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分层规划&lt;/strong&gt;：这是人类处理复杂任务的高效策略，也是AI需要具备的能力。人类计划从纽约去巴黎时，不是一步步规划到每个动作细节，而是先定宏观目标，再逐级分解到具体操作。AI要做到类似的分层规划，需要拥有高度解耦、可组合的抽象表征，不同层次捕捉不同时间尺度和抽象程度的信息。目前这个问题在AI领域仍未完全解决，但已有研究开始探索如何让AI自主发现有用的抽象层次。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=yUmDRxV0krg"&gt;Yann LeCun | Self-Supervised Learning, JEPA, World Models, and the future of AI&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Yann LeCun&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;主办方&lt;/td&gt;
 &lt;td&gt;Harvard CMSA&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>大型语言模型与世界模型以奥赛罗游戏为例的探讨</title><link>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-othello-case/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-othello-case/</guid><description>&lt;h1 id="大型语言模型与世界模型以奥赛罗游戏为例的探讨"&gt;大型语言模型与世界模型：以奥赛罗游戏为例的探讨&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文通过奥赛罗游戏（Othello）的经典案例，深入探讨大型语言模型是否能够通过训练数据自发形成对世界的抽象模型。研究者训练了名为OthelloGPT的Transformer网络，通过探针技术分析其内部激活，发现该模型确实编码了棋盘状态信息。然而，关于这种编码是否构成真正的&amp;quot;世界模型&amp;quot;仍存在争议，文章进一步分析了线性与非线性探针的对比结果，以及对人类与机器模型构建方式的思考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章以奥赛罗游戏为实验平台，介绍了OthelloGPT的训练过程：该模型在2000万种合法棋步序列上进行训练，仅学习棋步合法性而无关策略优劣，训练后在预测合法棋步上达到近乎完美的表现。研究者通过&amp;quot;探针&amp;quot;技术——一种用于解码Transformer内部激活的分类器——来分析模型是否形成了世界模型。&lt;/p&gt;
&lt;p&gt;实验过程中出现了有趣的转折：最初的线性探针无法有效预测棋盘状态，而非线性探针虽能达到98%的准确率，却可能归功于探针本身的能力而非Transformer的编码。Neel Nanda等研究者通过调整探针分类方式，将分类从&amp;quot;黑、白、空&amp;quot;改为&amp;quot;我的、你的、空&amp;quot;，使线性探针在第7层激活上的准确率提升至99.5%，这为OthelloGPT确实编码了棋盘状态提供了有力证据。&lt;/p&gt;
&lt;p&gt;然而，高准确率的探针结果并不等同于&amp;quot;世界模型&amp;quot;的存在。文章指出，OthelloGPT的内部表示更像是一个&amp;quot;启发式规则集&amp;quot;——某些神经元的激活代表非常具体的规则，这些规则能产生正确预测但缺乏抽象性和泛化能力。真正的世界模型应该具有连贯性和抽象性，能够应对未曾见过的情境。&lt;/p&gt;
&lt;p&gt;文章最后从认知科学角度对比了人类与机器的模型构建方式。人类受到工作记忆、处理速度和能量限制的约束，这些限制迫使我们形成更抽象、更具泛化的内部模型。机器可能也需要类似的限制和挑战，才能形成真正抽象的模型，从而更好地泛化到新情境中。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;：指智能体内部形成的对外部世界的抽象表征，它不仅仅是对输入-输出模式的记忆，而是对世界运作方式的理解和模拟。真正的世界模型具有抽象性和泛化能力，能够处理未曾遇到的情况。在OthelloGPT的案例中，尽管模型编码了棋盘状态，但这是否构成真正的世界模型仍存争议。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;探针技术&lt;/strong&gt;：一种用于分析神经网络内部表征的技术，通过训练分类器来预测网络内部激活所编码的信息。线性探针简单直接，能够揭示网络本身的编码能力；非线性探针功能更强大，但可能掩盖网络的真实编码情况，因其自身具有强大的信息提取能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启发式规则集&lt;/strong&gt;：与抽象的世界模型相对，指由大量具体规则组成的知识表示方式。在OthelloGPT中，某些神经元可能编码了非常具体的规则（如&amp;quot;当某个位置被占据时采取某种行动&amp;quot;），这些规则能产生正确预测，但缺乏抽象性和泛化能力，无法应对规则之外的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;认知限制与抽象化&lt;/strong&gt;：人类在构建模型时受到工作记忆容量、处理速度和能量消耗等限制，这些限制反而促使我们形成更抽象、更具泛化的内部模型。这为AI研究提供了启示：或许需要给机器模型添加类似的限制，促使其发展出真正的抽象理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表征与理解的区别&lt;/strong&gt;：神经网络能够表征信息（如OthelloGPT编码了棋盘状态），并不等同于它理解了这些信息背后的因果关系和抽象结构。这是当前AI研究面临的核心挑战之一——如何让模型从数据记忆走向真正的理解。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://aiguide.substack.com/p/llms-and-world-models-part-2"&gt;LLMs and World Models, Part 2&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;AI Guide&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未标注&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>大型语言模型与世界模型第一部分LLMs如何理解它们的世界</title><link>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-part-1/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/llms-world-models-part-1/</guid><description>&lt;h1 id="大型语言模型与世界模型第一部分llms-如何理解它们的世界"&gt;大型语言模型与世界模型（第一部分）：LLMs 如何理解它们的&amp;quot;世界&amp;quot;&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文由 Melanie Mitchell 撰写，深入探讨了大型语言模型（LLMs）是否发展出了类似人类的&amp;quot;世界模型&amp;quot;以理解其运作的&amp;quot;世界&amp;quot;。文章回顾了早期机器学习系统的脆弱性问题，介绍了世界模型的概念定义与分类方法，并围绕 LLMs 是否真正具备世界模型能力展开了学术界的重要辩论。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先通过多个经典案例揭示了早期 AI 系统的根本局限性——它们依赖于训练数据中的启发式规则和表面特征，而非真正的概念理解。从皮肤病变分类错误地依赖尺子存在，到语言模型仅凭词汇重叠判断逻辑关系，再到强化学习系统在游戏设置微小变化下的性能崩溃，这些案例都指向同一个问题：缺乏对世界因果结构的理解。&lt;/p&gt;
&lt;p&gt;接着文章转向当前备受争议的话题——大型语言模型是否突破了这一局限。OpenAI 联合创始人 Ilya Sutskever 认为，通过预测下一个词的训练目标，LLMs 确实学习了世界的压缩表征，包括人类的情感和动机。然而，包括 Yann LeCun 在内的多位研究者对此表示强烈怀疑，认为仅靠语言训练无法达到真正的理解。2022 年的一项调查显示，NLP 研究者群体在这一问题上几乎呈现对半分的分裂态势。&lt;/p&gt;
&lt;p&gt;为了厘清这一辩论，文章详细梳理了&amp;quot;世界模型&amp;quot;的多种定义。从最基础的内部表征到保留因果结构的复杂模型，再到能够支持反事实推理的完整模拟器。MIT 教授 Jacob Andreas 提出了一个清晰的分类框架，从静态查找表、地图、机械天体仪到完整的模拟器，每种类型代表了对世界理解的不同深度。人类正是通过这样的世界模型，才能快速理解复杂场景、预测因果关系并规划行动。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;世界模型&lt;/strong&gt;：指智能体内部形成的、对外部世界的压缩且可模拟的表征，它不仅能够存储信息，还能捕捉世界的因果结构，支持预测、规划和回答反事实问题。人类的世界模型使我们能够在瞬间理解街景照片中的复杂场景，推断行为者的意图和可能的后续发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启发式规则与表面特征&lt;/strong&gt;：早期机器学习系统依赖的捷径思维，它们通过发现训练数据中的统计关联来解决问题，但这种方式缺乏真正的理解。就像皮肤病变分类器记住&amp;quot;尺子=恶性&amp;quot;这样的关联，当环境变化时就会失效，因为系统并不理解尺子和病变之间的真实关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;情境模型&lt;/strong&gt;：LLMs 可能具备的一种中间层次世界模型，能够跟踪文本中的行为者、状态和动作变化。这类似于一个机械天体仪，可以模拟特定场景中的动态过程，但可能缺乏对更广泛世界因果知识的整合。目前尚不清楚 LLMs 的情境模型能否推广到训练数据之外的全新场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因果模拟模型&lt;/strong&gt;：世界模型的最高层次，能够回答复杂的&amp;quot;如果-那么&amp;quot;类型反事实问题，需要对世界的深层因果结构有精确理解。目前缺乏证据表明 LLMs 具备这种能力，这是判断它们是否真正&amp;quot;理解&amp;quot;世界的关键检验标准。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://aiguide.substack.com/p/llms-and-world-models-part-1"&gt;LLMs and World Models, Part 1 - How do Large Language Models Make Sense of Their &amp;ldquo;Worlds&amp;rdquo;?&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Melanie Mitchell&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未明确说明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>