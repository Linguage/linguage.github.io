<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>批次不变内核 on Linguista</title><link>https://linguista.cn/tags/%E6%89%B9%E6%AC%A1%E4%B8%8D%E5%8F%98%E5%86%85%E6%A0%B8/</link><description>Recent content in 批次不变内核 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 13 Sep 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%89%B9%E6%AC%A1%E4%B8%8D%E5%8F%98%E5%86%85%E6%A0%B8/index.xml" rel="self" type="application/rss+xml"/><item><title>击败大模型推理非确定性 Thinking Machines Lab批次不变内核突破详解</title><link>https://linguista.cn/infos/tldrcards/henrinotes-2025-p1/defeating-llm-inference-nondeterminism-batch-invariant-kernel/</link><pubDate>Sat, 13 Sep 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes-2025-p1/defeating-llm-inference-nondeterminism-batch-invariant-kernel/</guid><description>&lt;h1 id="击败大模型推理非确定性-thinking-machines-lab批次不变内核突破详解"&gt;击败大模型推理非确定性 Thinking Machines Lab批次不变内核突破详解&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文聚焦由OpenAI前CTO米拉·穆拉蒂创立的Thinking Machines Lab发布的最新突破性研究。该团队发现大模型推理输出不可复现的根本原因在于GPU批次归约顺序变化引发的浮点数误差累积，并提出&amp;quot;批次不变内核&amp;quot;（Batch-Invariant Kernel）解决方案，通过固定关键操作的归约顺序，以可控的性能损耗换取推理结果的完全一致性，对金融、医疗及强化学习等高可靠性场景具有里程碑意义。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了Thinking Machines Lab的特殊背景——这家公司在2025年7月尚未推出产品便完成20亿美元种子轮融资，估值高达120亿美元。其首篇技术博客即直面一个困扰AI开发者的核心问题：即使输入和随机种子完全固定，大模型推理仍会产生不同输出。实验显示1000次推理竟可得出80种不同结果，且分歧点之前的内容完全一致。&lt;/p&gt;
&lt;p&gt;随后文章深入剖析了非确定性的根本原因。业界通常将其归咎于GPU并发或浮点数误差，但研究发现真正根源在于浮点数的&amp;quot;非结合性&amp;quot;特质与GPU内部批次归约顺序的不固定。当服务器按负载将请求打包成不同大小的批次时，内核归约策略随之变化，导致浮点运算的累计顺序不同，最终产生输出分歧。&lt;/p&gt;
&lt;p&gt;文章核心部分详细解读了&amp;quot;批次不变内核&amp;quot;解决方案的具体实现，分别在RMSNorm、矩阵乘法和注意力机制三个关键环节强制固定归约顺序。实验验证表明开启该方案后1000次同条件推理结果完全一致，优化后性能损耗控制在20%以内。&lt;/p&gt;
&lt;p&gt;最后文章探讨了这一成果的深远影响，特别是在在线策略强化学习领域——未启用批次不变内核时RL训练318步即出现奖励崩溃，启用后奖励表现稳定，为RL训练提供了全新突破点。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;浮点数非结合性（Floating-Point Non-Associativity）&lt;/strong&gt;：浮点数运算中 (a+b)+c ≠ a+(b+c)，这一数学特性是大模型推理非确定性的底层根因。在矩阵乘法、RMSNorm、注意力机制等大量浮点操作中，运算顺序的微小变化会导致精度损失逐步累积，最终在关键token处引发输出分歧。理解这一特性是认识大模型推理不可复现问题的关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;批次不变内核（Batch-Invariant Kernel）&lt;/strong&gt;：Thinking Machines Lab提出的核心解决方案，其设计思路是无论批次大小如何变化，始终以固定顺序完成关键归约操作。具体措施包括：RMSNorm强制数据并行、矩阵乘法禁用Split-K策略并统一张量指令尺寸、注意力机制固定KV拆分块大小为256并统一预填充和解码阶段的数据格式。该方案以约20%的性能损耗换取推理结果的100%一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;批次归约顺序（Batch Reduction Order）&lt;/strong&gt;：GPU在处理不同大小的批次时，会自动选择不同的内核调度策略——小批次采用单核完整归约，大批次采用并行分割归约。这种策略差异直接改变了浮点运算的执行顺序，是导致相同输入产生不同输出的直接原因。固定归约顺序是实现推理确定性的核心工程手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可重复性优先原则（Reproducibility-First Principle）&lt;/strong&gt;：文章提出的框架性思维——AI系统的产业级落地应将可重复性作为第一优先级，工程团队需从GPU指令级行为出发审视底层内核，而非仅停留在参数调优层面。这一原则对医疗、金融等高可靠性场景尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在线策略强化学习的确定性保障&lt;/strong&gt;：批次不变内核使训练与推理输出完全一致，KL散度长期保持为零。实验对比显示，未启用时RL训练318步奖励崩溃，启用后无须复杂离线校正即可保持奖励稳定。这解决了大模型用作在线策略训练&amp;quot;不可落地&amp;quot;的核心障碍，是强化学习领域的重要突破。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=eYq6Zc1M6pU"&gt;【人工智能】击败大模型推理的非确定性 Thinking Machines Lab突破详解&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;最佳拍档（大飞）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-09-13&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>