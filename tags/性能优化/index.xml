<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>性能优化 on Linguista</title><link>https://linguista.cn/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</link><description>Recent content in 性能优化 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 22 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>静态搜索树比二分搜索快40倍</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/static-search-trees-faster-binary-search/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/static-search-trees-faster-binary-search/</guid><description>&lt;h1 id="静态搜索树比二分搜索快40倍"&gt;静态搜索树：比二分搜索快40倍&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文详细介绍了如何通过一系列优化技术，将静态有序数组的搜索性能从传统二分搜索的1150ns优化至27ns，实现超过40倍的性能提升。文章从缓存行优化、SIMD向量化、预取技术、内存布局改进等多个维度展开，深入探讨了S+树（S-tree）的设计与实现，并展示了在生物信息学基因索引等实际场景中的应用价值。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先明确了问题场景：给定一个静态的排序32位无符号整数数组，需要高效查询返回不小于给定值的最小元素。这是一个典型的静态搜索问题，常见于后缀数组索引、数据库索引等场景。作者以生物信息学中的DNA索引为动机背景——单个人类基因组包含30亿碱基对，需要高效的静态数据结构进行快速查询。&lt;/p&gt;
&lt;p&gt;接着，文章系统性地介绍了优化路径。第一步是理解硬件特性：CPU以64字节缓存行为单位工作，传统二分搜索和Eytzinger布局在每个缓存行中只使用一个值，内存带宽利用率极低。第二步是优化节点内的查找操作：从线性扫描开始，逐步引入自动向量化、尾随零计数、Popcount、手动SIMD等技术。第三步是优化搜索过程本身：通过批处理查询、预取下一节点、减少指针算术开销、交错处理多层查询等手段提升吞吐量。&lt;/p&gt;
&lt;p&gt;在树布局优化方面，文章探讨了左树布局（存储左子树最大值而非右子树最小值）、反向存储、全数组存储等方案，并确定了节点大小B=15（分支因子16）为较优选择。更进一步，文章介绍了前缀分区技术：将值按前缀分为多个部分，每个部分构建独立搜索树，包括完全布局、紧凑子树、混合方案、重叠树等变体。在人类基因组数据上的测试显示，分区方法可以进一步提升性能。&lt;/p&gt;
&lt;p&gt;最终，通过多线程并行，查询时间可从27ns进一步降至7ns，但此时性能瓶颈转移到了总RAM带宽。文章总结了完整的优化路径，并展望了分支搜索、插值搜索、数据压缩、范围查询等未来工作方向。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Eytzinger布局&lt;/strong&gt;：一种数组重排策略，将完全二叉树的节点按广度优先顺序存储，使得根节点位于中间位置，其子节点分别位于1/4和3/4位置，以此类推。这种布局使得二分搜索路径上的元素在内存中更加紧凑，提高了缓存命中率，是后续S+树优化的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;S+树（S-tree）&lt;/strong&gt;：B树与Eytzinger布局的混合数据结构。内部节点存储多个键值（通常B=15，分支因子16）用于分支选择，所有实际值存储在叶子节点中并在内部节点重复。它兼顾了B树的高分支因子（减少树高度）和Eytzinger布局的缓存友好性，是静态搜索场景下的高效选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SIMD向量化&lt;/strong&gt;：单指令多数据流技术的应用。在节点内查找时，不是逐个比较，而是使用AVX2/AVX-512等指令集同时比较16个或更多值，将比较结果转为位掩码，通过尾随零计数或popcount指令快速定位目标位置。这充分利用了现代CPU的并行计算能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;预取与批处理&lt;/strong&gt;：CPU在执行当前操作的同时，提前从内存加载即将需要的数据到缓存中。通过同时处理多个查询（批处理），CPU可以在等待一个查询的内存访问完成时，继续处理其他查询，从而隐藏内存延迟，提高整体吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前缀分区&lt;/strong&gt;：将整个值空间按高位前缀分为多个区间，每个区间构建独立的搜索树。这样第一层只需找到正确的分区树，后续搜索在较小的树上进行。对于非均匀分布的数据，可以通过前缀映射表平衡各分区大小，进一步提升缓存局部性。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://curiouscoding.nl/posts/static-search-tree/"&gt;Static search trees: 40x faster than binary search&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Curious Coding&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>本地大模型硬件配置与性能需求完全指南</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/local-llm-hardware-performance-guide/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/local-llm-hardware-performance-guide/</guid><description>&lt;h1 id="本地大模型硬件配置与性能需求完全指南"&gt;本地大模型硬件配置与性能需求完全指南&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文深入探讨本地部署大语言模型的硬件需求分析，从模型基础架构出发，详细解析Transformer架构的工作原理、模型推理的预填充与自回归解码两个阶段，以及参数量与存储需求的计算方法。文章重点分析了内存优化技术（包括FP16、INT8、INT4量化）和推理速度的影响因素，并针对CPU和GPU选择提供了实用的性价比建议，帮助读者在硬件选购时做出明智决策。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了现代大语言模型的基础架构，指出当前主流模型均采用仅有解码器的Transformer架构。作者通过简洁的说明帮助读者理解模型内部工作原理，为后续的硬件需求分析奠定基础。&lt;/p&gt;
&lt;p&gt;接着，文章详细阐述了模型推理的两个核心阶段：预填充阶段处理输入提示词，自回归解码阶段逐个生成输出token。这种两阶段的工作模式直接影响硬件配置的选择，因为不同阶段对计算资源和内存带宽的需求存在显著差异。&lt;/p&gt;
&lt;p&gt;在技术层面，文章深入探讨了模型参数量的计算方法和存储需求。以10B参数模型为例，作者详细说明了不同精度格式（FP32、FP16、INT8、INT4）下的存储空间需求，并介绍了量化技术如何在保持模型性能的同时显著降低内存占用。这些分析为读者提供了评估硬件配置能力的量化标准。&lt;/p&gt;
&lt;p&gt;最后，文章针对硬件选择提出了实用建议。作者强调了FP16算力和内存带宽作为关键性能指标的重要性，并推荐了不同预算下的性价比选择，包括M4 Mac mini和高性能NVIDIA GPU等方案。文章总结指出，选择本地大模型硬件配置需要综合考虑模型规模、推理速度需求和预算约束。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Transformer架构&lt;/strong&gt;：现代大语言模型的统一架构基础，采用仅有解码器的设计模式。这种架构通过自注意力机制处理序列数据，能够有效捕捉长距离依赖关系，是当前所有主流大模型（包括GPT系列、Llama等）的技术基石。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;预填充与解码&lt;/strong&gt;：模型推理的两个不同阶段。预填充阶段并行处理输入提示词，计算密集型；解码阶段逐个生成输出token，带宽需求高。这种差异导致硬件配置时需要在算力和内存带宽之间进行权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;量化技术&lt;/strong&gt;：通过降低模型参数精度来减少存储需求和加速推理的技术。从FP32到FP16可减少一半存储，进一步量化到INT8或INT4可在保持绝大部分性能的同时实现更大幅度的资源节省，是本地部署大模型的关键优化手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;内存带宽&lt;/strong&gt;：决定推理速度的关键因素，特别是在自回归解码阶段。每生成一个token都需要将全部模型参数从内存加载到计算单元，因此内存带宽往往比纯计算能力更重要，这也是GPU相比CPU在AI推理中的主要优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;算力与带宽的权衡&lt;/strong&gt;：硬件选择需要在FP16算力和内存带宽之间找到平衡。预填充阶段受算力限制，解码阶段受带宽限制，因此理想的硬件配置应该在两者都有较好表现，而不是单纯追求某一项指标。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://sspai.com/post/95262"&gt;本地大模型之路（二）：了解模型能力与性能需求，让硬件选购恰到好处&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;yzlnew&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-30&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>