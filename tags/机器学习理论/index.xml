<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习理论 on Linguista</title><link>https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/</link><description>Recent content in 机器学习理论 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 20 Aug 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><item><title>AI学习理论的意外革命</title><link>https://linguista.cn/curated/henrinotes_2025_p4/ai-learning-theory-accidental-revolution/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p4/ai-learning-theory-accidental-revolution/</guid><description>&lt;h1 id="ai学习理论的意外革命"&gt;AI学习理论的意外革命&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文讲述了人工智能领域一个颠覆性发现：大规模神经网络为何能成功学习，推翻了三百年来关于&amp;quot;模型过大必然过拟合&amp;quot;的理论。通过&amp;quot;彩票票据假说&amp;quot;（Lottery Ticket Hypothesis）和&amp;quot;双重下降&amp;quot;现象，研究者们发现，庞大的模型并非简单地记忆数据，而是在庞大的参数空间中寻找最优、最简洁的解决方案。这一发现不仅改变了AI模型的设计思路，也重新定义了我们对智能和学习本质的理解。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了机器学习领域长期遵循的&amp;quot;偏差-方差权衡&amp;quot;原则，这一理论认为模型过于复杂会导致过拟合，从而丧失泛化能力。按照传统理论，神经网络因参数众多，被认为极易陷入记忆训练数据的陷阱，无法有效应对新数据。这一理论主导了整个研究领域数百年，研究者们专注于小型、受控模型，避免扩大模型规模。&lt;/p&gt;
&lt;p&gt;然而，2019年一批研究者选择无视传统警告，继续扩大模型规模到理论所称的&amp;quot;危险区&amp;quot;，结果发现了意想不到的&amp;quot;双重下降&amp;quot;现象：模型误差在因过拟合上升后，会再次大幅下降并超越过拟合的限制。这一发现迅速改变了行业风向，各大科技公司投入数十亿美元打造更大规模的模型，&amp;ldquo;越大越好&amp;quot;成为新信条。&lt;/p&gt;
&lt;p&gt;为了解释这一现象，MIT的研究者于2018年提出了&amp;quot;彩票票据假说&amp;rdquo;。该假说指出，大网络的成功不是因为学会了复杂解法，而是因为提供了更多机会去寻找简单解法。每组随机初始化的权重都是一张潜在的&amp;quot;彩票&amp;quot;，而训练过程就是筛选出最优初始化的子网络。这一发现调和了经验与经典理论，表明大模型不是在记忆数据，而是在庞大参数空间中找到最简洁的解决方案。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;偏差-方差权衡&lt;/strong&gt;：这是机器学习领域的传统铁律，认为模型需要在拟合数据与保持简洁之间取得平衡。模型太简单会遗漏重要规律（欠拟合），模型太复杂则会记忆噪声（过拟合），导致泛化能力丧失。这一理论主导了机器学习研究三百年，但被AI领域的最新发现所挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双重下降现象&lt;/strong&gt;：这是2019年研究者发现的意外现象。当模型规模扩大到理论所称的&amp;quot;过拟合危险区&amp;quot;时，误差曲线出现两次下降——第一次是正常的学习下降，第二次是在过了过拟合区后的意外下降。这一现象与偏差-方差分析的传统智慧相悖，表明大模型在庞大参数空间中能找到更优解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票票据假说&lt;/strong&gt;：由MIT研究者于2018年提出，该假说认为大网络中隐藏着&amp;quot;中奖票据&amp;quot;——极小的子网络能达到完整网络的性能。大网络的成功不是因为它学会了复杂解法，而是因为提供了更多机会去寻找简单解法。每组随机初始化的权重都是一张彩票，训练过程就是筛选出最优初始化的子网络，其余则被淘汰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;奥卡姆剃刀原理&lt;/strong&gt;：这一简约原理指出最简单的解释通常是最优的。彩票票据假说揭示了大模型并非违背这一原理，而是更高效地找到这些简单解释。大模型的庞大参数空间为寻找最简解提供了更多可能，而非存储复杂解法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;科学进步模式&lt;/strong&gt;：这一发现揭示了科学突破的本质——最重要的发现往往源于突破理论边界的勇气。几十年来研究者因理论限制而不敢扩大模型规模，直到有人勇于质疑假设、进行实证测试，才带来了颠覆性的发现。这与历史上大陆漂移理论、量子力学等的突破模式相似。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://nearlyright.com/how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/"&gt;How AI researchers accidentally discovered that everything they thought about learning was wrong&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;NearlyRight&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-08-18&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>