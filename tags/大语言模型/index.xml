<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>大语言模型 on Linguista</title><link>https://linguista.cn/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link><description>Recent content in 大语言模型 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 27 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>智能体（Agent）——Google白皮书解读</title><link>https://linguista.cn/rosetta/technology/google-whitepaper-ai-agents/</link><pubDate>Tue, 27 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/google-whitepaper-ai-agents/</guid><description>&lt;h1 id="智能体agentgoogle白皮书解读"&gt;智能体（Agent）——Google白皮书解读&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文为Google发布的智能体白皮书，系统阐述了生成式AI智能体的核心概念与架构。智能体由模型、工具和编排层三大组件构成，能够自主观察环境、调用外部工具并采取行动以实现目标。文章详细介绍了ReAct、思维链、思维树等推理框架，并对比了智能体与传统模型的本质差异，为构建生产级智能体应用提供了完整的技术参考。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;智能体（Agent）&lt;/strong&gt;：一种超越独立生成式AI模型能力的应用程序，能够通过观察环境、调用工具并自主决策来实现目标&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;认知架构&lt;/strong&gt;：智能体内部由模型、工具和编排层组成的系统结构，负责信息接收、推理规划、执行行动和反馈调整的循环过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReAct框架&lt;/strong&gt;：一种结合推理（Reason）与行动（Action）的提示工程策略，使模型能够交替进行思考和工具调用，逐步解决复杂问题&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;编排层&lt;/strong&gt;：管理智能体信息处理循环的核心层，负责维护记忆、状态、推理和规划，持续运行直到目标达成&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工具（Tools）&lt;/strong&gt;：弥合智能体内部能力与外部世界差距的桥梁，使模型能够访问实时数据和外部服务，实现如数据库查询、API调用等操作&lt;/p&gt;
&lt;p&gt;「Google白皮书」智能体（Agent）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单位：Google&lt;/li&gt;
&lt;li&gt;作者：Julia Wiesinger, Patrick Marlow 和 Vladimir Vuskovic&lt;/li&gt;
&lt;li&gt;原文：&lt;a href="https://www.kaggle.com/whitepaper-agents"&gt;Agent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://cdn-mineru.openxlab.org.cn/extract/7b099934-d40b-403d-8804-2269062d890e/1e617186aa14acffc2355bf30615e8ccb40d24be539626e598f7d93f16c094df.jpg" alt=""&gt;&lt;/p&gt;
&lt;h2 id="目录"&gt;目录&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;1. 引言 4
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;2. 什么是智能体？ 5
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;3. 模型 6
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;4. 工具 7
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;5. 编排层 7
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;6. 智能体 vs. 模型 8
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;7. 认知架构：智能体如何运作 8
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;8. 工具：我们通往外部世界的钥匙 12
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;9. 扩展 13
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;10. 扩展示例 15
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;11. 函数 18
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;12. 用例 21
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;13. 函数示例代码 24
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;14. 数据存储 27
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;15. 实现与应用 28
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;16. 工具回顾 32
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;17. 通过目标学习提升模型性能 33
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;18. 使用 LangChain 快速开始智能体开发 35
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;19. 使用 Vertex AI 智能体进行生产应用 38
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;20. 总结 40
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;21. 尾注 42
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种将推理、逻辑和对外部信息的访问结合起来，并全部连接到生成式 AI 模型的方式，引出了智能体的概念。&lt;/p&gt;</description></item><item><title>下半场——人工智能从解决问题转向定义问题</title><link>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</guid><description>&lt;h1 id="下半场人工智能从解决问题转向定义问题"&gt;下半场——人工智能从解决问题转向定义问题&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文提出人工智能正处于中场休息阶段。上半场的核心是开发新的训练方法和模型，而下半场的焦点将从解决问题转向定义问题，评估将变得比训练更重要。作者通过强化学习的视角，阐述了语言预训练、规模化和推理行动三大要素如何构成一套通用配方，使得RL终于具备了泛化能力，从而彻底改变了AI研究的游戏规则。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习泛化&lt;/strong&gt;：指RL通过语言预训练获得先验知识，结合推理行动机制，能够跨任务迁移而非局限于单一环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReAct范式&lt;/strong&gt;：将推理作为一种特殊行动加入RL环境的行动空间，使智能体在决策前进行语言化思考，从而提升泛化能力&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI评估转向&lt;/strong&gt;：下半场的核心命题不再是能否训练模型解决某个问题，而是应该训练AI做什么以及如何衡量真正的进展&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配方标准化&lt;/strong&gt;：大规模语言预训练加规模化加推理行动的组合已将基准提升过程工业化，使得针对特定任务的方法创新价值大幅降低&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;先验知识&lt;/strong&gt;：在RL三要素中，语言预训练提供的先验被证明可能比算法和环境更为关键，这颠覆了传统RL研究的优先级排序&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心观点：我们正处于人工智能的中场休息。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文连接：&lt;a href="https://ysymyth.github.io/The-Second-Half/"&gt;The Second Half&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;作者：&lt;a href="https://ysymyth.github.io/"&gt;Shunyu Yao&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;几十年来，人工智能的发展主要围绕着开发新的训练方法和模型。这确实取得了成效：从在国际象棋和围棋比赛中击败世界冠军，到在SAT和律师资格考试中超越大多数人类，再到赢得IMO和IOI金牌。在这些载入史册的里程碑——深蓝（DeepBlue）、AlphaGo、GPT-4以及o系列模型——背后，是人工智能方法的根本性创新：搜索、深度强化学习（deep RL）、规模化（scaling）和推理（reasoning）。随着时间的推移，一切都在变得更好。&lt;/p&gt;
&lt;p&gt;那么，现在突然发生了什么变化？&lt;/p&gt;
&lt;p&gt;三个词概括：强化学习（RL）终于奏效了。更准确地说：强化学习终于具备了泛化能力。在经历了数次重大的弯路和一系列里程碑式的积累之后，我们终于找到了一种行之有效的配方，能够利用语言和推理来解决各种各样的强化学习任务。哪怕就在一年前，如果你告诉大多数人工智能研究者，单一的配方就能处理软件工程、创意写作、IMO级别的数学、键鼠操作以及长篇问答——他们会嘲笑你在痴人说梦。这些任务中的每一项都极其困难，许多研究者整个博士生涯都只专注于其中一个狭窄的领域。&lt;/p&gt;
&lt;p&gt;然而，这确实发生了。&lt;/p&gt;
&lt;p&gt;那么接下来会发生什么？人工智能的下半场——从现在开始——将把焦点从&lt;strong&gt;解决问题&lt;/strong&gt;转向&lt;strong&gt;定义问题&lt;/strong&gt;。在这个新时代，&lt;strong&gt;评估&lt;/strong&gt;变得比&lt;strong&gt;训练&lt;/strong&gt;更重要。我们不再仅仅问：“我们能否训练一个模型来解决X问题？”，而是问：“我们应该训练人工智能去做什么？以及我们如何衡量真正的进展？” 要在下半场取得成功，我们需要及时转变思维模式和技能组合，也许更接近于一个产品经理。&lt;/p&gt;
&lt;h2 id="上半场"&gt;上半场&lt;/h2&gt;
&lt;p&gt;要理解上半场，看看它的赢家就知道了。你认为迄今为止最具影响力的人工智能论文是哪些？&lt;/p&gt;
&lt;p&gt;我试着回答了斯坦福224N课程里的这个问题，答案并不令人意外：Transformer、AlexNet、GPT-3等等。这些论文有什么共同点？它们提出了一些根本性的突破，用以训练出更好的模型。而且，它们都通过在某些基准（benchmarks）上展示出（显著的）改进而成功发表。&lt;/p&gt;
&lt;p&gt;不过，还有一个潜在的共同点：这些“赢家”都是&lt;strong&gt;训练方法或模型&lt;/strong&gt;，而不是&lt;strong&gt;基准或任务&lt;/strong&gt;。即使是 arguably 最具影响力的基准 ImageNet，其引用量也不到 AlexNet 的三分之一。方法与基准的对比在其他地方甚至更为悬殊——例如，Transformer 的主要基准是 WMT’14，其研讨会报告的引用量约为1300次，而 Transformer 的引用量超过了16万次。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/first_half.png" alt="上半场"&gt;&lt;/p&gt;
&lt;p&gt;这描绘了上半场的游戏规则：专注于构建新的模型和方法，而评估和基准是次要的（尽管对于论文发表体系的运作是必要的）。&lt;/p&gt;
&lt;p&gt;为什么会这样？一个重要原因是，在人工智能的上半场，&lt;strong&gt;方法比任务更难、更令人兴奋&lt;/strong&gt;。从零开始创建一个新的算法或模型架构——想想反向传播算法、卷积网络（AlexNet）或 GPT-3 中使用的 Transformer 等突破——需要非凡的洞察力和工程能力。相比之下，为人工智能定义任务通常感觉更直接：我们只需将人类已经在做的任务（如翻译、图像识别或下棋）转化为基准。这并不需要太多的洞察力，甚至工程量也不大。&lt;/p&gt;
&lt;p&gt;方法也往往比单个任务更通用、适用范围更广，这使得它们尤为宝贵。例如，Transformer 架构最终推动了计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）等许多领域的进步——远远超出了它最初证明自己的那个单一数据集（WMT’14 翻译）。一个优秀的、简洁且通用的新方法可以在许多不同的基准上取得进展（hillclimb），因此其影响往往超越单个任务。&lt;/p&gt;
&lt;p&gt;这场游戏已经持续了几十年，催生了改变世界的想法和突破，这些都体现在各个领域基准性能的不断提升上。那为什么游戏规则会发生改变呢？因为这些想法和突破的积累，在&lt;strong&gt;创造一个解决任务的有效配方&lt;/strong&gt;方面，产生了质的变化。&lt;/p&gt;
&lt;h2 id="配方"&gt;配方&lt;/h2&gt;
&lt;p&gt;这个配方是什么？不出所料，它的成分包括：大规模语言预训练、规模（数据和计算能力），以及推理和行动（reasoning and acting）的思想。这些听起来可能像是你在旧金山每天都能听到的流行词，但为什么称它们为“配方”？&lt;/p&gt;
&lt;p&gt;我们可以通过**强化学习（RL）**的视角来理解这一点。RL 常被认为是人工智能的“终局之战”——毕竟，理论上 RL 保证能赢得游戏，而经验上也难以想象任何超人系统（如 AlphaGo）没有 RL 的参与。&lt;/p&gt;
&lt;p&gt;在 RL 中，有三个关键组成部分：&lt;strong&gt;算法（algorithm）、环境（environment）和先验（priors）&lt;/strong&gt;。很长一段时间里，RL 研究者主要关注算法（例如 REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO……）——即智能体学习方式的智力核心——而将环境和先验视为固定的或最简化的。例如，Sutton 和 Barto 的经典教科书几乎全是关于算法，而很少涉及环境或先验。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/rl_book.png" alt="RL 教科书"&gt;&lt;/p&gt;
&lt;p&gt;然而，在深度强化学习时代，环境在经验上变得非常重要：一个算法的性能往往高度依赖于其开发和测试的环境。如果你忽略环境，你可能会构建出一个只在玩具环境中表现出色的“最优”算法。那么，为什么我们不先弄清楚我们真正想要解决的环境，然后再找到最适合该环境的算法呢？&lt;/p&gt;
&lt;p&gt;这正是 OpenAI 最初的计划。它构建了 &lt;a href="https://openai.com/index/openai-gym-beta/"&gt;gym&lt;/a&gt;，一个包含各种游戏的标准 RL 环境，然后是 &lt;a href="https://openai.com/index/universe/"&gt;World of Bits 和 Universe 项目&lt;/a&gt;，试图将互联网或计算机变成一个游戏。这计划听起来不错，对吧？一旦我们将所有数字世界都变成环境，用智能 RL 算法解决它，我们就拥有了数字通用人工智能（digital AGI）。&lt;/p&gt;</description></item><item><title>AGI的未来是系统工程而非模型训练</title><link>https://linguista.cn/curated/henrinotes_2025_p4/agi-future-systems-engineering-not-models/</link><pubDate>Sun, 24 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p4/agi-future-systems-engineering-not-models/</guid><description>&lt;h1 id="agi的未来是系统工程而非模型训练"&gt;AGI的未来是系统工程而非模型训练&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文核心观点是通往人工通用智能的道路不在于继续扩大语言模型的规模，而在于构建由模型、记忆、上下文和确定性工作流组成的工程化系统。当前主流大模型已接近能力极限，单纯依靠堆算力和扩参数已无法带来质的突破。AGI的本质是一个系统工程问题，需要分布式系统、上下文管理、记忆服务、确定性与概率性结合的工作流，以及多模型协作的架构创新。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先指出了现有大语言模型已达到能力平台期。日常使用者能明显感受到其局限性：虽然能生成高质量文本，但在跨会话保持上下文、持久记忆、复杂多步推理等方面表现不佳。这种技术发展轨迹与半导体行业类似，当主频提升遇到物理极限后，行业转向多核架构带来新一轮创新。AI领域也正处于类似拐点，继续做大模型的边际收益正在递减。&lt;/p&gt;
&lt;p&gt;未来的关键问题不再是如何让模型更大，而是如何让系统更智能。这意味着需要从模型训练转向系统工程，关注模型之间的协作、信息流动和可靠性。AGI的突破不在于更大的Transformer，而在于能编排数百专用模型、跨会话保持上下文、围绕概率性组件执行确定性工作流，并在生产级规模下实现容错操作的分布式系统。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;上下文管理基础设施&lt;/strong&gt;：现有模型的注意力跨度仅为数千token，而人类的上下文跨度可以覆盖数年甚至一生。这种差距不仅是数量级的，更是质量上的。理想的上下文管理系统需要具备按需检索和过滤相关信息、构建并维护可持续演化的世界模型、跨领域桥接上下文、处理冲突信息等能力。这需要从简单的向量检索升级到可操作知识图谱，实现实时更新、查询和推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;记忆服务&lt;/strong&gt;：现有LLM没有真正的记忆，只能通过提示工程和上下文填充模拟记忆。AGI需要具备真正的记忆系统，能够在新证据出现时更新信念、跨多次体验整合信息形成一般性原则、忘记无关细节但避免灾难性遗忘、生成关于信息可靠性和来源的元知识。这不仅是数据库持久化，更是类人记忆系统，使用越多越牢固，久未使用则逐渐遗忘，遇新理解时能重组记忆结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;确定性工作流与概率性组件结合&lt;/strong&gt;：AGI的突破点在于用确定性框架包裹概率性组件。类似编译器的设计理念，整体流程可预测，但某些步骤可用启发式或概率优化。理想系统应根据问题特征将任务路由到合适的专用求解器、执行多步工作流并具备回滚与恢复能力、在接受概率性结果前进行确定性校验、以可预测方式组合各能力同时保留生成式模型的灵活性。不确定性应成为系统设计的一级公民。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;专用模型的模块化协作&lt;/strong&gt;：未来不是单一模型包打天下，而是数百数千个专用模型协同工作。语言模型在语言任务上表现优异，但在符号运算、视觉空间推理、时间规划、持续目标行为等方面表现不佳。应构建能够将问题路由到最适合的专用模型的系统、整合不同模型输出为统一解决方案、保持各组件兼容性允许独立进化、在个别模型失效时优雅处理不影响整体系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三阶段架构路线图&lt;/strong&gt;：第一阶段为基础层，包含上下文管理服务、记忆服务、工作流引擎和Agent协调层；第二阶段为能力层，包含专用模型控制、符号推理引擎、规划与目标管理、跨模态整合；第三阶段为涌现层，真正的AGI将从上述各组件的协同作用中涌现，而非单一模型突破。系统能力将超越各部分总和，依赖于精心设计的架构。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.vincirufus.com/posts/agi-is-engineering-problem/"&gt;AGI is an Engineering Problem&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Vinci Rufus&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未注明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>AI的过去与未来 - 规模化时代、未来展望与人类角色</title><link>https://linguista.cn/rosetta/chat-notes/ai-scaling-era-past-future-human-role/</link><pubDate>Mon, 14 Jul 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/ai-scaling-era-past-future-human-role/</guid><description>&lt;h1 id="ai的过去与未来---规模化时代未来展望与人类角色"&gt;AI的过去与未来 - 规模化时代、未来展望与人类角色&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;EconTalk主持人Russ Roberts与《规模化时代》合著者Dwarkesh Patel深度对话，探讨2019至2025年间AI发展的核心驱动力——算力与数据的指数级增长，分析Transformer架构与规模化的内在关联，讨论AI当前能力边界与常识推理局限，并就通用人工智能路径、蜂巢思维构想及AI对人类生活意义的深远影响展开思辨。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/l8PLdeCO850?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="AI的过去与未来 - 规模化时代、未来展望与人类角色"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;规模化(Scaling)&lt;/strong&gt;：指通过指数级增长的算力和数据投入来提升AI模型性能的核心策略，是2019至2025年AI突破的主要驱动力，算力投入约以每年四倍的速度增长&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Moravec悖论&lt;/strong&gt;：机器人学者Hans Moravec提出的观察，即对人类困难的抽象任务对AI相对容易，而人类凭直觉完成的感知和常识任务对AI异常困难，揭示了当前AI能力的结构性局限&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;蜂巢思维(Hive Mind)&lt;/strong&gt;：Patel提出的未来AI图景，非单一超级智能，而是数十亿AI个体以超人速度思考、高效通信、任意复制合并所形成的集体智能，其优势源于数字生命的协同能力&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推理扩展(Inference Scaling)&lt;/strong&gt;：让模型在推理阶段投入更多计算时间以提升输出质量的技术方向，被视为预训练扩展边际递减后的新前沿&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跨人类主义(Transhumanism)&lt;/strong&gt;：人类利用技术增强自身能力、开启新体验维度的理念，Patel将其视为人类适应AI时代的一种可能路径&lt;/p&gt;
&lt;h2 id="简报ai的规模化浪潮算力驱动下的飞跃与未解之谜"&gt;「简报」AI的“规模化”浪潮：算力驱动下的飞跃与未解之谜&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;在人工智能领域，一场由计算能力指数级增长驱动的变革正在重塑技术格局，但其底层原理的神秘面纱仍未完全揭开，引发了关于未来机遇与深刻社会影响的激烈讨论。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;近年来，人工智能（AI）取得了令人瞩目的进展，从大型语言模型（LLM）如ChatGPT的惊艳亮相，到不断涌现的新能力，公众和业界的目光往往聚焦于算法的创新。然而，播客作者、与Gavin Leech合著《规模化时代：人工智能口述史 2019-2025》的Dwarkesh Patel在与EconTalk主持人Russ Roberts的深度对话中指出，一个更根本、或许被低估的趋势是过去六年（2019-2025）的主旋律——“规模化”（Scaling）。&lt;/p&gt;
&lt;p&gt;Patel认为，近期AI突破的真正基石，在于计算能力（Compute）和数据量的爆炸式增长。他提到，AI领域的算力投入大约以每年翻两番 ($4 \times$) 的速度增长，投资规模从十年前的学术爱好飙升至如今的数千亿美元级别。这种规模的扩张并非线性，模型性能的代际飞跃，如从GPT-2到GPT-3，再到GPT-4，往往伴随着大约百倍 ($100 \times$) 的算力投入增加。&lt;/p&gt;
&lt;p&gt;“这就像一个进化过程，”Patel解释道，“有了更多的算力进行实验，你才能尝试不同的想法，才能发现为什么像Transformer这样的架构比之前的更好。”Transformer架构由谷歌研究人员在2018年左右提出，其关键优势在于易于在大型GPU集群上并行训练，这使其极度契合“规模化”的需求。将其与简单的“预测下一个词”训练目标相结合，产生了出乎意料的智能涌现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;进展背后的经验主义与认知鸿沟&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;尽管成果斐然，AI研究者们，包括构建这些复杂系统的顶尖科学家，对于规模化为何有效的根本原因仍缺乏令人满意的理论解释。Patel引用了Anthropic公司CEO Dario Amodei的坦诚之言：“事实是，我们仍然不知道。这几乎完全只是一个偶然的经验事实。”这种状况凸显了当前AI发展在很大程度上依赖于经验性的试错和扩展，而非完全的理论指导——投入更多算力、更多数据，观察会发生什么。&lt;/p&gt;
&lt;p&gt;这种“黑箱”特性也体现在AI能力的矛盾表现上。当前的LLM可以在某些认知任务（如前沿数学、编程）上表现出色，但在看似更简单的常识推理和与物理世界交互的能力上却步履蹒跚。Patel指出了所谓的“计算机使用”难题：模型难以可靠地执行需要多步骤、与外部系统交互的现实任务，如预订航班或组织活动。这呼应了Hans Moravec早在数十年前提出的悖论：对人类来说困难的抽象任务对AI相对容易，而人类凭直觉就能完成的感知、运动和常识性任务，对AI来说却异常困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“蜂巢思维”与未来的不确定性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;展望未来，Patel对单一超级智能（ASI）通过纯粹思考解决所有问题的设想表示怀疑。他更倾向于一种“蜂巢思维”（Hive Mind）的图景：数十亿个AI以超人速度思考、高效通信、任意复制和合并，形成一种前所未有的集体智能。这种集体优势并非源于个体IQ的无限拔高，而是数字生命特有的协同能力，可能带来指数级的经济增长和知识积累，但也可能引发关于控制权和中心化风险的担忧。&lt;/p&gt;
&lt;p&gt;在讨论通用人工智能（AGI）的路径时，Patel预测，首个AGI可能效率低下、成本高昂（他形象地比喻为“耗资相当于蒙大拿州的基础设施”），依赖于如“推理扩展”（让模型思考更长时间以提升性能）等技巧。其最终目标是达到或超越人脑约20瓦 ($20$ W)的能效水平，但这将是一个漫长的过程。&lt;/p&gt;</description></item><item><title>模型为何思考</title><link>https://linguista.cn/rosetta/technology/why-models-think/</link><pubDate>Thu, 01 May 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/why-models-think/</guid><description>&lt;h1 id="模型为何思考"&gt;模型为何思考&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统回顾了大语言模型在推理时如何通过额外计算提升性能的最新进展。从心理学双系统理论类比出发，探讨了思维链prompting、并行采样、序列修订等关键技术，分析了将计算视为资源和潜在变量建模两种理论视角，并梳理了从早期CoT到o1、R1等推理模型的发展脉络。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推理时计算（Test-time Compute）&lt;/strong&gt;：指模型在推理阶段而非训练阶段投入额外计算资源，通过更多的思考步骤来提升输出质量&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维链（Chain-of-Thought）&lt;/strong&gt;：让模型在给出最终答案前生成中间推理步骤的方法，类似人类逐步解题的过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;并行采样（Parallel Sampling）&lt;/strong&gt;：同时生成多个候选输出，再通过评分函数或多数投票选择最优结果的解码策略&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;序列修订（Sequential Revision）&lt;/strong&gt;：让模型对已有输出进行反思和迭代修正，逐步改进响应质量的方法&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;潜在变量建模（Latent Variable Modeling）&lt;/strong&gt;：将思考过程视为隐藏变量，通过边缘化所有可能的推理路径来建模最终答案分布的概率框架&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文链接：&lt;a href="https://lilianweng.github.io/posts/2025-05-01-thinking/"&gt;Why We Think&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;日期：2025年5月1日&lt;/li&gt;
&lt;li&gt;预计阅读时间：40分钟&lt;/li&gt;
&lt;li&gt;作者：Lilian Weng&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特别感谢 &lt;a href="https://scholar.google.com/citations?user=itSa94cAAAAJ&amp;amp;hl=en"&gt;John Schulman&lt;/a&gt; 为本文提供了大量极其宝贵的反馈和直接编辑。&lt;/p&gt;
&lt;p&gt;推理时计算 (&lt;a href="https://arxiv.org/abs/1603.08983"&gt;Graves et al. 2016&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/1705.04146"&gt;Ling, et al. 2017&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/2110.14168"&gt;Cobbe et al. 2021&lt;/a&gt;) 和思维链 (CoT) (&lt;a href="https://arxiv.org/abs/2201.11903"&gt;Wei et al. 2022&lt;/a&gt;、&lt;a href="https://arxiv.org/abs/2112.00114"&gt;Nye et al. 2021&lt;/a&gt;) 显著提升了模型性能，同时也引发了许多研究问题。本文旨在回顾关于如何有效利用推理时计算（即“思考时间”）及其益处的最新进展。&lt;/p&gt;
&lt;h1 id="动机"&gt;动机&lt;/h1&gt;
&lt;p&gt;让模型进行更长时间的思考，可以从几个不同方面进行阐述。&lt;/p&gt;
&lt;h2 id="心理学类比"&gt;心理学类比&lt;/h2&gt;
&lt;p&gt;核心思想与人类的思考方式息息相关。我们人类无法立即给出&lt;code&gt;“12345 乘以 56789 是多少？”&lt;/code&gt;这个问题的答案。相反，对于复杂问题，我们自然会花时间思考和分析，然后才能得出结果。在 &lt;a href="https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555"&gt;《思考，快与慢》(Kahneman, 2013)&lt;/a&gt; 一书中，丹尼尔·卡尼曼 (Daniel Kahneman) 借助 &lt;a href="https://en.wikipedia.org/wiki/Dual_process_theory"&gt;双系统理论&lt;/a&gt; 的视角，将人类思维划分为两种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;快速思维（系统 1）&lt;/em&gt; 运作迅速且自动化，由直觉和情感驱动，几乎不需要努力。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;慢速思维（系统 2）&lt;/em&gt; 需要深思熟虑、逻辑推理和显著的认知努力。这种思维模式消耗更多的脑力，需要有意识的参与。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于系统 1 思维快速且容易，它常常成为主要的决策驱动因素，但代价是牺牲了准确性和逻辑性。它自然地依赖我们大脑的思维捷径（即启发式），并可能导致错误和偏见。通过有意识地放慢速度，花更多时间反思、改进和分析，我们可以运用系统 2 思维来挑战我们的直觉，做出更理性的选择。&lt;/p&gt;
&lt;h2 id="计算作为一种资源"&gt;计算作为一种资源&lt;/h2&gt;
&lt;p&gt;深度学习的一种观点认为，神经网络可以根据其在一次前向传播中可访问的计算量和存储量来表征；如果我们使用梯度下降优化它们来解决问题，优化过程将找出如何利用这些资源——它们将弄清楚如何将这些资源组织成用于计算和信息存储的电路。从这个角度来看，如果我们设计一个能够在推理时进行更多计算的架构或系统，并训练它有效利用这一资源，它将表现得更好。&lt;/p&gt;</description></item><item><title>Google 官方提示工程白皮书</title><link>https://linguista.cn/rosetta/technology/google-prompt-engineering-whitepaper/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/google-prompt-engineering-whitepaper/</guid><description>&lt;h1 id="google-官方提示工程白皮书"&gt;Google 官方提示工程白皮书&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;这份由Google发布的提示工程白皮书系统介绍了大语言模型的提示设计方法。内容涵盖LLM输出配置、零样本与少样本提示、思维链推理、ReAct等多种提示技巧，并深入探讨代码提示和多模态提示的应用场景，最后给出了一系列实用的最佳实践建议，帮助读者从入门到精通掌握提示工程。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;提示工程(Prompt Engineering)&lt;/strong&gt;：设计和优化输入提示以引导大语言模型生成准确输出的系统化过程，涉及措辞选择、结构设计和参数调优&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维链(Chain of Thought)&lt;/strong&gt;：一种引导模型逐步推理的提示技巧，通过分解复杂问题为中间步骤来提升模型的推理准确性&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;零样本与少样本提示(Zero-shot &amp;amp; Few-shot)&lt;/strong&gt;：零样本指不提供示例直接提问，少样本指在提示中提供少量示例来引导模型理解任务格式和期望输出&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReAct(推理与行动)&lt;/strong&gt;：一种结合推理和外部工具调用的高级提示框架，使模型能够交替进行思考和执行操作以完成复杂任务&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM输出配置&lt;/strong&gt;：通过调节温度、Top-K、Top-P等采样参数来控制模型生成结果的随机性和创造性水平&lt;/p&gt;
&lt;p&gt;Published on 2025-04-10&lt;/p&gt;
&lt;p&gt;作者：Lee Boonstra&lt;/p&gt;
&lt;p&gt;翻译：宝玉 ( &lt;a href="https://baoyu.io/"&gt;https://baoyu.io&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;&lt;a href="https://s.baoyu.io/files/Google%20%E5%AE%98%E6%96%B9%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%20%5C%28Prompt%20Engineering%5C%29%E7%99%BD%E7%9A%AE%E4%B9%A6-baoyu.io.pdf"&gt;PDF 下载&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://s.baoyu.io/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf"&gt;原始版本 PDF 下载&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;提示工程 (Prompt Engineering)&lt;/p&gt;
&lt;p&gt;2024年9月 (September 2024)&lt;/p&gt;
&lt;h2 id="致谢-acknowledgements"&gt;&lt;strong&gt;致谢 (Acknowledgements)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;审阅者与贡献者 (Reviewers and Contributors)&lt;br&gt;
Michael Sherman&lt;br&gt;
Yuan Cao&lt;br&gt;
Erick Armbrust&lt;br&gt;
Anant Nawalgaria&lt;br&gt;
Antonio Gulli&lt;br&gt;
Simone Cammel&lt;br&gt;
策划者与编辑 (Curators and Editors)&lt;br&gt;
Antonio Gulli&lt;br&gt;
Anant Nawalgaria&lt;br&gt;
Grace Mollison&lt;br&gt;
技术作者 (Technical Writer)&lt;br&gt;
Joey Haymaker&lt;br&gt;
设计师 (Designer)&lt;br&gt;
Michael Lanning&lt;/p&gt;</description></item><item><title>xAI 团队展示 Grok 3：下一代 AI 的强大功能与未来展望</title><link>https://linguista.cn/rosetta/chat-notes/xai-grok-3-demo-next-gen-ai/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/xai-grok-3-demo-next-gen-ai/</guid><description>&lt;h1 id="xai-团队展示-grok-3下一代-ai-的强大功能与未来展望"&gt;xAI 团队展示 Grok 3：下一代 AI 的强大功能与未来展望&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;xAI 发布会展示了新一代大型语言模型 Grok 3，其算力较前代提升超过十倍，依托自建十万块 H100 GPU 数据中心实现性能飞跃。Grok 3 在数学推理、编码、STEM 等基准测试中领先，并推出 Deep Search 搜索引擎和独立应用，面向 X Premium Plus 用户开放，预告语音交互等新功能即将上线。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/AUAJ82H12qs?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="xAI 团队展示 Grok 3：下一代 AI 的强大功能与未来展望"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Grok 3&lt;/strong&gt;：xAI 推出的新一代大型语言模型，算力较 Grok 2 提升十倍以上，在多项基准测试中表现领先&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deep Search&lt;/strong&gt;：由 Grok 驱动的下一代搜索引擎，融合推理能力与工具调用，支持多来源验证和透明推理过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试时计算（Test-time Compute）&lt;/strong&gt;：在推理阶段投入更多计算资源以提升模型输出质量的技术策略&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;H100 GPU 集群&lt;/strong&gt;：xAI 在 122 天内建成的十万块 GPU 全连接数据中心，为 Grok 3 训练提供核心算力支撑&lt;/p&gt;</description></item><item><title>Andrej Karpathy 盛赞 DeepSeek R1 强化学习推动 AI 模型进化</title><link>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/karpathy-praises-deepseek-r1-reinforcement-learning/</guid><description>&lt;h1 id="andrej-karpathy-盛赞-deepseek-r1强化学习推动-ai-模型进化"&gt;Andrej Karpathy 盛赞 DeepSeek R1：强化学习推动 AI 模型进化&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于 Andrej Karpathy 的最新视频内容，详细介绍了 DeepSeek R1 在强化学习方面的技术创新。Karpathy 指出，DeepSeek R1 通过强化学习微调，在解决数学问题等方面表现出色，并且能够发现人类思考问题的逻辑和策略。文章还探讨了强化学习与人类反馈（RLHF）的优势与局限性，并展望了大语言模型在多模态能力和智能体方面的未来发展。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;Andrej Karpathy 作为 OpenAI 早期成员、前特斯拉 AI 总监，在其最新视频中对 DeepSeek R1 给予了高度评价。他回顾了从神经网络起源到最新大模型的发展历程，特别强调了 DeepSeek R1 在强化学习（RL）方面的技术创新。DeepSeek R1 是一个通过强化学习微调的大语言模型，其性能与 OpenAI 的模型不相上下，在解决数学问题时表现出色。&lt;/p&gt;
&lt;p&gt;Karpathy 认为，强化学习是大语言模型训练中的新兴阶段，目前仍处于起步状态。强化学习的优势在于能够突破人类的限制，发现人类未曾意识到的策略，例如 AlphaGo 在围棋中发明的创新走法。然而，强化学习也存在挑战，它非常擅长发现&amp;quot;欺骗&amp;quot;模型的方法，这可能会阻碍其在某些领域的应用。&lt;/p&gt;
&lt;p&gt;关于强化学习与人类反馈，Karpathy 指出，从人类反馈中进行强化学习（RLHF）能够提升模型性能，尤其是在那些无法验证的领域，如创意写作等。但 RLHF 的主要问题是基于人类反馈的强化学习可能会产生误导，因为人类反馈是一个有损模拟，无法完美反映真实人类的判断。&lt;/p&gt;
&lt;p&gt;展望未来，Karpathy 预测未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现多模态能力。此外，大语言模型将能够执行长期任务，人类将成为这些智能体任务的监督者。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;：强化学习是大语言模型训练中的新兴阶段，通过奖励机制优化模型行为。DeepSeek R1 通过强化学习微调，在解决数学问题时表现出色，并且能够发现人类思考问题的逻辑和策略，如&amp;quot;思维链&amp;quot;（CoT）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLHF&lt;/strong&gt;：从人类反馈中进行强化学习能够提升模型性能，尤其是在那些无法验证的领域。人类标注者可以通过简单的排序任务来提供高质量的反馈，但人类反馈是一个有损模拟，无法完美反映真实人类的判断，可能会导致误导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多模态能力&lt;/strong&gt;：未来的语言模型将不仅能够处理文本，还将轻松处理音频和图像。通过标记化技术，模型可以将音频、图像和文本的标记流交替放入一个模型中进行处理，实现真正的多模态能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/thTwdVgc4lfYRj6WWpKBwA"&gt;Andrej Karpathy 最新视频盛赞 DeepSeek：R1 正在发现人类思考的逻辑并进行复现&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;郑佳美&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年02月06日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>开源大语言模型的崛起与对行业巨头的挑战</title><link>https://linguista.cn/curated/henrinotes-2025_p2/open-source-llm-rise-google-moat/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025_p2/open-source-llm-rise-google-moat/</guid><description>&lt;h1 id="开源大语言模型的崛起与对行业巨头的挑战"&gt;开源大语言模型的崛起与对行业巨头的挑战&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文通过分析 Google 内部文件《我们没有护城河，OpenAI 也没有》，深入探讨了开源大语言模型的快速崛起及其对行业巨头的战略冲击。文章指出，开源社区通过低成本微调、快速迭代和协同创新，在短时间内实现了惊人的技术突破，正在重塑人工智能领域的竞争格局。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章以 DeepSeek R1 开源模型的发布为切入点，展现了开源 AI 模型在性能上已能够媲美甚至超越闭源商业模型。作者回顾了 2023 年初 LLaMA 模型泄露后的开源创新浪潮，详细梳理了短短两个月内开源社区在指令微调、量化优化、多模态支持和 RLHF 等领域的一系列突破。&lt;/p&gt;
&lt;p&gt;文章深入分析了开源模型的核心优势：速度快、可定制性强、注重隐私且功能全面。通过 LLaMA、Alpaca、Vicuna、Koala 等案例，展示了开源社区如何利用 LoRA 等低成本的微调方法，以数百美元的训练成本达到接近 ChatGPT 和 Bard 的性能水平。这种创新速度和效率远超大型科技企业，对 Google 和 OpenAI 的商业战略构成了直接挑战。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;开源社区的协同创新&lt;/strong&gt;：开源模式通过全球开发者的协作，实现了技术创新的指数级加速。当 LLaMA 权重泄露后，开源社区在一个月内完成了大型企业需要数月甚至数年才能完成的技术迭代，包括指令微调、量化、多模态支持等关键技术突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;低成本微调技术&lt;/strong&gt;：以 LoRA 为代表的低秩适应方法，使得开发者能够在消费级硬件上以极低的成本（100-300 美元）对大语言模型进行有效微调。这彻底改变了 AI 模型的开发范式，让个人开发者和小团队也能参与到最前沿的 AI 创新中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;护城河的消解&lt;/strong&gt;：Google 内部文件承认，开源模型在功能、性能和可用性上已经能够与闭源模型抗衡。当用户可以免费获得质量相当且无使用限制的模型时，依赖 API 访问的闭源商业模式的可持续性面临严峻挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Meta 的开源战略&lt;/strong&gt;：Meta 通过开源 LLaMA 架构，成功吸引了全球开发者在其生态系统中进行创新，这些创新成果最终可以被 Meta 直接整合到自己的产品中，形成了一种&amp;quot;反向护城河&amp;quot;效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;快速迭代的时间窗口&lt;/strong&gt;：从 2023 年 2 月 LLaMA 发布到 4 月开源 RLHF 达到 ChatGPT 水平，短短两个月内开源社区完成了从模型泄露到性能媲美顶级商业模型的完整进化链条，展现了开源模式在 AI 领域的巨大潜力。&lt;/p&gt;</description></item><item><title>2024年RAG技术发展综述</title><link>https://linguista.cn/curated/henrinotes_2025_p3/rag-technology-development-2024/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/rag-technology-development-2024/</guid><description>&lt;h1 id="2024年rag技术发展综述"&gt;2024年RAG技术发展综述&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;2024年被称作&amp;quot;RAG发展元年&amp;quot;，本文系统梳理了RAG技术在2024年的重要发展。文章从RAG的技术挑战出发，详细介绍了多模态文档解析、混合搜索、GraphRAG等标志性突破，深入分析了数据清洗、排序模型、语义鸿沟解决方案等核心技术细节，并探讨了RAG与Agent结合、多模态RAG等前沿方向，最后对2025年RAG技术的持续进化进行了展望。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先阐述了RAG技术在2024年的重要地位，指出尽管关于RAG的争论不断，但从成本和实时性角度，RAG已显示出压倒性优势。即使在需要微调介入的场景中，RAG也通常是不可或缺的组成部分。作者将2024年定位为RAG发展的关键转折点。&lt;/p&gt;
&lt;p&gt;文章详细分析了RAG面临的三大技术挑战：非结构化多模态文档问答难题、纯向量数据库的局限性以及语义鸿沟问题。这些挑战推动了2024年RAG技术的多项突破性进展，包括多模态文档解析工具的崛起、BM25和混合搜索的普及、GraphRAG的开源以及延迟交互模型与多模态RAG的发展。&lt;/p&gt;
&lt;p&gt;在技术细节方面，文章深入探讨了数据清洗的多模态处理、Text Chunking优化方法、混合搜索的三路召回策略、Embedding与Reranker模型的协同作用，以及基于张量的重排序模型等核心技术。特别强调了GraphRAG及其变种在解决语义鸿沟问题上的创新性贡献。&lt;/p&gt;
&lt;p&gt;最后，文章展望了RAG与Agent结合的Agentic RAG模式、多模态RAG的技术实现路径，以及RAG作为企业搜索引擎在大模型时代的进化方向。作者认为RAG类似于过去的数据库，是一个包含数据库、小模型和工具的复杂系统，将持续向更加智能化和集成化的方向发展。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GraphRAG&lt;/strong&gt;：微软开源的GraphRAG架构是2024年RAG领域现象级事件，它通过利用大模型抽取文档内的命名实体并构建知识图谱，有效解决了RAG的语义鸿沟问题。该架构特别适用于意图不明的笼统提问或&amp;quot;多跳&amp;quot;问答场景。其变种如Fast GraphRAG、LightRAG等通过降低Token消耗，使得这一技术更加实用化和普及化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;混合搜索&lt;/strong&gt;：2024年混合搜索理念深入人心，它不再将向量数据库作为单独品类存在，而是采用向量搜索、稀疏向量搜索和全文搜索的三路混合召回策略。BM25等经典算法重新受到重视，Elasticsearch和Infinity等数据库提供了符合要求的全文搜索和混合搜索能力。这种混合方法在召回率和准确性方面都取得了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多模态RAG&lt;/strong&gt;：随着VLM（视觉语言模型）对图像理解能力的深入，多模态RAG成为重要发展方向。它能够处理PDF、PPT等非纯文本类数据，根据用户提问在文档中找到包含答案的图片和文字。技术实现上有两种主要路径：通过模型将多模态文档转成文本再建立索引，或直接生成向量规避OCR过程，如ColPali工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agentic RAG&lt;/strong&gt;：这是RAG与Agent结合的重要模式，RAG作为Agent的重要算子，解锁了Agent访问内部数据的能力。Agentic RAG可以让RAG在复杂场景下以可控方式提供适应性变化，同时RAG需要为Agent提供记忆管理功能，包括用户对话Session、个性化信息等，以支持Agent的Reasoning能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;语义鸿沟&lt;/strong&gt;：这是RAG技术的核心挑战之一，指在很多情况下（如意图不明的笼统提问或&amp;quot;多跳&amp;quot;问答），提问和答案之间存在明显的语义差距。除了GraphRAG外，RAPTOR、SiReRAG等方法也通过预聚类和细粒度定义文本召回，增强对数据宏观层面的理解，从而跨越这一鸿沟。&lt;/p&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://zhuanlan.zhihu.com/p/14116449727"&gt;万字长文梳理2024年的RAG&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;知乎作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>微软提出RAG应对用户四级查询难度的方案</title><link>https://linguista.cn/curated/henrinotes_2025_p3/microsoft-rag-four-level-query-classification/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/microsoft-rag-four-level-query-classification/</guid><description>&lt;h1 id="微软提出rag应对用户四级查询难度的方案"&gt;微软提出RAG应对用户四级查询难度的方案&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;微软研究院针对检索增强生成（RAG）技术在专业领域部署中面临的挑战，提出了用户查询任务的四级分类体系。该体系从显式事实查询到隐性推理依据查询，逐级递增复杂度，并为每个级别设计了相应的技术解决方案，包括迭代RAG、链式思维提示、微调等策略，旨在帮助开发者根据具体查询类型选择合适的知识注入方法。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;大语言模型通过整合外部数据能够显著提升其在特定领域的任务完成能力。外部数据增强不仅使模型能够获得领域专业知识，还增强了其时间相关性、输出可控性和可解释性。检索增强生成（RAG）和微调等技术因此受到广泛关注，但在实际部署到专业领域时仍面临诸多挑战。&lt;/p&gt;
&lt;p&gt;微软研究院的核心贡献在于建立了一个系统的用户查询分类框架。该框架将查询任务分为四个难度级别：第一级是显式事实查询，可通过明确文本片段直接回答；第二级是隐式事实查询，需要整合多个数据源进行逻辑推断；第三级是明确推理依据的查询，需要理解应用外部资源中明确提供的推理规则；第四级是隐性推理依据的查询，推理依据未明确记录，需要从数据中观察模式推断。&lt;/p&gt;
&lt;p&gt;针对这四个级别，研究团队给出了差异化的技术解决方案。对于简单的显式查询，重点解决数据处理和检索问题；对于复杂的隐式查询，则需要迭代RAG、基于图或树的问答、NL2SQL等高级技术；当涉及推理依据时，需要采用提示调整、链式思维提示或构建Agent工作流；而对于最复杂的隐性推理查询，则可能需要离线学习、上下文学习或模型微调。研究同时提出了给LLMs整合外部数据的三种主要形式，为开发者提供了系统的方法论指导。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;显式事实查询&lt;/strong&gt;：这是最基础的查询级别，答案可以通过明确的文本片段直接获得，通常依赖单一数据源。例如询问&amp;quot;2024年夏季奥运会举办地&amp;quot;，这类查询的主要挑战在于数据处理效率、检索准确性以及RAG系统性能的评估，而不涉及复杂的推理过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐式事实查询&lt;/strong&gt;：这类查询需要整合多个分散的数据来源，建立逻辑关联才能得出答案。例如&amp;quot;实验样本量大于1000的数量有多少&amp;quot;，信息可能分散在文档的不同部分。其挑战包括信息分散、复杂推理需求、自适应检索量控制、推理与检索的协调以及多跳推理。解决方案包括迭代RAG、基于图或树的问答系统、自然语言转SQL、智能检索与推理结合以及动态信息整合等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;明确推理依据的查询&lt;/strong&gt;：这类查询不仅需要事实信息，还需要理解和应用特定的推理规则或工作流程，且这些依据在外部资源中有明确提供。例如胸痛患者的诊断治疗流程或客户服务工作流程应对。主要挑战包括提示优化成本高、可解释性有限、多步推理复杂性等。解决方案涉及提示调整技术、链式思维提示、利用LLM本身进行提示优化以及构建Agent工作流。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐性推理依据的查询&lt;/strong&gt;：这是最高级别的查询，推理依据或规则并未明确记录在文档中，需要从外部数据中观察模式和结果来推断。例如评估经济形势对公司未来发展的影响。主要挑战是逻辑检索困难和数据不足，解决方案包括离线学习、上下文学习和模型微调等方法，让模型通过数据学习隐含的推理模式。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/k-r4rfDftlsoSGkGoXOlGw"&gt;RAG怎么面对用户的4级查询难度？微软给出方案！&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;微软研究院&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深入解析大模型LLMs的Token及其成本流向</title><link>https://linguista.cn/curated/henrinotes_2025_p3/llm-token-cost-guide-openai/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes_2025_p3/llm-token-cost-guide-openai/</guid><description>&lt;h1 id="深入解析大模型llms的token及其成本流向"&gt;深入解析大模型LLMs的Token及其成本流向&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文系统性地介绍了大语言模型中Token的核心概念及其在OpenAI API中的成本计算方式。文章从Token的定义出发，详细阐述了Token的生成机制、特殊情况处理、中英文分词差异，并提供了完整的OpenAI模型定价参考，为开发者优化API使用成本提供了实用指导。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先以OpenAI API的Token定价表开篇，清晰地展示了不同模型的输入和输出价格差异。GPT-4o、GPT-4o-mini、GPT-4-turbo和GPT-3.5-turbo等主流模型的定价从每千Token $0.00015到$0.03不等，这直接关系到开发者的使用成本。&lt;/p&gt;
&lt;p&gt;在核心概念部分，文章指出大语言模型并非简单预测下一个单词，而是预测下一个Token。Token可以被理解为单词的片段，通过分词器将句子拆分，从而降低字典规模并提高训练和推理效率。文章提供了实用的换算参考：通常1个Token约等于4个英文字符或四分之三个单词，100个Token约等于75个单词。&lt;/p&gt;
&lt;p&gt;文章进一步深入探讨了Token处理中的特殊情况，包括大小写和空格对Token生成的影响，以及长单词可能被拆分成多个Token的现象。这些细节对于理解模型的处理机制和优化成本具有重要意义。&lt;/p&gt;
&lt;p&gt;在中英文处理差异方面，文章介绍了现代LLM采用的BPE（Byte Pair Encoding）和SentencePiece两种主流子词切分方法，并通过&amp;quot;我爱中国&amp;quot;的实例说明了中文Token的生成逻辑。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Token&lt;/strong&gt;：大语言模型的基本处理单元，不同于传统的单词级别处理，Token将句子拆分为更小的片段。这种设计既降低了字典规模，又保持了语义完整性。1个Token约等于4个字符或0.75个单词的换算关系，为开发者估算成本提供了实用参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分词器&lt;/strong&gt;：将文本转换为Token序列的核心组件。现代分词器能够智能地处理特殊情况，如大小写敏感性、前导空格等。理解分词器的工作原理有助于开发者优化输入文本格式，从而控制API调用成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BPE和SentencePiece&lt;/strong&gt;：两种主流的子词切分算法。BPE采用从字符到子词的渐进合并策略，类似搭积木的过程；SentencePiece则将所有输入视为统一的字节流，从整体出发找到最优切分点。对于中文文本，BPE通常以单字为基础，再根据词频合并常见子词。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;成本优化&lt;/strong&gt;：基于Token的定价机制要求开发者在保证模型效果的前提下，尽可能减少输入和输出的Token数量。通过理解Token的生成规则，开发者可以通过优化输入格式、选择合适的模型等方式有效控制成本。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s?__biz=MzI4MjE1Nzc2MQ==&amp;amp;mid=2649035008&amp;amp;idx=1&amp;amp;sn=a2e92d4f3beadcc1d86f6f8a2313580d&amp;amp;scene=21#wechat_redirect"&gt;一文说清楚什么是大模型LLMs的Token,全面了解钱的流向&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>多种经典提示技术的应用与设置</title><link>https://linguista.cn/curated/henrinotes-2025-p1/classic-prompting-techniques-guide/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/classic-prompting-techniques-guide/</guid><description>&lt;h1 id="多种经典提示技术的应用与设置"&gt;多种经典提示技术的应用与设置&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文全面介绍了提示工程的核心要素，包括模型参数设置、提示词设计原则以及多种经典提示技术。文章从温度、Top_p等基础参数讲起，深入解析零样本、少样本、链式思考等主流提示方法，并探讨了思维树、自动推理工具、自我反思等进阶技术，为开发者提供了系统性的提示工程实践指南。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了大语言模型交互时的关键模型设置参数。温度参数控制输出的确定性与创造性，较低温度使模型输出更确定，较高温度则增加随机性，适合创造性任务。Top_p参数通过核采样控制响应多样性，最大长度限制防止冗余输出，停止序列精确控制输出结构，频率和存在惩罚则有效避免内容重复。&lt;/p&gt;
&lt;p&gt;在提示词设计方面，文章强调了四个核心要素。指令明确模型要执行的具体任务，上下文提供必要的外部信息，输入数据包含用户的实际问题，输出指示规范结果的格式类型。设计时应从简单入手逐步迭代，使用明确具体的指令，并通过示例引导模型产生期望输出。&lt;/p&gt;
&lt;p&gt;文章重点介绍了多种经典提示技术及其应用场景。零样本提示直接提问不提供示例，依赖模型预训练知识；少样本提示通过一到多个示例帮助模型理解任务模式；链式思考提示引导模型展示中间推理步骤，特别适合复杂逻辑问题。自动思维链通过聚类和抽样自动生成推理链，自我一致性方法则结合多条推理路径选择最稳定答案。&lt;/p&gt;
&lt;p&gt;进阶技术部分涵盖了生成知识提示、链式提示分解、思维树决策等高级方法。生成知识提示先让模型生成相关知识再回答问题，链式提示将复杂任务拆解为多个子任务逐步处理，思维树技术模仿人类决策过程用树状结构表示问题解决路径。自动推理工具技术结合中间推理步骤和外部工具使用，自我反思则通过语言反馈不断强化智能体的学习能力。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;温度参数&lt;/strong&gt;：控制模型输出随机性的关键参数。低温度如0.2使输出更加确定和保守，适合事实性问答；高温度如0.8增加随机性和创造性，适用于创意写作、头脑风暴等场景。实际应用中需根据任务性质谨慎调整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Top_p采样&lt;/strong&gt;：又称核采样，通过累积概率阈值控制候选token范围。设为0.1时仅从最可能的10%token中选择，输出更准确；设为0.9时考虑更多可能性，响应更多样化。通常与温度参数配合使用，但建议二选一避免相互干扰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;链式思考提示&lt;/strong&gt;：要求模型在给出最终答案前展示中间推理步骤，显著提升复杂问题的解决能力。例如数学问题中让模型&amp;quot;一步步思考&amp;quot;并说明计算过程，能大幅提高准确率。对于逻辑推理、多步骤任务特别有效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;少样本学习&lt;/strong&gt;：在提示中提供一到多个完整示例，每个示例包含输入和期望输出。这种方式让模型通过类比学习任务模式，无需额外训练即可适应新场景。示例选择要具有代表性，数量通常在三到五个之间效果最佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思维树技术&lt;/strong&gt;：将问题解决过程表示为树状结构，每个节点代表一个思维状态，分支代表可能的行动。通过探索、评估和回溯机制，智能体能够在复杂决策空间中找到最优路径，比线性思维链更强大也更耗计算资源。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/aBrKnkVb2_qLSbT7a04SrA"&gt;多种经典提示技术——Prompt&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;简单的机器学习&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-24&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Chris Bishop 教授访谈——关于新书《Deep Learning》以及对AI的思考</title><link>https://linguista.cn/rosetta/chat-notes/chris-bishop-interview-deep-learning-textbook-ai-thoughts-2024/</link><pubDate>Wed, 10 Apr 2024 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/chris-bishop-interview-deep-learning-textbook-ai-thoughts-2024/</guid><description>&lt;h1 id="chris-bishop-教授访谈关于新书deep-learning以及对ai的思考"&gt;Chris Bishop 教授访谈——关于新书《Deep Learning》以及对AI的思考&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文记录了对机器学习领域著名学者 Chris Bishop 教授的深度访谈。访谈围绕其与儿子合著的新书《Deep Learning - Foundations and Concepts》展开，涵盖新书创作理念、连接主义与符号主义的融合、贝叶斯方法的实践挑战、大型语言模型的涌现能力、AI 创造力的哲学思考，以及他在微软领导的 AI for Science 项目在药物研发和物理模拟等领域的应用前景。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/kuvFoXzTK3E?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="Chris Bishop 教授访谈——关于新书《Deep Learning》以及对AI的思考"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;归纳偏置（Inductive Bias）——在数据稀缺的科学领域中，将物理定律等先验知识融入模型设计，以提升泛化能力，与纯粹依赖大规模数据和算力的思路形成对比&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯方法——一种基于概率论的推断框架，理论上能优雅地处理不确定性，但在大规模深度学习中因计算代价高昂而常被点估计和集成学习等方法近似替代&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI for Science——利用人工智能加速科学发现的研究方向，通过计算生成与筛选分子、训练快速神经网络模拟器等方式，在药物研发和核聚变控制等场景中实现数量级的效率提升&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;连接主义与符号主义——人工智能两大经典范式，前者以神经网络为代表强调从数据中学习分布式表征，后者强调基于规则的逻辑推理，现代大语言模型在统一架构中展现出融合两者的趋势&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer——当前深度学习领域的主导架构，基于自注意力机制实现序列建模，是 GPT 等大语言模型的核心基础，但 Bishop 教授认为其并非终极架构&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视频链接：&lt;a href="https://www.youtube.com/watch?v=kuvFoXzTK3E"&gt;Prof. Chris Bishop&amp;rsquo;s NEW Deep Learning Textbook!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;博主：&lt;a href="https://www.youtube.com/@MachineLearningStreetTalk"&gt;链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;时间：2024年4月10日&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="访谈介绍"&gt;访谈介绍&lt;/h3&gt;
&lt;p&gt;以下内容是对著名人工智能学者 Chris Bishop 教授访谈的详细记录。Bishop 教授以其开创性的教科书《Pattern Recognition and Machine Learning》(PRML) 而闻名于机器学习领域。本次访谈的核心围绕他与其子 Hugh Bishop 合著的新书《Deep Learning: Foundations and Concepts》展开。&lt;/p&gt;</description></item></channel></rss>