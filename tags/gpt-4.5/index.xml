<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPT-4.5 on Linguista</title><link>https://linguista.cn/tags/gpt-4.5/</link><description>Recent content in GPT-4.5 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/gpt-4.5/index.xml" rel="self" type="application/rss+xml"/><item><title>深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战</title><link>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</guid><description>&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;OpenAI内部访谈深入回顾了GPT-4.5的预训练全过程。团队以实现10倍智能提升为目标，经历了长达两年的研发，涉及ML与系统团队的深度协同设计、多集群训练中的硬件故障应对、大规模调试困境，以及从计算受限向数据受限的关键转变。访谈验证了Scaling Laws的持续有效性，并揭示了数据效率和容错机制等未来核心挑战。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/6nJZopACRuQ?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scaling Laws&lt;/strong&gt;：描述模型性能随计算量、数据量和参数规模增长而可预测提升的经验规律，是GPT-4.5训练规划和风险评估的核心方法论&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（Data Efficiency）&lt;/strong&gt;：指模型从有限数据中提取知识的能力，GPT-4.5研发标志着OpenAI从计算受限进入数据受限的新阶段，当前算法与人类学习效率仍有十万到百万倍的差距&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协同设计（Co-design）&lt;/strong&gt;：ML算法团队与系统基础设施团队在架构、通信、计算和存储等层面进行深度联合设计，确保软硬件在目标规模下高效协同工作&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;困惑度（Perplexity）&lt;/strong&gt;：衡量语言模型预测能力的核心指标，数值越低表示模型压缩和泛化能力越强，OpenAI以内部代码库的困惑度作为模型智能水平的关键标尺&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;风险解除（Derisking）&lt;/strong&gt;：在正式大规模训练前，从已知稳定配置出发逐层引入新特性并验证其跨规模扩展性的系统化实验流程，用于避免大规模训练中的不可预见失败&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视频链接：&lt;a href="https://www.youtube.com/watch?v=6nJZopACRuQ"&gt;Pre-Training GPT-4.5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;官方频道：&lt;a href="https://www.youtube.com/@OpenAI"&gt;OpenAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战-1"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;p&gt;近期，OpenAI 举办了一场特别的内部访谈，由 CEO Sam Altman 主持，邀请了参与 GPT-4.5 研发的核心技术成员——负责预训练数据与机器学习的 Alex Paino、首席系统架构师 Amin Tootoonchian 以及负责数据效率与算法的 Daniel Selsam，共同回顾了这款备受瞩目的大模型从构思到训练完成的艰辛历程。与以往发布新产品不同，这次讨论旨在揭示 GPT-4.5 背后深层次的研究投入、技术挑战与关键学习，回应社区对其超越 GPT-4 的优异表现及其成因的广泛兴趣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、 宏伟目标与规模化挑战的起点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GPT-4.5 项目的启动可追溯至访谈发生时的两年前，其核心目标被设定为实现相较于 GPT-4 约 &lt;code&gt;10x&lt;/code&gt; 的智能提升。这一雄心壮志的设定是基于对即将上线的新一代大规模计算集群能力的预判。团队预见到，这将是一项需要投入大量人力、时间及计算资源的庞大工程。正如 Amin Tootoonchian 所述，大型模型的训练过程是一个从概念诞生之初就需要机器学习（ML）与系统（Systems）团队深度协作，直至模型精确定义、训练启动并最终完成的复杂流程。而 GPT-4.5 的研发，更是集合了比以往模型（如 GPT-4）更多的人力，代表了一次在规模和复杂度上都截然不同的尝试。最终，团队认为所交付的模型在有效算力投入和智能水平上达到了预设的 &lt;code&gt;10x&lt;/code&gt; 目标。&lt;/p&gt;</description></item></channel></rss>