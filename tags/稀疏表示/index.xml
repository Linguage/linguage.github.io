<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>稀疏表示 on Linguista</title><link>https://linguista.cn/tags/%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA/</link><description>Recent content in 稀疏表示 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 04 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA/index.xml" rel="self" type="application/rss+xml"/><item><title>稀疏贝叶斯学习的理论与应用</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/sparse-bayesian-learning-theory-application/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/sparse-bayesian-learning-theory-application/</guid><description>&lt;h1 id="稀疏贝叶斯学习的理论与应用"&gt;稀疏贝叶斯学习的理论与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;稀疏贝叶斯学习（SBL）是一种结合稀疏性与贝叶斯统计的学习方法，广泛应用于信号处理和机器学习领域。本文从贝叶斯统计与稀疏表示的理论基础出发，详细阐述了SBL模型的构建与优化过程，包括边缘似然最大化和EM算法的具体步骤，并通过实验对比了SBL与Ridge回归在不同噪声条件下的表现，验证了SBL在高噪声环境中更强的稳定性和适应性。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了稀疏贝叶斯学习的基本定义与核心特点。SBL与传统稀疏方法（如L1正则化）的关键区别在于，它不需要预先指定稀疏度参数，而是通过贝叶斯推理自适应地发现数据中的稀疏结构，这使得模型在应用中具有更高的灵活性。&lt;/p&gt;
&lt;p&gt;在理论基础部分，文章系统回顾了贝叶斯统计的核心要素——先验分布、似然函数、后验分布和边缘似然，并介绍了稀疏表示的基本概念，包括Lasso、L1和L2正则化等常见方法，为后续的模型构建奠定了理论基础。&lt;/p&gt;
&lt;p&gt;模型构建与优化是文章的核心内容。文章以线性回归模型为框架，采用零均值高斯分布作为参数的稀疏先验分布，通过独立的精度参数控制每个参数的方差。在优化过程中，采用期望最大化（EM）算法，通过E步骤计算参数的后验分布，M步骤更新超参数和噪声方差，逐步最大化边缘似然。&lt;/p&gt;
&lt;p&gt;实验部分通过生成高维稀疏数据，对比了SBL与Ridge回归在不同噪声水平下的性能表现，使用MSE、MAE、最大误差和R²分数等多维指标进行评估，结果表明SBL在高噪声条件下展现出更强的稳定性和抗噪能力。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;稀疏贝叶斯学习（SBL）&lt;/strong&gt;：一种将贝叶斯推理与稀疏性约束相结合的学习框架。其核心优势在于无需手动设定稀疏度参数，而是通过贝叶斯推理自动识别数据中的稀疏结构。模型通过精度参数的自适应调节实现稀疏性——当某个精度参数值很大时，对应的参数方差趋近于零，从而自然地实现参数的&amp;quot;裁剪&amp;quot;，得到稀疏解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边缘似然最大化&lt;/strong&gt;：SBL模型优化的核心策略。边缘似然是在对模型参数积分后得到的数据概率，它综合考虑了模型复杂度和数据拟合度，天然具备奥卡姆剃刀效应。通过最大化边缘似然来估计精度参数和噪声方差，避免了过拟合风险，使模型能够在数据拟合与结构简洁之间取得平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;期望最大化（EM）算法&lt;/strong&gt;：SBL模型参数估计的迭代优化方法。E步骤在当前超参数下计算模型参数的后验分布，获得均值和协方差；M步骤利用后验统计量更新精度参数和噪声方差。两个步骤交替迭代直至收敛，逐步逼近边缘似然的最大值，是SBL实现自适应稀疏学习的关键算法机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;稀疏先验分布&lt;/strong&gt;：SBL模型中对参数施加的先验假设，通常采用零均值高斯分布，并为每个参数引入独立的精度（逆方差）超参数。这种层级化的先验设计使模型能够对不同参数施加不同强度的约束，精度越大则参数越可能为零，从而在贝叶斯框架下自然地引导出稀疏解，而非通过硬性的正则化惩罚项实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SBL与Ridge回归的对比实验&lt;/strong&gt;：实验在不同噪声水平下对比了两种方法的性能。在低噪声条件下，两者表现相当；但在高噪声环境中，Ridge回归的解释方差（R²分数）波动较大，而SBL保持相对稳定，体现了贝叶斯方法在不确定性建模方面的天然优势，以及SBL通过自适应稀疏机制过滤噪声干扰的能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D21HuJN6qq8QBjv86V5BTA"&gt;稀疏贝叶斯学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>压缩感知-讲义</title><link>https://linguista.cn/courses/compressive-sensing-lecture-notes/</link><pubDate>Sun, 01 Jul 2007 00:00:00 +0800</pubDate><guid>https://linguista.cn/courses/compressive-sensing-lecture-notes/</guid><description>&lt;h1 id="压缩感知-讲义"&gt;压缩感知-讲义&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文是关于压缩感知理论的讲座笔记，介绍了一种突破传统奈奎斯特采样定理限制的信号采集方法。通过利用信号的稀疏性或可压缩性，采用线性测量矩阵与优化重构算法，以远低于奈奎斯特率的测量次数完成信号恢复。文中详细阐述了测量矩阵的稳定性条件，包括约束等距性和不相干性等核心概念。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;压缩感知（Compressive Sensing）——一种利用信号稀疏性，通过少量非自适应线性测量直接获取压缩表示并重构原始信号的新型采集框架&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;稀疏表示（Sparse Representation）——信号在某组基下仅由少数非零系数表示的特性，是压缩感知理论成立的核心前提&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;约束等距性（Restricted Isometry Property, RIP）——要求测量矩阵对所有稀疏向量近似保持其范数不变的条件，是保证稳定重构的充分条件&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;不相干性（Incoherence）——测量矩阵与稀疏化基之间互不可稀疏表示的性质，是确保测量有效捕获稀疏信号信息的另一关键条件&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;变换编码（Transform Coding）——传统的先采样后压缩方法，通过变换到稀疏基域保留大系数丢弃小系数实现压缩，是压缩感知所要改进的对象&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;好的，这是您要求的《Compressive Sensing》文章的全文翻译，使用Markdown格式，参考文献部分保留原文：&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id="压缩感知-compressive-sensing"&gt;压缩感知 (Compressive Sensing)&lt;/h1&gt;
&lt;p&gt;Richard Baraniuk，莱斯大学 (Rice University)&lt;/p&gt;
&lt;p&gt;[《IEEE Signal Processing Magazine》讲座笔记] 第24卷，2007年7月&lt;/p&gt;
&lt;h1 id="1-范围-scope"&gt;1 范围 (Scope)&lt;/h1&gt;
&lt;p&gt;香农/奈奎斯特采样定理告诉我们，为了在对信号进行均匀采样时不丢失信息，我们的采样频率必须至少是其带宽的两倍。在许多应用中，包括数字图像和视频相机，奈奎斯特率可能非常高，导致我们最终得到过多的样本，必须进行压缩才能存储或传输。在其他应用中，包括成像系统（医学扫描仪、雷达）和高速模数转换器，将采样率或密度提高到超越当前技术水平是非常昂贵的。&lt;/p&gt;
&lt;p&gt;在本讲座中，我们将学习一种利用压缩感知 [1, 2] 来解决这些问题的新技术。我们将用一种更通用的线性测量方案结合优化方法来取代传统的采样和重构操作，以便以显著低于奈奎斯特率的速率获取某些类型的信号。&lt;/p&gt;
&lt;h1 id="2-相关性-relevance"&gt;2 相关性 (Relevance)&lt;/h1&gt;
&lt;p&gt;这里提出的思想可以用来阐明数据采集、线性代数、基展开、逆问题、压缩、降维和优化之间的联系，适用于从本科或研究生数字信号处理到统计学和应用数学的各种课程。&lt;/p&gt;
&lt;h1 id="3-先决条件-prerequisites"&gt;3 先决条件 (Prerequisites)&lt;/h1&gt;
&lt;p&gt;理解和讲授这些材料所需的先决条件是线性代数、基础优化和基础概率论。&lt;/p&gt;
&lt;h1 id="4-问题陈述-problem-statement"&gt;4 问题陈述 (Problem Statement)&lt;/h1&gt;
&lt;p&gt;奈奎斯特率采样通过利用信号的带限性来完全描述信号。我们的目标是通过利用信号的可压缩性来减少完全描述信号所需的测量次数。特殊之处在于，我们的测量不是点采样，而是信号的更一般的线性泛函。&lt;/p&gt;
&lt;p&gt;考虑一个实值、有限长度、一维的离散时间信号 $\mathbf { x }$，我们将其视为 $\mathbb { R } ^ { N }$ 中的一个 $N \times 1$ 列向量，其元素为 $x [ n ]$，$n = 1 , 2 , \ldots , N$。对于图像或更高维的数据，我们通过将其向量化为一个长的一维向量来处理。&lt;/p&gt;</description></item></channel></rss>