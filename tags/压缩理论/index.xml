<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>压缩理论 on Linguista</title><link>https://linguista.cn/tags/%E5%8E%8B%E7%BC%A9%E7%90%86%E8%AE%BA/</link><description>Recent content in 压缩理论 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E5%8E%8B%E7%BC%A9%E7%90%86%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><item><title>Ilya Sutskever演讲 - 从压缩视角理解无监督学习的理论框架</title><link>https://linguista.cn/rosetta/technology/ilya-sutskever-unsupervised-learning-compression-theory/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/ilya-sutskever-unsupervised-learning-compression-theory/</guid><description>&lt;h1 id="ilya-sutskever演讲---从压缩视角理解无监督学习的理论框架"&gt;Ilya Sutskever演讲 - 从压缩视角理解无监督学习的理论框架&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;Ilya Sutskever在本次演讲中探讨了无监督学习的理论基础，提出压缩是理解无监督学习的关键。他从监督学习的数学保证出发，引入Kolmogorov复杂性作为终极压缩器的概念，论证了通过联合压缩未标记数据与目标数据可以揭示共享结构，从而实现有效的无监督学习。演讲还结合iGPT实验验证了该理论，并讨论了线性表征涌现等开放问题。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Kolmogorov复杂性 - 衡量数据最短程序描述长度的理论概念，代表理想的终极压缩器，虽不可计算但为无监督学习提供了理论上界&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;压缩即预测 - 压缩与预测在数学上等价，好的压缩器必然能捕捉数据规律，大型语言模型通过下一token预测本质上在执行压缩任务&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PAC学习 - 概率近似正确学习理论，为监督学习提供了严格的数学保证，即低训练误差加有限自由度可确保泛化成功&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iGPT - OpenAI于2020年提出的图像生成预训练模型，通过下一像素预测在视觉领域验证了压缩理论对无监督学习的解释力&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;联合压缩 - 通过同时压缩未标记数据X和标记数据Y来发现两者共享的隐藏结构，是将压缩理论应用于无监督学习的核心机制&lt;/strong&gt;：&lt;/p&gt;
&lt;h3 id="讲座介绍"&gt;讲座介绍&lt;/h3&gt;
&lt;p&gt;在这场深度思考的演讲中，Ilya Sutskever探讨了无监督学习这一人工智能领域核心挑战的理论基础。与监督学习相对成熟的数学框架不同，无监督学习的内在机制和成功保证长期以来显得较为模糊。Sutskever提出，理解无监督学习的关键可能在于“压缩”这一概念。&lt;/p&gt;
&lt;p&gt;他首先回顾了监督学习的成功要素，并指出了无监督学习在缺乏明确指导信号时面临的根本困惑。随后，他介绍了一种早期的、有理论保证的无监督学习方法——分布匹配，并承认其在实际应用中的局限性。演讲的核心论点是，一个理想的压缩器，特别是理论上的Kolmogorov压缩器，能够通过联合压缩未标记数据和有标记（或目标）数据，揭示它们之间的共享结构，从而实现有效的无监督学习。Sutskever进一步将这一高度抽象的理论与现实中的大型神经网络联系起来，认为随机梯度下降（SGD）在某种程度上是在进行程序搜索，而大型模型则是在逼近这种理想的压缩能力。&lt;/p&gt;
&lt;p&gt;为了验证这一思想，他展示了OpenAI在2020年关于iGPT的工作，证明了在视觉领域通过“下一像素预测”（一种压缩形式）可以获得强大的无监督学习表征。尽管该理论为无监督学习提供了一个富有洞察力的视角，Sutskever也坦诚地讨论了其局限性，例如它并未完全解释线性表征的涌现，并对未来研究方向提出了展望。本次演讲为我们提供了一个重新审视和理解无监督学习本质的独特框架。&lt;/p&gt;
&lt;h3 id="内容纲要"&gt;内容纲要&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 引言与背景
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 演讲者与主题介绍 (Ilya Sutskever, 无监督学习的旧成果)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 当前研究重点 (AI对齐)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 学习理论基础回顾
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 学习的本质与可行性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 监督学习 (Supervised Learning)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义与数学保证 (PAC学习, 统计学习理论)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 成功条件 (低训练损失, 自由度 &amp;lt; 训练集大小, 同分布假设)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 对VC维度的评论 (处理无限精度参数, 实际有限精度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 无监督学习的挑战与困惑
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义与目标 (发现隐藏结构)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 缺乏类似监督学习的保证
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 核心困惑 (优化一个目标，关心另一个；均匀分布下的失效)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 一种有保证的无监督学习方法：分布匹配 (Distribution Matching)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 核心思想 (函数f使得F(X)分布 ≈ Y分布)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 应用示例 (机器翻译, 密码破译)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 局限性 (设置理想化)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 无监督学习的核心思想：压缩 (Compression)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 压缩与预测的等价性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 核心思想实验 (联合压缩X和Y, $C(X \text{concat} Y)$ vs $C(X) + C(Y)$)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ │ └── 共享结构/算法互信息
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 通过压缩实现无监督学习的形式化：遗憾 (Regret)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 低遗憾意味着充分利用未标记数据
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── Kolmogorov复杂性：终极压缩器
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义 ($K(X)$ - 输出X的最短程序长度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 作为最佳压缩器的特性 (模拟其他压缩器, 不可计算性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 与神经网络的联系 (SGD作为程序搜索, NN作为微型Kolmogorov压缩器近似)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 条件Kolmogorov复杂性：无监督学习的理论解
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 定义 ($K(Y|X)$ - 给定X输出Y的最短程序长度)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 作为无监督学习的解决方案 (终极低遗憾, 不可计算)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 联合压缩与条件压缩的等价性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 应用条件Kolmogorov复杂性的挑战 (对大数据集条件化困难)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 重要结果 ($K(X,Y)$ 在预测Y上 ≈ $K(Y|X)$)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 最大似然估计与联合压缩 (自然契合机器学习)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 理论的实证验证：iGPT
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── GPT模型与压缩理论的关系 (可解释, 但非唯一解释)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 在视觉领域寻求直接验证 (iGPT, 2020)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 方法 (图像转像素序列, Transformer进行下一像素预测)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 结果 (CIFAR-10, ImageNet上的表现, 显示可扩展性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;├── 关于线性表征的思考
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 压缩理论的局限性 (不直接解释线性可分性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ ├── 线性表征的普遍性
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;│ └── 观察与推测 (自回归模型 vs BERT, 长程结构的重要性)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;└── 问答环节要点
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 关于推测的详细说明 (下一像素预测 vs BERT掩码预测)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 鲁棒的二维下一像素预测 (扩散模型等概率模型)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── Kolmogorov复杂性与神经网络训练动态的差异
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 密码学中分布区分、预测与压缩的联系
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── VC维度的辩护 (Scott Aaronson的例子)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── Transformer/SGD作为最佳压缩程序的理解 (数据集总损失)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 压缩框架、iGPT与线性表征的关系 (线性表征是额外好处)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 无监督理论对监督学习的启示 (函数类别, 忽略计算成本)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 自回归建模的重要性 (与其他最大似然方法比较)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── GPT-4作为压缩器与压缩器自身大小的问题 (固定数据集 vs 无限测试集)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ├── 使用gzip和k-NN进行文本分类的讨论 (gzip压缩能力有限)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; └── 课程学习效应 (与架构优化难度相关)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id="ilya-sutskever关于大型语言模型llm与未来的演讲"&gt;Ilya Sutskever关于大型语言模型（LLM）与未来的演讲&lt;/h1&gt;
&lt;h2 id="一-引言与背景"&gt;一、 引言与背景&lt;/h2&gt;
&lt;p&gt;YEJIN CHOI: 它的引用次数达到了六位数，超过了139,000次，甚至更多。所以非常期待听到Ilya关于LLM和未来的分享。交给你了。&lt;/p&gt;</description></item><item><title>关于《对泛化的一个观察》的笔记</title><link>https://linguista.cn/rosetta/technology/notes-on-an-observation-on-generalization/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/notes-on-an-observation-on-generalization/</guid><description>&lt;h1 id="关于对泛化的一个观察的笔记"&gt;关于《对泛化的一个观察》的笔记&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文是对 Ilya Sutskever 在2023年西蒙斯研究所研讨会演讲的详细笔记。演讲从压缩理论的视角解释无监督学习为何有效，核心论点是预测与压缩之间存在一一对应关系。通过引入柯尔莫戈洛夫复杂度、条件复杂度和联合压缩等概念，论证了好的无监督学习算法本质上是好的压缩器，并最终将联合压缩与最大似然估计联系起来。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;压缩与预测的等价性&lt;/strong&gt;：所有压缩器和所有预测器之间存在一一对应关系，能更好预测下一个字符就能更好压缩数据，这为理解无监督学习提供了理论基础&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;柯尔莫戈洛夫复杂度&lt;/strong&gt;：一个对象的柯尔莫戈洛夫复杂度是能输出该对象的最短程序的长度，它代表了理论上的终极压缩器，虽然不可计算但为分析提供了理论上界&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;条件柯尔莫戈洛夫复杂度&lt;/strong&gt;：K(Y|X) 衡量在已知数据集 X 的情况下描述 Y 所需的最短程序长度，它形式化了无监督数据对下游任务的价值&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分布匹配&lt;/strong&gt;：在没有对应关系的两个数据源之间寻找映射函数，使一个数据源的变换分布近似另一个数据源的分布，是理解无监督学习的重要桥梁&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;联合压缩与最大似然估计&lt;/strong&gt;：将两个数据集拼接后联合压缩等价于最大似然估计，这将压缩理论的分析框架与机器学习的标准训练范式统一起来&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文链接：&lt;a href="https://sumanthrh.com/post/notes-on-generalization/"&gt;Notes on &amp;ldquo;An Observation on Generalization&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;深入探讨 Ilya Sutskever 在 2023 年西蒙斯研究所大型语言模型与 Transformer 研讨会上的演讲“对泛化的一个观察”。&lt;/p&gt;
&lt;p&gt;Ilya Sutskever，OpenAI 的联合创始人之一，最近在 2023 年西蒙斯研究所（Simons Institute）的大型语言模型与 Transformer 研讨会上发表了演讲。该演讲已被录制，并&lt;a href="https://www.youtube.com/live/AKMuA_TVz3A?si=vBMLcQcKzXPpFeom"&gt;可在 YouTube 上观看&lt;/a&gt;。演讲的主题是“对泛化的一个观察”（An Observation on Generalization），Ilya 探讨了我们如何利用压缩理论的视角来理解无监督学习。我觉得这次演讲非常有见地，但有时可能难以跟上思路，更难把握整体脉络，至少对于像我这样（大脑皮层功能没那么强大）的人来说是这样。不幸的是，这是研讨会中少数几个没有附带详细论文的演讲之一。我觉得这是一个很好的机会，可以写下我从演讲中做的笔记。鉴于 Ilya 阐述其理论的方式非常精彩，很难真正用不同的方式来解释和呈现。因此，我首先&lt;em&gt;转录&lt;/em&gt;了演讲内容，然后添加了我自己的笔记，以提供更好的背景信息或更详细的说明以求清晰。&lt;/p&gt;
&lt;h1 id="对泛化的一个观察"&gt;对泛化的一个观察&lt;/h1&gt;
&lt;p&gt;这次演讲的核心是试图真正理解为什么无监督学习能够奏效，并对其进行数学上的推理。为了达到这个目的，Ilya 首先提出了学习本身（从数据中学习）的概念以及为什么机器学习会起作用。这里的期望是数据具有规律性，而机器学习模型被&lt;em&gt;期望&lt;/em&gt;学习我们数据中的这种规律性。谈到监督学习，他提出了这个等式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;低训练误差 + 训练数据量 &amp;gt; “自由度” = 低测试误差&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;现在，对于无监督学习，我们的梦想是，给定所有这些未标记的数据（图像、文本块等），我们期望机器学习模型能够发现数据中“真实”的、隐藏的结构。无监督学习的一个关键点是，我们通常优化一个代理目标（例如下一个词预测或重构），而我们关心的是一个不同的目标（例如学习数据中的隐藏模式以进行序列分类）。这为什么会奏效呢？&lt;/p&gt;
&lt;h2 id="通过分布匹配进行无监督学习"&gt;通过分布匹配进行无监督学习&lt;/h2&gt;
&lt;p&gt;为了理解这一点，我们首先来看分布匹配。考虑两个数据源 X 和 Y，它们之间没有任何对应关系。这可能就像是不同语言的数据集（比如英语和法语），样本之间没有对应。分布匹配的思想是找到一个映射 F，使得：&lt;/p&gt;
&lt;p&gt;distribution(F(X)) ∼ Y&lt;/p&gt;
&lt;p&gt;在我们上面的例子中，就是：distribution(F(English)) ∼ French&lt;/p&gt;
&lt;p&gt;之前已经提出了许多方法（一个相关的例子：&lt;a href="https://arxiv.org/abs/1711.00043"&gt;无监督机器翻译&lt;/a&gt;），表明即使对于高维度的 X 和 Y，这种方法也是可行的。&lt;/p&gt;</description></item></channel></rss>