<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPU on Linguista</title><link>https://linguista.cn/tags/gpu/</link><description>Recent content in GPU on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 13 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/gpu/index.xml" rel="self" type="application/rss+xml"/><item><title>动态GPU分配技术详解</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/dynamic-gpu-fractions-explained/</link><pubDate>Mon, 13 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p4/dynamic-gpu-fractions-explained/</guid><description>&lt;h1 id="动态gpu分配技术详解"&gt;动态GPU分配技术详解&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;随着人工智能和高性能计算需求的爆炸式增长，GPU已成为支撑复杂计算任务的关键基础设施。然而，传统静态GPU资源分配模式导致资源利用率低下，造成计算资源闲置浪费。动态GPU分配技术应运而生，它允许AI工作负载根据瞬时需求动态请求GPU资源，实现按需分配、弹性伸缩的资源管理模式。本文系统阐述了动态GPU分配的核心机制、关键特性及未来发展趋势，为企业和开发者优化GPU资源配置提供参考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文围绕动态GPU分配技术展开系统性论述，首先分析了传统GPU资源分配方式的局限性，引出动态分配的技术背景。核心部分详细阐述了动态GPU分配的两大核心机制：GPU分配比例（工作负载保证获得的最小GPU资源比例）和GPU利用率上限（工作负载最多可使用的GPU资源比例），通过具体配置示例说明其工作原理。&lt;/p&gt;
&lt;p&gt;文章进一步解析了动态GPU分配的四大关键特性：资源利用率最大化、加速执行与提升生产力、灵活性与可扩展性，以及平滑切换GPU资源归属。这些特性共同构成了动态GPU分配技术价值体系，使其成为提升GPU集群整体利用效率的有效手段。&lt;/p&gt;
&lt;p&gt;最后，文章展望了动态GPU分配技术的三大未来发展方向：更精细化的资源分配与调度（包括细粒度资源切分和智能化调度策略）、与云原生技术的深度融合（如Kubernetes集成），以及支持异构计算环境（多架构GPU及其他加速器统一管理）。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GPU分配比例&lt;/strong&gt;：指工作负载保证获得的最小GPU资源比例，这是资源配置的下限保障。例如配置0.25的分配比例意味着该工作负载始终能获得25%的GPU算力资源，确保基本运行需求得到满足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU利用率上限&lt;/strong&gt;：指工作负载最多可以使用的GPU资源比例，这是资源配置的上限限制。当集群资源有剩余时，工作负载可动态扩展至上限比例，实现资源的弹性伸缩和高效利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;细粒度资源切分&lt;/strong&gt;：未来动态GPU分配技术将实现更精细的GPU资源分配单位，不仅限于算力比例，还包括显存、底层计算单元等维度的精确切分，进一步减少资源浪费，提升异构工作负载并存时的资源利用效率。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/W28AMNxoOnajbFyJ8ETAkg"&gt;Dynamic GPU Fractions（动态 GPU 分配），知多少？&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;-&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;-&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>GPU与CUDA简介</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/gpu-cuda-introduction-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/gpu-cuda-introduction-guide/</guid><description>&lt;h1 id="gpu与cuda简介"&gt;GPU与CUDA简介&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文是一篇关于GPU与CUDA基础知识的系统介绍，从计算机基础概念出发，逐步深入到GPU硬件架构和CUDA编程模型。文章首先讲解了计算机基础知识，包括IO设备、CPU与内存、并行与并发、编译器以及堆与栈等核心概念。接着详细介绍GPU的基础知识，包括GPU作为计算设备的特性、流多处理器执行单元、GPU产品架构分类以及GPU内存层级结构。最后重点阐述CUDA编程平台和模型，涵盖CUDA编程基础、线程模型、内存管理和nvcc编译器等内容。整篇文章为理解GPU并行计算和CUDA编程提供了完整的知识框架。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章采用递进式的知识结构，从计算机基础到GPU专业知识，再到CUDA编程实践，形成了完整的学习路径。第一部分计算机基础知识为后续内容奠定基础，帮助读者理解CPU、内存、并行并发等核心概念。第二部分GPU基础知识介绍GPU的独特架构和计算优势，特别是流多处理器SM的设计使GPU能够大规模并行执行任务。第三部分CUDA编程基础是文章的核心内容，详细解释了CUDA的线程模型、内存管理和编译流程。&lt;/p&gt;
&lt;p&gt;文章强调GPU与CPU的根本区别在于并行计算能力。一个CPU核心在同一时间点只能执行一个任务，而一个GPU的流多处理器SM可以同时执行多达32个任务。这种设计使GPU在处理大规模并行计算任务时具有显著优势，特别适合深度学习、科学计算等领域。CUDA作为英伟达推出的通用并行计算平台和编程模型，为开发者提供了使用GPU进行并行计算的标准化方法。&lt;/p&gt;
&lt;p&gt;文章还指出计算机知识体系的特点是会抽象出很多概念帮助读者理解，但真实运作方式往往更加复杂。计算机领域从业者喜欢&amp;quot;起名&amp;quot;来突出产品亮点，这在GPU和CUDA的术语体系中表现得尤为明显。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;流多处理器SM&lt;/strong&gt;：GPU的基本执行单元，是GPU实现大规模并行计算的关键。一个SM在同一时间点最多可以执行32个任务，而一个CPU Core只能执行1个任务。SM中包含多个CUDA Core作为实际运算单元，这种设计使GPU能够同时处理大量并行任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUDA线程模型&lt;/strong&gt;：CUDA编程的核心概念，采用嵌套式结构。多个线程构成线程块，多个线程块构成网格。Grid和Block可以是一维、二维或三维数组结构。在核函数中，通过gridDim、blockIdx、blockDim、threadIdx等对象来管理和协调线程的执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU内存层级&lt;/strong&gt;：GPU的内存系统采用多级缓存结构，从高到低依次是寄存器文件、L1 Cache、L2 Cache和DRAM。寄存器文件和L1 Cache属于片上内存，存在于SM上；L2 Cache和DRAM属于片外内存。当寄存器内存不足时会发生溢出，溢出部分称为本地内存。共享内存存在于L1 Cache之上，供同一个block中的所有线程访问，用于减少访问高带宽内存HBM的次数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主机与设备&lt;/strong&gt;：在GPU编程中，将计算机称为主机，GPU称为设备。两者之间是异步执行关系，通过PCIe总线连接。CUDA编程需要明确区分主机函数和设备函数，使用__host__和__device__修饰符来标识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nvcc编译器&lt;/strong&gt;：CUDA程序的专用编译器，编译时将代码分为主机代码和设备代码。主机代码的编译过程与gcc类似，设备代码会先转换成PTX伪汇编代码，再编译成cubin形式的目标代码。这种分离式编译设计使得CUDA能够充分利用GPU的并行计算能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://zhuanlan.zhihu.com/p/686772546"&gt;GPU 与 CUDA 简介 - 知乎&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;知乎作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-01-10&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>MI300X、H100与H200训练性能对比：CUDA护城河依然坚固</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</guid><description>&lt;h1 id="mi300xh100与h200训练性能对比cuda护城河依然坚固"&gt;MI300X、H100与H200训练性能对比：CUDA护城河依然坚固&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于SemiAnalysis团队为期五个月的独立基准测试研究，全面对比了AMD MI300X与NVIDIA H100/H200在AI模型训练场景下的实际性能表现。测试涵盖GEMM计算性能、HBM内存带宽、单节点与多节点训练吞吐量、集体通信效率等核心指标。研究结果表明，尽管MI300X在纸面规格上具有一定优势，但由于软件栈的诸多问题，其实际训练性能显著落后于NVIDIA产品。NVIDIA的CUDA生态系统护城河依然坚固，而AMD在开箱即用体验、软件稳定性、性能优化等方面仍需大量改进工作。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先介绍了研究的背景和方法论，SemiAnalysis团队与NVIDIA和AMD进行了深度互动，进行了为期五个月的独立测试，旨在反映真实用户在实际使用中可能遇到的情况。研究不仅关注纯性能指标，还特别重视用户体验和可用性，这是评估AI基础设施产品的重要维度。&lt;/p&gt;
&lt;p&gt;文章的核心发现部分揭示了AMD和NVIDIA产品在实际应用中的巨大差距。尽管MI300X的总拥有成本较低，但在按TCO计算的训练性能方面，使用AMD公共稳定版软件的表现不如H100/H200。关键问题在于AMD的软件体验存在诸多缺陷，导致MI300X的实际训练性能远低于其纸面规格。在BF16精度下，H100/H200的GEMM性能约为720 TFLOP/s，而MI300X仅为约620 TFLOP/s，比其市场宣传的1307 TFLOP/s低得多，比H100/H200慢14%。在FP8精度下，差距更为明显，H100/H200达到约1280 TFLOP/s，MI300X仅为约990 TFLOP/s，慢22%。&lt;/p&gt;
&lt;p&gt;文章深入分析了性能差距的技术原因，从GEMM性能、集体通信操作、网络拓扑等多个维度进行了详细对比。特别值得关注的是，研究指出了当前流行的GEMM基准测试方法存在严重缺陷，许多测试未正确清除L2缓存，且仅取最大性能值而非迭代过程中的中位数或平均值。在多节点训练场景下，H100的性能优势更加明显，这主要得益于NVIDIA的NVLink拓扑、InfiniBand SHARP技术以及高度优化的NCCL集体通信库。&lt;/p&gt;
&lt;p&gt;最后，文章对AMD提出了具体的改进建议，包括增加软件工程资源投入、改进GEMM库的启发式模型、减少对环境标志的依赖、加强内部测试流程等。作者指出，AMD许多库是基于NVIDIA开源或生态系统库分叉而来，这种策略长期来看不利于建立独立健康的软件生态系统。文章还提供了不同网络配置下GPU集群的成本和性能分析，为读者提供了全面的TCO视角。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GEMM性能与Transformer训练&lt;/strong&gt;：通用矩阵乘法是衡量Transformer架构模型训练性能的核心指标，因为现代深度学习模型的计算负载中，矩阵乘法操作占据了主导地位。研究显示，AMD MI300X在BF16和FP8精度下的GEMM性能均显著低于NVIDIA H100/H200，这是导致其实际训练性能落后于纸面规格的关键原因之一。值得注意的是，AMD的torch.matmul和F.Linear API在性能上存在差异，这是因为底层使用了不同的GEMM库，这种不一致性增加了用户的优化负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;集体通信操作与多节点训练&lt;/strong&gt;：在大规模分布式训练中，all_reduce、all_gather、reduce_scatter等集体通信操作的效率直接影响整体训练性能。NVIDIA的NCCL库经过多年优化，在单节点和多节点环境下都表现出色，特别是在配合NVLink和InfiniBand SHARP技术时，能够实现近乎线性的扩展性能。相比之下，AMD的RCCL库性能明显不足，在多节点场景下的差距尤为突出，这严重限制了MI300X在大规模训练任务中的竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUDA生态系统护城河&lt;/strong&gt;：NVIDIA的真正优势不仅体现在硬件性能上，更重要的是其软件生态系统的完整性。从开箱即用的PyTorch体验、高度优化的cuBLASLt库、智能的算法启发式模型，到完善的开发工具和文档，NVIDIA为用户提供了无缝的开发和部署体验。反观AMD，用户需要处理大量环境标志配置、手动调优、依赖库从源代码构建等复杂操作，这种体验差异构成了CUDA生态系统的深层次护城河。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总拥有成本与实际训练性能&lt;/strong&gt;：虽然MI300X的硬件价格较低，总拥有成本具有优势，但如果按TCO计算的实际训练性能来评估，使用AMD公共稳定版软件的性价比并不如NVIDIA产品。文章强调，即使使用AMD的定制开发构建能够改善性能，但等到这些构建成熟可用时，NVIDIA的下一代产品可能已经上市。这揭示了AI基础设施评估中一个重要原则：不能仅看硬件规格和价格，必须综合考虑软件成熟度、开发效率、时间成本等因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;软件质量与用户体验&lt;/strong&gt;：本文反复强调的一个核心观点是，AI训练硬件的竞争本质上是软件生态系统的竞争。AMD存在大量内部测试不足导致的软件问题，如PyTorch原生Flash Attention内核性能极低、注意力层与torch.compile兼容性问题、PYTORCH_TUNABLE_OPS标志导致的内存泄漏和程序崩溃等。这些看似细微的软件缺陷累积起来，会严重影响用户体验，降低开发效率，最终影响产品的市场竞争力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/#executive-recommendation-to-amd"&gt;MI300X vs H100 vs H200 Benchmark Part 1: Training – CUDA Moat Still Alive&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;SemiAnalysis&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-22&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>NVIDIA GPU核心详解：CUDA、Tensor与光线追踪核心架构</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-gpu-cores-cuda-tensor-raytracing-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-gpu-cores-cuda-tensor-raytracing-guide/</guid><description>&lt;h1 id="nvidia-gpu核心详解cudatensor与光线追踪核心架构"&gt;NVIDIA GPU核心详解：CUDA、Tensor与光线追踪核心架构&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文全面解析了NVIDIA GPU的三种核心架构：CUDA Cores作为并行计算的基石，负责处理大规模浮点运算和整数运算；Tensor Cores专为深度学习设计，通过混合精度计算实现AI任务的高效加速；Ray-Tracing Cores则是光线追踪渲染技术的专用核心。三者通过共享硬件资源和统一编程模型实现协同工作，在深度学习、游戏渲染和科学计算等领域发挥各自优势，共同推动GPU性能的持续突破。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了NVIDIA GPU Core在人工智能生态中的核心地位，指出现代NVIDIA GPU通过精心设计的多类型核心架构，实现了在计算性能、人工智能和图形渲染等领域的跨越式发展。这三种核心各司其职又紧密协作，构成了GPU强大性能的硬件基础。&lt;/p&gt;
&lt;p&gt;CUDA Cores作为GPU中最基础的处理单元，采用大规模并行设计，能够同时运行数百万个线程，在图像视频处理、科学计算和实时物理模拟等场景中表现卓越。其优势在于核心数量远超传统CPU，通过简化指令流水线实现高吞吐量计算。&lt;/p&gt;
&lt;p&gt;Tensor Cores则是NVIDIA针对AI工作负载的专用解决方案，首次在Volta架构中引入。相比传统CUDA Cores，Tensor Cores在矩阵运算方面具有显著性能优势，特别是在FP16和INT8混合精度计算中，可实现10倍以上的运算加速，成为深度学习训练和推理任务的核心驱动力。&lt;/p&gt;
&lt;p&gt;Ray-Tracing Cores代表了图形渲染技术的革命性突破，从Turing架构开始引入，专门用于加速光线追踪计算。通过硬件加速光线与场景的交互检测、路径追踪和动态光影渲染，实现了高分辨率游戏和虚拟现实场景中的实时光线追踪效果。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;CUDA Cores（并行计算单元）&lt;/strong&gt;：作为GPU的计算基础，CUDA Cores采用SIMT（单指令多线程）架构，能够同时执行大量相同操作。其核心价值在于将传统CPU的串行计算模式转变为大规模并行计算模式，特别适合图像处理、科学计算等可并行化的任务。在现代GPU中，CUDA Cores数量通常达到数千个，通过共享内存和寄存器实现高效的数据交换和任务调度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tensor Cores（张量计算单元）&lt;/strong&gt;：这是NVIDIA对AI计算需求的创新回应，专门为神经网络的矩阵乘法和累加运算而设计。与传统标量计算不同，Tensor Cores采用矩阵运算的并行化思路，一个指令周期内可完成4×4矩阵的乘加运算。在深度学习训练中，前向传播和反向传播都涉及大量矩阵运算，Tensor Cores的专用设计使其在这些场景下的性能达到CUDA Cores的数倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ray-Tracing Cores（光线追踪单元）&lt;/strong&gt;：光线追踪技术通过模拟光线在三维空间中的传播路径来生成逼真的光影效果，但计算复杂度极高。Ray-Tracing Cores通过专用硬件加速光线与三角形求交、包围盒层次结构遍历等关键计算，使得实时光线追踪成为可能。这不仅是图形渲染的技术突破，也为科学可视化、虚拟仿真等领域提供了强大的渲染工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SM（Streaming Multiprocessor）架构&lt;/strong&gt;：这是三种核心协同工作的硬件基础。每个SM模块同时集成CUDA Cores、Tensor Cores和Ray-Tracing Cores，共享L1缓存、寄存器文件和内存接口。NVIDIA的统一编程模型让开发者能够灵活调度三种核心的资源，根据任务类型自动分配最合适的计算单元，实现硬件资源的优化配置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;混合精度计算&lt;/strong&gt;：Tensor Cores的核心技术特征，通过在计算过程中使用FP16或INT8等低精度格式，在保持模型精度的同时显著提升计算速度和内存效率。这种精度与性能的平衡策略，使得大规模深度学习模型的训练和推理在时间和成本上都变得可行，推动了AI技术的快速普及。&lt;/p&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/hIMv0OtLOWhH1_H613ieEw"&gt;一文读懂 NVIDIA GPU Core&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;技术文章&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>为何我们仍然使用CPU而非仅依赖GPU</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/cpu-gpu-architectural-differences-use-cases/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/cpu-gpu-architectural-differences-use-cases/</guid><description>&lt;h1 id="为何我们仍然使用cpu而非仅依赖gpu"&gt;为何我们仍然使用CPU而非仅依赖GPU&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文通过对比CPU与GPU的架构特点和适用场景，揭示了两者在计算机系统中的互补关系。GPU虽然在并行计算能力上远超CPU，但CPU在处理复杂逻辑、任务调度和应对不可预测事件方面具有独特优势，因此现代芯片设计将两者结合以满足不同计算需求。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先通过TFLOPS指标对比了CPU与GPU的原始计算能力，指出GPU的浮点运算性能通常是CPU的30倍以上。但作者强调，这种性能差异只在特定类型的任务上才能体现。&lt;/p&gt;
&lt;p&gt;接着，文章将程序分为顺序程序和并行程序两类。顺序程序如斐波那契数列计算，每一步都依赖前一步的结果，无法并行化；而并行程序如对数字列表进行批量运算，各计算相互独立，可分配给多个处理器同时执行。现实中大型应用通常是两者的混合。&lt;/p&gt;
&lt;p&gt;在架构设计上，CPU采用少量大核心设计，擅长顺序执行和复杂决策，类似餐厅大厨能应对各种突发情况；GPU则拥有众多小核心，适合对大量数据执行相同的并行操作，如视频游戏中的像素渲染。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TFLOPS（每秒万亿次浮点运算）&lt;/strong&gt;：衡量处理器计算能力的指标，Nvidia A100 GPU可达9.7 TFLOPS，而英特尔24核处理器仅0.33 TFLOPS。但这只是理论峰值，实际性能取决于程序类型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;顺序程序与并行程序&lt;/strong&gt;：顺序程序指令必须依次执行，每步依赖前步结果；并行程序指令相互独立，可同时执行。现实应用通常是两者的混合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大核心与小核心架构&lt;/strong&gt;：CPU的少量大核心优化了单线程性能和任务切换能力；GPU的众多小核心优化了吞吐量，适合数据并行任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;任务的灵活调度&lt;/strong&gt;：CPU需要处理各种不可预测事件，如应用启动停止、网络连接中断、文件访问等，其任务切换能力确保系统响应性和资源合理分配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协同计算模式&lt;/strong&gt;：现代计算机中，CPU负责任务管理和资源分配，GPU负责大规模并行计算，两者协同工作以发挥各自优势。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://codingstuff.substack.com/p/if-gpus-are-so-good-why-do-we-still"&gt;If GPUs Are So Good, Why Do We Still Use CPUs At All?&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未知&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>NVIDIA CEO黄仁勋CES 2025主题演讲全文</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-ces-2025-jensen-huang-keynote/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-ces-2025-jensen-huang-keynote/</guid><description>&lt;h1 id="nvidia-ceo黄仁勋ces-2025主题演讲全文"&gt;NVIDIA CEO黄仁勋CES 2025主题演讲全文&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;黄仁勋在CES 2025主题演讲中宣布了RTX 5090系列显卡的发布，采用Blackwell架构，AI性能达到4000 TOPS。他详细介绍了NVIDIA在人工智能领域的最新进展，包括三个AI扩展定律、Agentic AI的概念和应用，以及物理AI的发展愿景。演讲还涵盖了NVIDIA Cosmos世界基础模型的发布、与Windows PC的AI集成计划，以及Project Digits个人AI超级计算机的推出。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本次演讲以NVIDIA从1993年成立至今的技术演进历程开篇，黄仁勋回顾了从GPU发明到CUDA推出，再到AI时代的关键节点。他重点阐述了AI发展的三个阶段：感知AI、生成式AI和Agentic AI，并前瞻性地提出了物理AI的概念。演讲的核心内容围绕GeForce与AI的相互促进展开，展示了RTX 5090系列显卡如何利用AI技术实现神经渲染。&lt;/p&gt;
&lt;p&gt;在AI扩展定律部分，黄仁勋提出了三个重要定律：预训练扩展、后训练扩展和测试时间扩展，这些定律正在推动AI技术的快速发展。他详细介绍了Blackwell芯片的生产情况和GB200 NVLink 72系统的技术规格，强调了能效比和成本效益的显著提升。演讲还重点介绍了Agentic AI在企业应用中的潜力，包括NVIDIA提供的Nims、Nemo和蓝图等构建模块。&lt;/p&gt;
&lt;p&gt;物理AI是本次演讲的重要主题，黄仁勋发布了NVIDIA Cosmos世界基础模型，这是一个基于2000万小时视频训练的物理世界理解模型。他阐述了Cosmos与Omniverse结合将如何推动机器人和工业AI的发展，并展示了在工业数字化、自动驾驶和人形机器人等领域的应用案例。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;神经渲染&lt;/strong&gt;：AI与计算机图形学的融合技术，通过DLSS深度学习超级采样，AI可以预测未渲染的像素，为每帧渲染生成三个额外帧。这使得RTX 5090能够以极高的性能和能效比实现逼真的图像渲染，仅需计算200万像素即可生成3300万像素的完整画面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI扩展定律&lt;/strong&gt;：包括预训练扩展、后训练扩展和测试时间扩展三个层面。预训练扩展指出数据越多、模型越大、计算能力越强，模型性能就越好。后训练扩展通过强化学习和人类反馈提高模型在特定领域的技能。测试时间扩展允许AI在使用过程中动态分配计算资源进行推理和思考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;物理AI&lt;/strong&gt;：理解物理世界并能根据请求生成动作的AI系统。与语言模型处理文本Token不同，物理AI需要理解重力、摩擦力、惯性等物理动力学，以及几何空间关系和因果关系。NVIDIA Cosmos是首个面向物理AI的世界基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三计算机解决方案&lt;/strong&gt;：NVIDIA提出的机器人技术架构，包括用于训练AI的DGX计算机、部署AI的AGX计算机，以及连接二者的数字孪生系统。这一架构正在推动工业数字化、自动驾驶和人形机器人等领域的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Agentic AI&lt;/strong&gt;：由多个模型协同工作的AI系统，能够理解任务、检索信息、使用工具并生成结果。NVIDIA通过Nims（AI微服务）、Nemo（数字员工培训系统）和蓝图帮助企业构建和部署Agentic AI应用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=XASnBeNKg6A"&gt;CES 2025: NVIDIA CEO Jensen Huang delivers keynote speech&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Jensen Huang&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-07&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深入解析NVIDIA GPU产品线及其选型指南</title><link>https://linguista.cn/infos/tldrcards/henrinotes-2025-p1/nvidia-gpu-product-line-selection-guide/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes-2025-p1/nvidia-gpu-product-line-selection-guide/</guid><description>&lt;h1 id="深入解析-nvidia-gpu-产品线及其选型指南"&gt;深入解析 NVIDIA GPU 产品线及其选型指南&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文全面解析了 NVIDIA GPU 产品线的命名体系和选型策略，深入解读了架构代号、性能层级的含义，并通过具体型号对比帮助读者理解不同 GPU 的适用场景，为构建高效的 AI 计算平台提供实用参考。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章从数据中心 GPU 选型的复杂性切入，将 GPU 选择过程类比汽车购买，强调预算、应用场景和性能需求是三大核心考量因素。作者系统性地拆解了 NVIDIA GPU 的命名规则：首字母代表核心架构（如 K-Kepler、T-Turing、A-Ampere、H-Hopper、L-Ada Lovelace），数字则标识性能层级，从入门级的&amp;quot;4&amp;quot;系列到旗舰级的&amp;quot;100&amp;quot;系列形成完整的产品梯度。&lt;/p&gt;
&lt;p&gt;文章通过四组典型型号的对比分析，直观展示了不同代际 GPU 的性能差异：T4 与 L4 体现了从 Turing 到 Ada Lovelace 架构的演进，A100 与 A10 揭示了旗舰级与中端产品的定位差距，K80 与 T4 的对比则说明核心数量并非性能的唯一决定因素。最后还针对模型服务场景，对比了 T4 适合中等规模模型、A10 更胜任大型模型推理的差异化定位。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;架构代号体系&lt;/strong&gt;：NVIDIA GPU 命名规则中的首字母对应不同的微架构设计，从早期的 Kepler（K）、Maxwell（M）、Pascal（P）、Volta（V），到 Turing（T）、Ampere（A）、Hopper（H）及最新的 Ada Lovelace（L）。每代架构在能效比、张量核心设计、显存带宽等关键指标上持续优化，理解代际差异是选型的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性能层级分级&lt;/strong&gt;：数字标识反映了产品在家族中的定位层级。&amp;ldquo;4&amp;quot;系列定位入门或低功耗场景，&amp;ldquo;10&amp;quot;系列专注中端推理优化，&amp;ldquo;40&amp;quot;系列面向高端图形和虚拟工作站，&amp;ldquo;100&amp;quot;系列则是为高性能计算和人工智能打造的旗舰产品。数字越大通常意味着更强的算力、更大的显存和更高的功耗。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;选型决策框架&lt;/strong&gt;：GPU 选型需要综合评估三个维度——预算约束决定可接受的产品档次，应用场景（训练 vs 推理、模型规模、并发需求）指向性能侧重点，性能需求则具体到算力、显存容量、带宽等硬指标。文章强调应避免唯核心数论，需结合架构代际、实际测试数据和应用反馈做出综合判断。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/UGm2-sbegRNQQJa5zlKHlw"&gt;一文读懂 NVIDIA GPU 产品线&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Luga Lee&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-29&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>