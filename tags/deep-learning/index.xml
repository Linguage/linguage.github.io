<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Linguista</title><link>https://linguista.cn/tags/deep-learning/</link><description>Recent content in Deep Learning on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 13 Oct 2025 08:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>nanochat - 信息卡</title><link>https://linguista.cn/info/htmlcards/nanochat-gemini/</link><pubDate>Mon, 13 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/nanochat-gemini/</guid><description>Andrej Karpathy 发布的 nanochat 项目提供了一个约 8,000 行代码的全栈 LLM 训练与推理管线，旨在以最小化依赖构建一个可黑客攻击的 ChatGPT 克隆体。该项目涵盖了从 Rust 分词器、FineWeb 预训练到 SFT 和 RL 的完整流程，强调低成本可达性（最低约 $100）与坚实的基线骨架，非常适合作为深入理解 LLM 工作原理的研究平台与教学压轴项目。</description></item><item><title>nanochat 速览</title><link>https://linguista.cn/info/htmlcards/nanochat-kimi/</link><pubDate>Mon, 13 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/nanochat-kimi/</guid><description>nanochat 是 Andrej Karpathy 推出的一个极简 ChatGPT 克隆训练管道，旨在通过一个单一的、可被黑客攻击的仓库来建立强有力的基线。该项目不仅包含了从预训练到微调、强化学习再到推理的全栈代码，还提供了 WebUI 界面，使开发者能够在短时间内以极低的成本（约 100 美元）复现类似 GPT 的对话模型。作为一个强调可读性和可扩展性的研究利器，它全流程仅依赖约 8000 行核心代码，彻底打通了从 FineWeb 预训练到 SmolTalk 微调的最后一公里。</description></item><item><title>苦涩教训 - Rich Sutton核心解读</title><link>https://linguista.cn/info/htmlcards/bitter_lesson_magazine/</link><pubDate>Sat, 04 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/bitter_lesson_magazine/</guid><description>本文深入解读 Rich Sutton 的经典文章《苦涩教训》，总结了过去 70 年人工智能发展的核心经验：从长远来看，利用通用计算能力的方法最终总是会战胜依赖人类特定领域知识的方法。文章通过国际象棋和围棋等历史案例，揭示了摩尔定律推动下的通用算法（如搜索和学习）如何通过规模化算力实现突破，并警示研究者克服利用自身先验知识的心理陷阱，拥抱计算力的指数级增长。</description></item><item><title>Yann LeCun：自监督学习世界模型与AI未来</title><link>https://linguista.cn/info/htmlcards/yann_lecun_jepa_harvard_2025/</link><pubDate>Fri, 03 Oct 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/yann_lecun_jepa_harvard_2025/</guid><description>Yann LeCun在哈佛CMSA的讲座深入剖析了当前AI技术的局限，指出大语言模型（LLM）并非通往通用人工智能（AGI）的终极路径。他提出自监督学习、JEPA架构和世界模型是实现类人认知与推理能力的关键，通过模拟人类与物理世界的交互方式，AI才能真正理解现实并具备常识推理。</description></item><item><title>AI时代的三次进化</title><link>https://linguista.cn/info/htmlcards/karpathy-from-pre-training-to-reinforcement-learning/</link><pubDate>Sat, 30 Aug 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/karpathy-from-pre-training-to-reinforcement-learning/</guid><description>本文深入解析了人工智能发展的三个关键进化阶段：从依赖海量互联网文本的预训练时代，到对话数据驱动的监督微调，再到基于环境交互的强化学习。文章详细阐述了每个阶段的核心特征与数据要素，揭示了AI从被动理解世界到主动与环境交互的演进轨迹，为理解现代大模型的训练逻辑提供了清晰的框架。</description></item><item><title>AI学习理论的革命：彩票假说</title><link>https://linguista.cn/info/htmlcards/visual-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</link><pubDate>Sat, 23 Aug 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/visual-article-how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/</guid><description>本文深入探讨了深度学习领域中“彩票假说”的意外发现，解释了为何神经网络中大部分参数是冗余的，以及这一发现如何彻底改变了我们对模型训练和优化的传统认知。文章通过可视化的方式，展示了研究人员如何通过剪枝技术发现这些“中彩票”的子网络，并讨论了这一理论对提高 AI 模型效率和降低计算成本的深远影响。</description></item><item><title>弗朗索瓦·肖莱论通往AGI之路</title><link>https://linguista.cn/info/htmlcards/chollet-path-to-agi/</link><pubDate>Tue, 08 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/chollet-path-to-agi/</guid><description>本文深入探讨了François Chollet对当前AI“规模化”范式的批判。通过可视化图表展示了模型参数增长与ARC基准测试得分之间的脱节，揭示了单纯增加算力无法通向通用人工智能。Chollet重新定义了智能为“程序合成”而非“技能记忆”，并提出了基于元学习的新型架构作为未来实现AGI的潜在路径。</description></item><item><title>AI 的当下与未来: Ilya Sutskever 与 Jensen Huang 炉边谈话</title><link>https://linguista.cn/info/htmlcards/ilya-jensenhuang-2023/</link><pubDate>Sat, 05 Jul 2025 08:00:00 +0800</pubDate><guid>https://linguista.cn/info/htmlcards/ilya-jensenhuang-2023/</guid><description>这场炉边谈话深入探讨了人工智能从早期边缘概念到现代技术核心的演变历程。Ilya Sutskever 回溯了深度学习的起源，特别是 AlexNet 如何点燃了现代 AI 的“大爆炸”。对话不仅揭示了神经网络规模对性能的指数级影响，还讨论了 OpenAI 核心理念的融合与演进。通过 GPT-3.5 与 GPT-4 在高难度测试中的量化对比，文章具象化了多模态能力的飞跃。最后，两位行业巨擘展望了 AGI 的未来，分析了数据墙、推理能力及能源等核心挑战。</description></item></channel></rss>