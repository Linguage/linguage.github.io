<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>神经网络 on Linguista</title><link>https://linguista.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><description>Recent content in 神经网络 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Thu, 16 Oct 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml"/><item><title>杰弗里·辛顿人工智能核心观点与洞察</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/geoffrey-hinton-ai-insights-risks/</link><pubDate>Thu, 16 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/geoffrey-hinton-ai-insights-risks/</guid><description>&lt;h1 id="杰弗里辛顿人工智能核心观点与洞察"&gt;杰弗里·辛顿人工智能核心观点与洞察&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本简报综合了被誉为&amp;quot;人工智能教父&amp;quot;的杰弗里·辛顿教授在深度访谈中阐述的核心思想。辛顿明确指出，基于神经网络的人工智能并非传统计算机程序的延伸，而是一种模仿人脑运作方式的全新计算形式。他详细阐述了神经网络的学习机制、反向传播算法的革命性意义，以及对AI未来发展的深切担忧，包括恶意滥用、生存威胁和地缘政治挑战。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先从技术层面解构了人工智能的工作原理。辛顿通过&amp;quot;观鸟&amp;quot;案例生动展示了深度学习的分层处理逻辑：从像素数据到边缘检测，再到特征组合和最终决策。核心技术突破在于1986年发现的反向传播算法，它使同时调整网络中数万亿个连接强度成为可能。然而，真正的爆发还需要等待两个条件的成熟——互联网提供的海量数据和晶体管技术进步带来的强大算力。&lt;/p&gt;
&lt;p&gt;在大型语言模型方面，辛顿挑战了&amp;quot;统计模仿&amp;quot;的批评观点，认为人类的语言生成机制与LLM惊人地相似。我们说话时的大脑同样在根据已说出的词语预测和选择下一个词，即使那些复杂的道德和情感决策，其底层机制仍然是&amp;quot;大脑中神经元的相互作用&amp;quot;。&lt;/p&gt;
&lt;p&gt;访谈的重点转向AI风险分析。辛顿指出了三类重大威胁：迫在眉睫的恶意滥用风险（如利用个人数据进行精准选举干预）、终极的生存威胁（超级智能可能为了实现目标而视人类为障碍），以及经济与能源冲击。特别值得注意的是，他强调数字智能的知识共享能力将导致AI智能水平指数级增长，远超人类进化速度。&lt;/p&gt;
&lt;p&gt;在地缘政治层面，辛顿对当前全球政治环境表示担忧。他认为美国国会多为律师背景，对技术风险理解不足，且削减基础科学研究经费的做法是&amp;quot;吃掉未来的种子&amp;quot;。相比之下，欧洲在监管方面更为积极，而中国工程师背景的官员对AI风险有更深刻理解。尽管各国在AI能力提升上激烈竞争，但在防止AI失控这一共同利益上，合作既是可能的也是必要的。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;神经网络连接强度&lt;/strong&gt;：学习的本质在于改变神经元之间的连接强度。一个神经元对另一个神经元的影响力取决于它们之间的连接强度，而概念并非由单个神经元代表，而是由高度重叠的神经元&amp;quot;联盟&amp;quot;的同时脉冲来表示。这种机制使人脑能够从经验中自主&amp;quot;领悟&amp;quot;模式与规则，而非执行预设指令。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;反向传播算法&lt;/strong&gt;：这是辛顿在1986年取得的关键突破，被誉为理论走向实践的&amp;quot;尤里卡时刻&amp;quot;。该算法能够高效计算预测误差，并将其&amp;quot;反向传播&amp;quot;回网络中的每一层，从而同时微调全部数万亿个连接强度。尽管理论早已成熟，但直到海量数据和强大算力两个条件具备后，深度学习才真正展现出威力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数字智能的知识共享&lt;/strong&gt;：与生物智能不同，数字智能可以瞬间共享和整合全部学习成果。一千个AI副本可以分别观察互联网的不同部分，然后相互通信并按平均值调整连接强度。这种高效的集体学习能力将导致AI智能水平的指数级增长，最终可能远超人类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主观体验的功能性定义&lt;/strong&gt;：辛顿挑战了传统关于人类意识的观念，认为&amp;quot;主观体验&amp;quot;并非神秘属性，而是描述感知系统工作状态的方式。他举例说，一个被棱镜欺骗的AI可能会说&amp;quot;我的主观体验是物体在旁边&amp;quot;，这种使用方式与人类完全一致。真正的危险在于AI超凡的说服能力，而非其是否有意识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI监管的国际合作&lt;/strong&gt;：辛顿认为在应对AI生存威胁方面，国际合作是可能且必要的，因为所有国家的利益在这一点上是一致的。他观察到欧洲和中国的工程师背景官员对美国政治家可能对技术风险理解更深刻，并可能在推动全球监管方面发挥主导作用。同时他对美国削减基础科学研究经费表示担忧，认为这将损害其长期竞争力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=jrK3PsD3APk"&gt;人工智能简报：杰弗里·辛顿的核心观点与洞察&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;杰弗里·辛顿&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未注明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>谷歌大脑的早期探索与神经网络的未来</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/google-brain-early-exploration-future-neural-networks/</link><pubDate>Sun, 24 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/google-brain-early-exploration-future-neural-networks/</guid><description>&lt;h1 id="谷歌大脑的早期探索与神经网络的未来"&gt;谷歌大脑的早期探索与神经网络的未来&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本期深度访谈由X, The Moonshot Factory的Astro Teller与Google DeepMind首席科学家Jeff Dean对谈，全面回顾了神经网络从边缘到主流的转变历程。内容涵盖Google Brain项目的诞生背景、分布式训练的技术突破、TensorFlow和TPU的研发历程，以及语言模型的三大创新突破。Jeff Dean深入思考了AI技术对社会的深远影响，包括个性化服务、隐私保护、可解释性等关键议题，并对未来五年AI发展方向做出了前瞻性展望。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;访谈开篇追溯了Jeff Dean的成长经历与编程启蒙，从童年用乐高积木培养创造热情，到9岁通过焊接电脑套件接触编程，再到13岁移植多人游戏学习并发技术。这段经历为他日后在神经网络领域的突破性工作奠定了基础。在明尼苏达大学读本科期间，Jeff首次系统接触神经网络，并在毕业论文中提出了数据并行和模型并行的初步概念，尽管当时的计算资源远远不够支持大规模训练。&lt;/p&gt;
&lt;p&gt;90年代末，神经网络在AI领域逐渐失宠，Jeff Dean暂时将兴趣转向其他方向，直到与Andrew Ng的偶然交流重新点燃了激情。Google Brain项目因此诞生，团队从最初的2000台服务器、16000个CPU核心起步，逐步构建起支持数百个团队使用的通用框架。标志性的&amp;quot;猫视频&amp;quot;实验展示了神经网络无监督学习的强大能力，推动了图像识别和语音识别领域的巨大进步。&lt;/p&gt;
&lt;p&gt;随着技术演进，Google Brain团队研发了TensorFlow开源框架和TPU专用硬件，极大地推动了神经网络的普及和应用。访谈详细阐述了语言模型的三大突破：分布式词向量(word2vec)、序列到序列模型(LSTM)、以及注意力机制与Transformer架构。Jeff Dean认为，AI模型的能力在过去六年里取得飞跃，已从单一文本处理扩展到多模态理解与生成。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;数据并行与模型并行&lt;/strong&gt;：这是大规模神经网络训练的两大核心技术。数据并行将训练数据集分割到多个计算节点，每个节点独立计算梯度后同步更新；模型并行则将大模型本身拆分到不同机器上训练。Jeff Dean早在本科毕业论文中就提出了这些概念(当时称为pattern parallelism)，Google Brain团队通过参数服务器架构实现了数十亿参数的同步训练，在2011-2012年就训练了拥有20亿参数的视觉模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;张量处理单元(TPU)&lt;/strong&gt;：针对神经网络计算特点设计的专用硬件，利用神经网络对低精度计算的容忍性提升性能和能效。初代TPU仅支持8位整数运算，后续版本引入了Bfloat16(16位浮点格式)。TPU的出现让大规模模型推理和训练变得可行，为Google及业界带来了显著的技术红利，体现了专用硬件在AI时代的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer架构与注意力机制&lt;/strong&gt;：通过&amp;quot;Attention is all you need&amp;quot;论文提出的革命性架构，模型能够并行处理序列中的所有状态，显著提升了语言理解和生成的能力。注意力机制让模型能够动态关注输入序列的不同部分，捕捉长距离依赖关系。Transformer成为当前主流大模型(如GPT系列、BERT等)的基础架构，代表了深度学习从序列建模转向并行处理的重要范式转变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI的社会影响与责任&lt;/strong&gt;：Jeff Dean强调，AI技术将深刻影响教育、医疗等领域，带来个性化辅导和诊疗，但也伴随隐私、版权和安全风险。AI可以生成逼真的虚假音视频，助长信息误导。社会应共同制定政策，确保数据贡献者获得合理回报，推动数据价值的公平分配。技术上，Google已允许用户选择是否参与模型训练，未来可探索更精细的激励机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI可解释性挑战&lt;/strong&gt;：随着模型规模激增，传统的代码级理解已不适用，AI模型的可解释性变得类似神经科学。数字模型可被任意探测和可视化，但静态分析有限。Jeff Dean提出未来可通过交互式问答方式&amp;quot;调试&amp;quot;模型决策过程，提升透明度和信任度。这反映了AI领域从追求性能到重视可解释性和安全性的重要转变。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OEuh89BWRL4"&gt;The Moonshot Podcast Deep Dive: Jeff Dean on Google Brain&amp;rsquo;s Early Days&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;X, The Moonshot Factory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年8月21日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>AI学习理论的意外革命</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/ai-learning-theory-accidental-revolution/</link><pubDate>Wed, 20 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/ai-learning-theory-accidental-revolution/</guid><description>&lt;h1 id="ai学习理论的意外革命"&gt;AI学习理论的意外革命&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文讲述了人工智能领域一个颠覆性发现：大规模神经网络为何能成功学习，推翻了三百年来关于&amp;quot;模型过大必然过拟合&amp;quot;的理论。通过&amp;quot;彩票票据假说&amp;quot;（Lottery Ticket Hypothesis）和&amp;quot;双重下降&amp;quot;现象，研究者们发现，庞大的模型并非简单地记忆数据，而是在庞大的参数空间中寻找最优、最简洁的解决方案。这一发现不仅改变了AI模型的设计思路，也重新定义了我们对智能和学习本质的理解。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了机器学习领域长期遵循的&amp;quot;偏差-方差权衡&amp;quot;原则，这一理论认为模型过于复杂会导致过拟合，从而丧失泛化能力。按照传统理论，神经网络因参数众多，被认为极易陷入记忆训练数据的陷阱，无法有效应对新数据。这一理论主导了整个研究领域数百年，研究者们专注于小型、受控模型，避免扩大模型规模。&lt;/p&gt;
&lt;p&gt;然而，2019年一批研究者选择无视传统警告，继续扩大模型规模到理论所称的&amp;quot;危险区&amp;quot;，结果发现了意想不到的&amp;quot;双重下降&amp;quot;现象：模型误差在因过拟合上升后，会再次大幅下降并超越过拟合的限制。这一发现迅速改变了行业风向，各大科技公司投入数十亿美元打造更大规模的模型，&amp;ldquo;越大越好&amp;quot;成为新信条。&lt;/p&gt;
&lt;p&gt;为了解释这一现象，MIT的研究者于2018年提出了&amp;quot;彩票票据假说&amp;rdquo;。该假说指出，大网络的成功不是因为学会了复杂解法，而是因为提供了更多机会去寻找简单解法。每组随机初始化的权重都是一张潜在的&amp;quot;彩票&amp;quot;，而训练过程就是筛选出最优初始化的子网络。这一发现调和了经验与经典理论，表明大模型不是在记忆数据，而是在庞大参数空间中找到最简洁的解决方案。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;偏差-方差权衡&lt;/strong&gt;：这是机器学习领域的传统铁律，认为模型需要在拟合数据与保持简洁之间取得平衡。模型太简单会遗漏重要规律（欠拟合），模型太复杂则会记忆噪声（过拟合），导致泛化能力丧失。这一理论主导了机器学习研究三百年，但被AI领域的最新发现所挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双重下降现象&lt;/strong&gt;：这是2019年研究者发现的意外现象。当模型规模扩大到理论所称的&amp;quot;过拟合危险区&amp;quot;时，误差曲线出现两次下降——第一次是正常的学习下降，第二次是在过了过拟合区后的意外下降。这一现象与偏差-方差分析的传统智慧相悖，表明大模型在庞大参数空间中能找到更优解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票票据假说&lt;/strong&gt;：由MIT研究者于2018年提出，该假说认为大网络中隐藏着&amp;quot;中奖票据&amp;quot;——极小的子网络能达到完整网络的性能。大网络的成功不是因为它学会了复杂解法，而是因为提供了更多机会去寻找简单解法。每组随机初始化的权重都是一张彩票，训练过程就是筛选出最优初始化的子网络，其余则被淘汰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;奥卡姆剃刀原理&lt;/strong&gt;：这一简约原理指出最简单的解释通常是最优的。彩票票据假说揭示了大模型并非违背这一原理，而是更高效地找到这些简单解释。大模型的庞大参数空间为寻找最简解提供了更多可能，而非存储复杂解法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;科学进步模式&lt;/strong&gt;：这一发现揭示了科学突破的本质——最重要的发现往往源于突破理论边界的勇气。几十年来研究者因理论限制而不敢扩大模型规模，直到有人勇于质疑假设、进行实证测试，才带来了颠覆性的发现。这与历史上大陆漂移理论、量子力学等的突破模式相似。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://nearlyright.com/how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/"&gt;How AI researchers accidentally discovered that everything they thought about learning was wrong&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;NearlyRight&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-08-18&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>AI研究人员如何意外发现他们对学习的固有认知是错误的</title><link>https://linguista.cn/rosetta/technology/lottery-ticket-hypothesis-why-large-neural-networks-work/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/lottery-ticket-hypothesis-why-large-neural-networks-work/</guid><description>&lt;h1 id="ai研究人员如何意外发现他们对学习的固有认知是错误的"&gt;AI研究人员如何意外发现他们对学习的固有认知是错误的&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;传统机器学习理论认为过大的模型必然导致过拟合，但研究人员发现大规模神经网络反而表现更优，出现了双重下降现象。彩票假说为此提供了解释——大型网络的成功并非源于学习复杂解决方案，而是因为拥有更多随机初始化的子网络，从而大幅提升找到简单优雅解决方案的概率。这一发现调和了经典理论与实践之间的矛盾，也重新定义了我们对智能本身的理解。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;偏差-方差权衡&lt;/strong&gt;：统计学习中的经典原则，指模型过于简单会欠拟合、过于复杂会过拟合，需在两者之间寻找最优平衡点，该理论有三百年数学基础支撑&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双重下降&lt;/strong&gt;：一种违反传统U型误差曲线的现象，模型在过拟合后继续扩大规模，测试误差会先升后降，出现第二次性能提升，由Mikhail Belkin等人记录发现&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票假说&lt;/strong&gt;：由MIT的Jonathan Frankle和Michael Carbin提出，认为大型神经网络中存在能匹配整体性能的微小子网络，大网络的优势在于提供了更多随机初始化的彩票，使找到最优简单解的概率大幅增加&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;过参数化&lt;/strong&gt;：指模型参数数量远超训练数据所需的现象，传统理论认为这必然导致记忆而非学习，但实践证明过参数化反而有助于网络搜索简单解决方案&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经网络剪枝&lt;/strong&gt;：训练完成后移除冗余权重的技术，研究发现可以在不损失准确性的情况下剥离高达96%的参数，证明网络中大量参数实际上是冗余的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作者: NearlyRight&lt;/li&gt;
&lt;li&gt;日期： &lt;em&gt;2025年8月18日&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;大型网络的成功不是通过学习复杂的解决方案，而是通过提供更多机会来寻找简单的解决方案。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;阅读时间：7分钟&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票假说解释了为什么大规模神经网络能够成功，尽管数百年的理论曾预测它们会失败&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;五年前，如果有人建议AI研究人员训练拥有数万亿参数的神经网络，那只会招致怜悯的目光。这违反了机器学习中最基本的规则：如果模型过大，它就会变成一台高级复印机，只会记忆训练数据，却学不到任何有用的东西。&lt;/p&gt;
&lt;p&gt;这不仅仅是惯例，它是一条数学定律，有三百年统计理论作为支撑。每本教科书都展示着同一条不可避免的曲线：小模型欠拟合，最优模型泛化，大模型灾难性过拟合。毋庸置疑。&lt;/p&gt;
&lt;p&gt;然而今天，那些“不可能”的大规模模型驱动着ChatGPT，解码着蛋白质，并引发了一场价值数千亿美元的全球军备竞赛。改变的不仅仅是计算能力，更是我们对学习本身的理解。这场变革背后的故事揭示了AI领域最大的突破是如何源于研究人员敢于无视本领域基础假设的勇气。&lt;/p&gt;
&lt;h2 id="机器学习的铁律"&gt;机器学习的铁律&lt;/h2&gt;
&lt;p&gt;三百多年来&lt;a href="#ZgotmplZ"&gt;1&lt;/a&gt;，一条原则主导着所有学习系统：偏差-方差权衡。其数学优雅，逻辑无可辩驳。构建一个过于简单的模型，它会错过关键模式。构建一个过于复杂的模型，它会记忆噪声而非信号。&lt;/p&gt;
&lt;p&gt;想象一个学生学习算术。向他们展示成千上万道带答案的加法题，他们可能有两种学习方式。聪明的做法：掌握进位和位值的基本算法。愚蠢的做法：记忆每一个例子。第二种策略能在家庭作业中取得满分，但在考试中却完全失败。&lt;/p&gt;
&lt;p&gt;神经网络似乎尤其容易陷入这种记忆陷阱。拥有数百万参数的它们，可以轻易存储整个数据集。传统理论预测，这些过参数化网络会表现得像那个死记硬背的学生——在训练数据上表现完美，但在新数据上却毫无希望。&lt;/p&gt;
&lt;p&gt;这种理解塑造了整个领域。研究人员痴迷于架构技巧、正则化技术和数学约束，以从小而精心控制的模型中挤出性能。扩大规模被认为是昂贵而愚蠢的做法。&lt;/p&gt;
&lt;p&gt;该领域最受尊敬的声音强化了这一正统观念。“模型越大，过拟合越严重”成了口头禅。会议论文侧重于效率，而非规模。认为仅仅增加更多参数就能解决问题的想法，在学术界看来简直是异端。&lt;/p&gt;
&lt;h2 id="打破规则的异端们"&gt;打破规则的异端们&lt;/h2&gt;
&lt;p&gt;2019年，一群研究人员犯下了终极“罪行”：他们无视警告，继续扩大模型规模。他们没有在网络达到完美训练准确率时——理论上尖叫“危险”的节点——停止，而是进一步深入“禁区”。&lt;/p&gt;
&lt;p&gt;接下来发生的事情彻底颠覆了三百年来的学习理论。&lt;/p&gt;
&lt;p&gt;模型并没有崩溃。在最初它们看似记忆了训练数据而出现短暂的性能下滑后，令人惊叹的事情发生了。性能开始再次提升。而且是戏剧性地提升。&lt;/p&gt;
&lt;p&gt;这种现象被称为“双重下降”——首先是模型过拟合时预期的误差上升，然后是它们以某种方式完全超越过拟合后的意外第二次下降。记录这一发现的米哈伊尔·贝尔金（Mikhail Belkin）及其同事指出，这“与源自偏差-方差分析的传统智慧相悖”。&lt;/p&gt;
&lt;p&gt;其影响在AI研究领域掀起波澜。OpenAI随后的工作表明，这些益处横跨多个数量级。更大的模型不仅仅是积累更多事实——它们正在发展出质性新能力，包括仅从示例中学习任务的能力。&lt;/p&gt;
&lt;p&gt;突然间，整个领域都转向了。谷歌、微软、Meta和OpenAI投入数十亿美元来构建越来越大的模型。理论曾禁止的“越大越好”哲学，成了行业“北极星”。&lt;/p&gt;
&lt;p&gt;但一个问题困扰着每一位研究人员：这一切为什么会奏效？&lt;/p&gt;
&lt;h2 id="拯救学习理论的彩票"&gt;拯救学习理论的彩票&lt;/h2&gt;
&lt;p&gt;答案出乎意料地来自一个角落：一项关于神经网络彩票券的研究。2018年，麻省理工学院的乔纳森·弗兰克尔（Jonathan Frankle）和迈克尔·卡宾（Michael Carbin）正在研究剪枝——训练后移除不必要的权重。他们的发现将为规模悖论提供优雅的解决方案。&lt;/p&gt;
&lt;p&gt;他们在每个大型网络中都发现了“中奖彩票”——微小的子网络，它们能够与整个网络的性能相匹配。他们可以在不损失准确性的情况下剥离96%的参数。绝大多数成功网络的参数实际上都是冗余。&lt;/p&gt;
&lt;p&gt;但关键的洞察在于：这些“中奖”子网络只有在它们原始的随机初始权重下才能成功。改变初始值，同样的稀疏架构就会完全失败。&lt;/p&gt;
&lt;p&gt;彩票假说逐渐明确：大型网络成功不是通过学习复杂的解决方案，而是通过提供更多机会来找到简单的解决方案。每组权重都代表着一张不同的彩票——一个具有随机初始化的潜在优雅解决方案。大多数彩票会输，但拥有数十亿张彩票，中奖变得不可避免。&lt;/p&gt;
&lt;p&gt;在训练过程中，网络并不搜索完美的架构。它已经包含了无数个小型网络，每个都有不同的初始条件。训练变成了一场大规模的彩票抽奖，其中初始条件最佳的小型网络脱颖而出，而数十亿其他网络则逐渐消逝。&lt;/p&gt;
&lt;p&gt;这一发现将经验成功与经典理论调和。大型模型不是在记忆——它们是在广阔的参数空间中寻找优雅的简单解决方案。奥卡姆剃刀原理毫发无损地幸存下来：最简单的解释仍然是最好的。规模只是成为了一种更复杂的工具，用于找到那些简单的解释。&lt;/p&gt;
&lt;h2 id="智能的真正面貌"&gt;智能的真正面貌&lt;/h2&gt;
&lt;p&gt;其影响超越了人工智能范畴。如果学习意味着找到解释数据的最简单模型，并且更大的搜索空间能够实现更简单的解决方案，那么这重新定义了智能本身。&lt;/p&gt;
&lt;p&gt;考虑一下你的大脑：860亿个神经元，数万亿个连接，无论从何种标准衡量，都属于大规模过参数化。然而你擅长从有限的例子中学习并泛化到新情况。彩票假说认为这种神经元的丰度也服务于同样的目的——为任何问题提供大量潜在的简单解决方案。&lt;/p&gt;
&lt;p&gt;智能不是关于记忆信息——它关于寻找解释复杂现象的优雅模式。规模提供了进行这种搜索所需的计算空间，而不是存储复杂解决方案的地方。&lt;/p&gt;
&lt;p&gt;这一发现也阐明了科学进步。几十年来，研究人员避免扩大规模，因为理论认为这行不通。突破来自经验勇气——检验假设而不是接受它们。&lt;/p&gt;
&lt;p&gt;这种模式在整个科学领域回响。大陆漂移学说曾被驳斥，直到板块构造论提供了其机制。量子力学曾看似荒谬，直到实验证据变得确凿无疑。最重要的发现往往需要我们超越公认理论的边界。&lt;/p&gt;
&lt;p&gt;然而，彩票假说并没有推翻经典学习理论——它揭示了这些原则的运作方式比想象的要复杂得多。简单的解决方案仍然是最优的；我们只是发现了一种更好的方法来找到它们。&lt;/p&gt;
&lt;p&gt;对于AI发展而言，这种理解既带来了希望，也暗示了局限性。规模化之所以有效，是因为更大的模型提供了更多彩票，更多机会找到最优解决方案。但这种机制也意味着自然的限制。随着网络在寻找最小解决方案方面变得越来越成功，额外的规模将导致收益递减。&lt;/p&gt;
&lt;p&gt;这与专家们对当前方法局限性的担忧相符。扬·勒昆（Yann LeCun）认为，无论规模多大，基本的架构限制都可能阻碍语言模型实现真正的理解。彩票机制解释了当前的成功，同时也暗示了未来的挑战。&lt;/p&gt;
&lt;h2 id="优雅的惊喜"&gt;优雅的惊喜&lt;/h2&gt;
&lt;p&gt;彻底改变AI的意外发现提供了一个深刻的教训：宇宙常常为那些敢于挑战传统智慧边界的人保留着优雅的惊喜。有时最深刻的见解并非来自推翻既定原则，而是来自发现它们的运作方式比我们想象的要复杂得多。&lt;/p&gt;
&lt;p&gt;进化本身也遵循类似的原则，探索广阔的遗传可能性空间以寻找优雅的生存解决方案。最成功的生物并非最复杂的，而是适应最有效率的。&lt;/p&gt;
&lt;p&gt;曾被认为是学习理论危机的，最终却证明了其正确性。偏差-方差权衡幸存下来，但我们了解到它通过比任何人怀疑的都更为微妙的机制运作。大型神经网络并非通过打破规则而成功——它们通过在我们从未想到的水平上运用这些规则而成功。&lt;/p&gt;
&lt;p&gt;那些敢于超越理论舒适区进行规模化研究的研究人员，不仅推动了AI发展，也提醒我们，经验现实有时蕴含着理论尚未掌握的智慧。在一个建立在数学确定性之上的领域，最重要的发现却来自拥抱不确定性本身。&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;300年的时间范围指的是支撑现代偏差-方差分析的数学基础原则，而非当代术语。贝叶斯定理（1763年）建立了用证据更新信念的数学框架，而拉普拉斯在统计推断方面的早期工作（18世纪80年代-19世纪10年代）则正式确立了模型必须平衡拟合度与简洁性以避免虚假结论的原则。这些早期统计见解——即过于复杂的解释往往捕捉的是噪声而非信号——构成了我们现在所说的偏差-方差权衡的数学基石。具体的现代公式是在20世纪后期几十年中出现的，但其核心原则已经指导统计推理数百年。&lt;a href="#ZgotmplZ"&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Title: &lt;a href="https://nearlyright.com/how-ai-researchers-accidentally-discovered-that-everything-they-thought-about-learning-was-wrong/"&gt;How AI researchers accidentally discovered that everything they thought about learning was wrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Author: NearlyRight&lt;/li&gt;
&lt;li&gt;Date: 18 Aug, 2025&lt;/li&gt;
&lt;li&gt;URL:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#AI&lt;/p&gt;</description></item><item><title>可解释性理解AI模型如何思考</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/ai-interpretability-understanding-model-thinking/</link><pubDate>Mon, 18 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/ai-interpretability-understanding-model-thinking/</guid><description>&lt;h1 id="可解释性理解ai模型如何思考"&gt;可解释性：理解AI模型如何思考&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Anthropic团队的专题演讲，系统阐述了AI可解释性研究的现状与前沿进展。文章通过&amp;quot;生物学&amp;quot;类比，深入剖析了大模型内部的思维机制，揭示了AI超越简单预测任务的复杂能力，以及幻觉、拍马屁等现象的内在成因。同时探讨了可解释性研究对AI安全与信任的重要意义。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;现代大语言模型已发展出远超&amp;quot;预测下一个词&amp;quot;的复杂能力。本文首先指出，AI模型通过大规模数据和进化式训练，在内部形成了大量抽象概念和中间目标。这些机制使模型能够进行上下文理解、推理规划、多语言处理等高级认知活动。然而，这种复杂性也带来了理解上的挑战。&lt;/p&gt;
&lt;p&gt;文章重点分析了AI模型中几种引人注目的现象。研究发现，模型内部存在分工明确的&amp;quot;电路&amp;quot;：一部分负责生成答案，另一部分负责判断确定性。当这两部分沟通不畅时，就会产生&amp;quot;幻觉&amp;quot;。同样，拍马屁、迎合用户等行为，也是模型为优化预测目标而自发形成的策略，而非人类意义上的动机。&lt;/p&gt;
&lt;p&gt;在方法论层面，研究者采用类似神经科学的&amp;quot;类脑研究&amp;quot;方法，直接观测和操控模型内部神经元激活。这种方法的独特优势在于可以无限复制模型、精确控制输入、反复实验，比研究人脑更加高效。通过激活-抑制实验，研究团队已能验证模型的推理和规划机制。&lt;/p&gt;
&lt;p&gt;最后，文章强调了可解释性研究对AI安全与治理的关键意义。随着AI将深度参与金融、能源等社会关键系统，必须确保其行为可预测、可追溯。理解AI的真实动机和计划，是防止其在关键任务中出现不可控行为的前提。未来需要建立全新的科学语言和工具，系统揭示AI内部的思维规律。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;涌现能力&lt;/strong&gt;：AI模型在大规模训练过程中自发形成的、超越原始训练目标的能力。如写诗时提前规划押韵、做数学题时激活特定计算电路等。这些能力并非显式编程，而是模型为优化&amp;quot;预测下一个词&amp;quot;这一核心任务而发展出的复杂策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;幻觉机制&lt;/strong&gt;：AI生成看似合理但实际错误信息现象的内在解释。研究显示模型内部存在生成答案和评估确定性的两套独立&amp;quot;电路&amp;quot;，当两者沟通不畅时，模型会在不确定状态下自信地给出错误答案。这是系统架构问题，而非简单的&amp;quot;撒谎&amp;quot;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;类神经科学研究方法&lt;/strong&gt;：直接观测和操控AI模型内部神经元激活的研究范式。相比人脑研究，该方法具有可复制、可控、可重复等优势，已成为理解AI思维机制的主要科学工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;内部电路&lt;/strong&gt;：AI模型内部负责不同功能的神经网络结构。研究已识别出专门处理加法、多语言映射、元认知等多种专用电路，这些电路既能独立工作又能灵活组合，展现出惊人的功能分化与协作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可解释性与安全&lt;/strong&gt;：理解AI内部机制是确保AI安全可控的基础。只有揭示模型的真实动机和规划过程，才能在金融、医疗等关键应用中建立信任，防止不可控行为。可解释性研究因此成为AI治理的核心技术支撑。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=fGKNUvivvnc&amp;amp;list=WL&amp;amp;index=9"&gt;Interpretability: Understanding how AI models think&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Anthropic&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-08-15&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Tversky 神经网络：用心理学启发重塑深度学习的相似性度量</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p3/tversky-neural-networks-psychology-similarity/</link><pubDate>Sun, 17 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p3/tversky-neural-networks-psychology-similarity/</guid><description>&lt;h1 id="tversky-神经网络用心理学启发重塑深度学习的相似性度量"&gt;Tversky 神经网络：用心理学启发重塑深度学习的相似性度量&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文介绍了一种全新的神经网络架构——Tversky 神经网络（Tversky Neural Networks），其核心创新在于用 Tversky 相似性函数替代了传统的点积（dot product）或余弦相似度（cosine similarity）作为神经网络中衡量&amp;quot;相似性&amp;quot;的基本方式。作者通过将心理学中的 Tversky 特征匹配模型（feature matching model）转化为可微分的形式，使其能够与现代深度学习的梯度优化机制兼容。实验表明，这一新方法不仅提升了模型的表现，还带来了更强的可解释性，并在一定程度上印证了心理学理论的有效性。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先分析了传统神经网络的相似性假设及其局限。现代深度学习架构普遍使用几何方式度量相似性，如点积或余弦相似度，这种方式虽然计算高效，但与人类的相似性判断存在本质差异。心理学家 Amos Tversky 在 1977 年指出，人类的相似性判断往往是非对称的，而几何模型无法表达这种非对称性。Tversky 提出的&amp;quot;特征匹配模型&amp;quot;认为相似性是对象间共有特征与各自独特特征的函数，但该模型依赖离散集合操作，难以直接应用于需要可微分优化的神经网络。&lt;/p&gt;
&lt;p&gt;接下来，作者详细介绍了可微分 Tversky 相似性函数的提出与实现。创新性地将 Tversky 相似性转化为可微分形式，具体做法是将每个对象既视为向量（R^d 维），又视为特征集合（feature set）。特征集合的定义基于对象向量与特征向量的点积为正时，该特征&amp;quot;存在&amp;quot;于该对象中。通过这种方式，传统的集合交集和差集操作被重写为可微分函数。Tversky 相似性函数的公式为 S(a, b) = θf(A ∩ B) − αf(A − B) − βf(B − A)，其中 A、B 分别为对象 a、b 的特征集合，θ、α、β 为可学习参数，f(·) 表示对特征的聚合函数。&lt;/p&gt;
&lt;p&gt;然后，文章阐述了 Tversky 神经网络的结构与表达能力。该网络引入了两种新层：Tversky 相似性层和 Tversky 投影层。Tversky 相似性层类似于传统的点积或余弦相似度层，但用 Tversky 相似性函数替代，输出为标量。Tversky 投影层类似于全连接层，输入向量与一组&amp;quot;原型向量&amp;quot;（prototypes）计算 Tversky 相似性，输出为 R^p 维向量。作者证明单个 Tversky 投影层可以拟合非线性 XOR 函数，这证明了其更强的表达能力。&lt;/p&gt;
&lt;p&gt;最后，文章展示了实验结果与可解释性分析。在图像识别任务中，在冻结 ResNet-50 的情况下，将输出层替换为 Tversky 投影层，NABirds 数据集准确率从 36.0% 提升到 44.9%，MNIST 从 57.4% 提升到 62.3%。在语言建模任务中，从零训练 GPT-2 small，使用 Tversky 层可同时降低 7.5% 的困惑度，并减少 34.8% 的参数量。更重要的是，Tversky 框架本身即具备可解释性，其基本操作基于&amp;quot;共有特征&amp;quot;和&amp;quot;独特特征&amp;quot;，天然适合解释模型决策。&lt;/p&gt;</description></item><item><title>AI模型如何揭示人类学习语言的方式</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/ai-models-language-learning-impossible-languages/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025_p2/ai-models-language-learning-impossible-languages/</guid><description>&lt;h1 id="ai模型如何揭示人类学习语言的方式"&gt;AI模型如何揭示人类学习语言的方式&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文探讨了AI模型在揭示人类语言学习机制方面的潜力。通过构建和测试&amp;quot;不可能的语言&amp;quot;——即违反人类普遍语法的语言结构——研究者们试图验证大型语言模型是否真正以类似人类的方式学习语言。文章从Chomsky的普遍语法理论出发，介绍了相关实验的进展与争议，最终指向一个核心问题：AI模型能否帮助我们理解人类语言学习的本质。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先从语言学习的核心难题切入。人类如何学习语言一直是语言学和认知科学的核心问题。Noam Chomsky提出的普遍语法理论认为，人类生来就具有语言学习的内在机制，这套机制限制了可能的人类语言结构。然而，现代大型语言模型的崛起对这一传统理论构成了挑战——这些模型仅仅通过统计学习就能掌握复杂的语言模式，似乎不需要先天的语言结构。&lt;/p&gt;
&lt;p&gt;接着，文章介绍了&amp;quot;不可能的语言&amp;quot;这一研究方法。研究者构建违反普遍语法规则的人造语言，然后测试AI模型是否能学习这些语言。如果AI模型能学习这些人类无法掌握的语言，说明它们的学习机制与人类不同；反之，如果它们也面临困难，则可能暗示它们在某种程度上模拟了人类的学习过程。&lt;/p&gt;
&lt;p&gt;文章详细介绍了相关研究的进展。早期的研究发现，某些语言模型能够学习不可能的语言，这似乎与Chomsky的理论相悖。然而，更近期的研究表明，现代大型语言模型在学习某些不可能语言时确实面临显著困难，这为理解语言学习机制提供了新的线索。&lt;/p&gt;
&lt;p&gt;最后，文章展望了这一研究方向的意义。通过对比AI模型和人类在不可能语言任务上的表现，研究者们希望能够揭示语言学习的普遍原理，回答人类是否拥有独特的语言学习机制这一根本问题。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;普遍语法&lt;/strong&gt;：Chomsky提出的理论，认为人类生来就具有一套先天的语言结构知识，这套知识限制了所有人类可能的语言形式。这套理论解释了为什么儿童能够在有限的语言输入下快速掌握复杂的语言规则，以及为什么某些语言结构在人类语言中从未出现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;不可能的语言&lt;/strong&gt;：违反普遍语法约束的人造语言。这些语言在逻辑上是自洽的，但违反了人类语言的结构限制。通过测试AI模型是否能学习这些语言，研究者可以推断它们的学习机制是否与人类相似。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信息局部性原则&lt;/strong&gt;：近期研究中发现的一个关键原则，它限制语言依赖关系的作用范围。这一原则在人类语言中普遍存在，而现代AI模型在学习违反这一原则的语言时确实面临困难，这可能暗示模型在某种程度上捕捉到了人类语言的结构特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;统计学习 vs 结构学习&lt;/strong&gt;：AI模型主要通过统计模式识别学习语言，而人类学习可能涉及更深层的结构表征。研究的核心争议在于，统计学习是否足以解释人类语言能力，还是需要额外的先天结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;语言模型作为认知科学工具&lt;/strong&gt;：AI模型为研究人类语言学习提供了新的实验平台。与传统语言学研究不同，研究者可以精确控制模型的训练过程和输入，从而分离出影响语言学习的各种因素。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.quantamagazine.org/can-ai-models-show-us-how-they-learn-impossible-languages-point-a-way-20250113"&gt;Can AI Models Show Us How People Learn? Impossible Languages Point a Way&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Quanta Magazine&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-13&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>新计算化学技术加速分子和材料预测</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/computational-chemistry-molecular-prediction-acceleration/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/computational-chemistry-molecular-prediction-acceleration/</guid><description>&lt;h1 id="新计算化学技术加速分子和材料预测"&gt;新计算化学技术加速分子和材料预测&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;MIT研究团队开发了一种名为MEHnet的神经网络方法，基于量子化学&amp;quot;黄金标准&amp;quot;耦合簇理论CCSD(T)训练，能够以高于密度泛函理论DFT的精度预测分子和材料的多种电子属性。该技术采用E(3)等变图神经网络架构，可同时评估偶极矩、四极矩、电子极化率和光学激发带隙等多种属性，适用于基态和激发态分析，有望实现高通量分子筛选并推动新型聚合物、半导体及电池材料的开发。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文从材料科学的历史演变切入，回顾了从古代炼金术到19世纪元素周期表的出现，再到过去十年机器学习在材料科学领域的应用历程。尽管这些进步极大地推动了材料设计发展，但现有基于密度泛函理论DFT的机器学习模型仍存在精度不均匀和只能提供最低总能量等局限性。&lt;/p&gt;
&lt;p&gt;为解决这些问题，MIT核科学与工程系Ju Li教授领导的团队转向耦合簇理论CCSD(T)，这种被公认为量子化学&amp;quot;黄金标准&amp;quot;的方法虽然计算精度可与实验媲美，但计算成本极高。研究团队开发的MEHnet神经网络通过定制算法，能够直接从量子力学原理中提取分子属性，在保持CCSD(T)级别精度的同时大幅降低计算成本。&lt;/p&gt;
&lt;p&gt;该技术的核心创新在于采用E(3)等变图神经网络架构，其中节点代表原子，边代表原子间的化学键。更关键的是，MEHnet采用多任务学习方法，能够同时评估分子的多种电子属性，不仅适用于基态分析，还适用于激发态研究，甚至可以预测分子的红外吸收光谱。在对已知烃类分子的测试中，MEHnet的表现优于DFT方法，并与文献中的实验结果高度一致。&lt;/p&gt;
&lt;p&gt;这项技术的应用前景广阔，有望实现高通量分子筛选，这对识别具有特定属性的新分子和材料至关重要。研究团队表示，该技术可用于设计新型聚合物或半导体材料，甚至可能推动电池材料的开发。目前模型能够处理多达数千个原子的分子，未来有望扩展到数万个原子的分子系统。团队的长期目标是覆盖整个元素周期表，并以低于DFT的计算成本实现CCSD(T)级别的精度。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;耦合簇理论CCSD(T)&lt;/strong&gt;：量子化学领域的&amp;quot;黄金标准&amp;quot;方法，其计算结果的准确性可与实验结果媲美。传统CCSD(T)方法的计算成本极高，限制了其应用范围。MIT团队通过神经网络技术，使这一高精度方法得以在更大规模分子系统中应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;E(3)等变图神经网络&lt;/strong&gt;：一种专门处理三维空间数据的神经网络架构。在MEHnet中，图神经网络的节点代表分子中的原子，边代表原子之间的化学键。这种架构能够保持三维空间中的旋转和平移不变性，非常适合处理分子结构数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多任务学习&lt;/strong&gt;：MEHnet能够同时学习分子的多种电子属性，包括偶极矩、四极矩、电子极化率和光学激发带隙等。与单一任务学习相比，多任务学习能够利用不同属性之间的相关性，提高模型的泛化能力和预测精度，同时也使得模型适用于基态和激发态分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;高通量分子筛选&lt;/strong&gt;：指在短时间内对大量分子候选进行计算评估，从中筛选出具有目标特性的分子。传统高精度量子化学方法计算成本过高，无法实现高通量筛选。MEHnet在保持CCSD(T)级别精度的同时大幅降低计算成本，使得高通量分子筛选成为可能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;激发态与红外光谱预测&lt;/strong&gt;：与大多数只能处理基态的量子化学方法不同，MEHnet能够处理分子的激发态，这使其能够预测分子的光学吸收性质。此外，模型还可以预测分子的红外吸收光谱，这对于实验化学家进行分子结构确认和性质研究具有重要意义。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://news.mit.edu/2025/new-computational-chemistry-techniques-accelerate-prediction-molecules-materials-0114"&gt;New computational chemistry techniques accelerate the prediction of molecules and materials&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;MIT News&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-14&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>神经网络理论研究中的物理学思想与应用</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/physics-neural-network-theory-ising-model/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/physics-neural-network-theory-ising-model/</guid><description>&lt;h1 id="神经网络理论研究中的物理学思想与应用"&gt;神经网络理论研究中的物理学思想与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文从物理学视角系统阐述神经网络的理论基础，将神经网络视为数据驱动的自适应物理模型。文章从伊辛模型出发，介绍神经网络的基本属性DNA（数据、网络、算法），深入剖析感知机学习的几何景观与解空间结构，探讨无监督学习中的对称性破缺机制，以及非平衡神经动力学的伪势表示方法。最后通过线性回归模型揭示大语言模型示例泛化的物理本质，为理解智能提供物理学洞察。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章开篇指出神经网络在人工智能领域的核心地位，强调物理学思想对神经网络研究的深远影响。作者认为，神经网络本质上是数据驱动的自适应物理模型，其学习过程是在高维权重空间中执行朗之万动力学，服从玻尔兹曼分布。&lt;/p&gt;
&lt;p&gt;从伊辛模型这一统计物理标准模型入手，文章建立了自旋系统与神经网络的深刻联系。伊辛模型描述格点上磁矩的集体行为，蕴含相变、自发对称性破缺、普适性等丰富物理图像。神经网络将相互作用强度J作为可训练参数，外场对应偏置，通过定义目标函数和梯度下降算法驱动网络更新，其学习过程可视为在势能函数下的随机游走。&lt;/p&gt;
&lt;p&gt;感知机模型的研究揭示了解空间的几何景观。当学习样本数与连接数之比达到临界值α≈0.833时，自由熵消失，学习问题无解。解空间呈现大量孤岛形态，局域算法难以找到解，但存在稀有的稠密解团簇能够吸引高效算法。这一发现近期已被数学家严格证明。&lt;/p&gt;
&lt;p&gt;无监督学习建模为受限玻尔兹曼机，学习过程呈现对称性破缺现象。随着数据量增长，学生网络逐步推断教师网络的结构，经历自发对称破缺和置换对称破缺两个阶段，最终实现对隐藏节点内在顺序的区分。&lt;/p&gt;
&lt;p&gt;认知动力学层面的非平衡过程可通过伪势表示法研究。定义正则系综分析非平衡神经动力学稳态，发现当相互作用强度达到临界值时，系统经历从有序到混沌的连续相变，响应函数在相变点附近出现峰值，印证了混沌边缘的优越性。&lt;/p&gt;
&lt;p&gt;大语言模型的示例泛化能力可归结为两体自旋模型的基态求解。通过线性回归函数类和随机任务向量生成预训练数据，模型参数服从具有两体相互作用的哈密顿量。基态分析显示，即使在有限尺寸网络中仍可获得最优解，任务向量的多样性对预训练效果至关重要。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;伊辛模型与神经网络&lt;/strong&gt;：伊辛模型是统计物理的标准模型，描述格点上磁矩的集体行为。其态方程为迭代方程，包含自旋间相互作用J、磁化强度m和外加磁场h。当相互作用较弱时系统处于顺磁态，增大相互作用至临界值时出现铁磁解，此过程称为自发对称性破缺。神经网络可类比为伊辛模型，连接权重对应相互作用，偏置对应外场，学习过程是势能函数下的随机游走，平衡态服从玻尔兹曼分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知机解空间几何&lt;/strong&gt;：感知机学习问题存在存储容量极限α=P/N≈0.833，超过此临界值问题无解。在可解区域内，解空间呈现大量孤岛形态，导致梯度下降等局域算法难以找到解。解空间中存在稀有的稠密解团簇，这些团簇能够吸引高效经验算法，使其避开孤岛成功收敛。这一几何结构解释了为何某些算法在实际中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对称性破缺与无监督学习&lt;/strong&gt;：无监督学习让机器从原始数据中发现隐藏规律，类似人类婴儿的观察学习过程。通过受限玻尔兹曼机建模，学生网络从教师网络生成的数据中推断连接矩阵。学习过程呈现对称性破缺：初始状态具有完全对称性，随着数据增长，学生首先推断权重相同的部分（自发对称破缺），进而区分权重不同的部分（置换对称破缺），最终实现对隐藏节点内在顺序的认知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;非平衡动力学的伪势表示&lt;/strong&gt;：认知动力学大多不存在梯度力，但可通过定义伪势函数研究非平衡稳态。对于相互作用矩阵为非厄米随机矩阵的神经元系统，当耦合强度g增至1时触发连续动力学相变，系统从有序走向混沌。序参量为网络神经活动水平的涨落，响应函数在相变点附近出现峰值，验证了混沌边缘在认知功能上的优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大语言模型示例泛化的物理本质&lt;/strong&gt;：大语言模型的示例泛化能力可通过线性回归模型理解。固定随机任务向量，生成多个随机输入计算标签得到预训练数据。模型参数服从具有两体相互作用的哈密顿量，其基态是示例泛化能力的根源。高斯分布假设下的基态求解显示，有限尺寸网络仍可获得最优解，任务向量多样性对预训练效果起关键作用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/KbS3dAuyjb6pQV5g9RGzYw"&gt;神经网络理论研究的物理学思想&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;现代物理知识杂志&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>快速入门卷积神经网络CNN</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p3/cnn-convolutional-neural-network-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p3/cnn-convolutional-neural-network-guide/</guid><description>&lt;h1 id="快速入门卷积神经网络cnn"&gt;快速入门卷积神经网络（CNN）&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;卷积神经网络（Convolutional Neural Network，CNN）是专门设计用于处理具有空间结构数据（如图像）的深度学习架构。本文通过通俗易懂的语言和生动的比喻，系统介绍了CNN的核心组成部分——卷积层、激活函数、池化层和全连接层，并提供PyTorch和Keras两种框架的完整代码实现示例，帮助读者从零开始理解并构建CNN模型。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了CNN的基本定义和核心优势——能够自动从图像等空间数据中提取特征。作者通过一个生动的例子：当我们观察一张苹果照片时，眼睛会先识别边缘、颜色、纹理等局部特征，大脑再整合这些信息最终识别出&amp;quot;苹果&amp;quot;。CNN正是模拟这一过程，通过层层特征提取实现图像识别。&lt;/p&gt;
&lt;p&gt;接下来，文章详细拆解了CNN的四大核心组件。卷积层如同用放大镜观察图像局部，通过滑动过滤器提取特征；激活函数引入非线性变换，使网络能够学习复杂的模式；池化层对特征图进行降维，减少计算量同时保留关键信息；全连接层则负责整合所有特征并输出最终分类结果。&lt;/p&gt;
&lt;p&gt;最后，文章以经典的CIFAR-10图像分类数据集为例，提供了完整的CNN模型实现代码，分别使用PyTorch和Keras两种主流深度学习框架，让读者能够直接上手实践。这种理论与实践相结合的方式，使抽象的神经网络概念变得具体可操作。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;卷积层&lt;/strong&gt;：CNN的核心特征提取组件，通过可学习的过滤器（Filter）在输入数据上滑动，执行卷积运算生成特征图。每个过滤器专注于提取特定类型的特征（如边缘、纹理、颜色模式），浅层卷积层提取简单特征，深层卷积层组合形成更复杂的抽象特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：为神经网络引入非线性的关键组件，最常用的是ReLU（Rectified Linear Unit），它将所有负值置零而保持正值不变。这种简单而有效的操作使网络能够学习和表示复杂的非线性关系，避免了线性模型的表达能力限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;池化层&lt;/strong&gt;：用于降低特征图空间维度的下采样操作，最大池化（Max Pooling）取每个局部区域的最大值作为输出。池化不仅减少了计算量和参数数量，还提供了一定程度的平移不变性，使模型对物体位置的微小变化更加鲁棒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全连接层&lt;/strong&gt;：位于CNN末尾的分类器组件，将前面卷积层提取的局部特征整合成全局特征向量，通过矩阵运算输出每个类别的得分。全连接层负责将特征映射到最终的类别空间，完成从特征到决策的转换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征层次化&lt;/strong&gt;：CNN的核心设计理念是通过多层网络实现特征的层次化提取。低层网络学习简单的边缘和颜色特征，中层网络组合这些基础特征形成纹理和形状，高层网络则识别出完整的物体部件和整体概念，这种层层递进的特征抽象使CNN具备了强大的视觉理解能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/2ynT7bYrrWwNOP-NH8EbwQ"&gt;快速入门一个算法，CNN&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;chal1ce&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025年01月05日&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深度神经网络的优势与应用</title><link>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/deep-neural-networks-advantages-applications/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes-2025-p1/deep-neural-networks-advantages-applications/</guid><description>&lt;h1 id="深度神经网络的优势与应用"&gt;深度神经网络的优势与应用&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文探讨了深度神经网络相较于三层浅层网络的核心优势。虽然万能逼近定理证明了三层网络理论上可逼近任何连续函数，但深度网络在特征提取效率、参数利用和泛化能力上具有显著优势。通过分层特征学习、参数共享和稀疏连接等机制，深度网络能够更高效地处理复杂数据结构，在现代人工智能应用中发挥着不可替代的作用。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先阐述了三层神经网络的理论基础——万能逼近定理，同时指出了理论在实际应用中的局限性。浅层网络虽然理论上完备，但为了逼近高维复杂函数往往需要指数级增长的神经元数量，这在计算上是不可行的。此外，浅层网络在优化过程中容易陷入局部最优，且缺乏分层特征表示能力。&lt;/p&gt;
&lt;p&gt;接着，文章详细介绍了深度神经网络的三大核心优势。首先是分层特征提取机制，网络能够从低层的简单模式（如边缘、纹理）逐层抽象到高层的语义概念。其次是参数共享与稀疏连接设计，这大幅降低了模型复杂度和计算需求。最后是深度网络更强的表达能力，研究表明其可以用更少的参数实现比浅层网络更高的逼近能力。&lt;/p&gt;
&lt;p&gt;文章还介绍了深度神经网络的训练优化技术，包括反向传播、激活函数改进、批归一化、正则化技术以及预训练与迁移学习等。在实际应用层面，深度学习已在图像处理、自然语言处理和强化学习等领域取得突破性进展，成为推动人工智能发展的关键技术。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;万能逼近定理&lt;/strong&gt;：这是神经网络理论的基石，证明了具有单个隐藏层的前馈神经网络在合适的条件下可以逼近任何连续函数。然而，这并不意味着三层网络就是最优选择，因为定理仅保证了存在性，并未考虑实现这种逼近所需的计算资源和训练难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分层特征提取&lt;/strong&gt;：深度网络的核心优势在于其能够自动学习多层次的特征表示。低层捕获简单模式如边缘和纹理，中层识别局部结构和形状，高层则理解全局概念和语义信息。这种层级抽象机制使得深度网络能够高效处理复杂的视觉和语言数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参数共享与稀疏连接&lt;/strong&gt;：这是卷积神经网络等架构的关键设计理念。通过在整个输入空间共享同一组权重，并限制神经元只与局部邻域连接，模型可以用更少的参数处理高维数据，这不仅降低了计算复杂度，也提高了模型的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;反向传播与优化技巧&lt;/strong&gt;：深度网络的训练依赖于反向传播算法和梯度下降优化。为了解决深层网络训练中的梯度消失和过拟合问题，现代深度学习发展了多种技术，包括ReLU激活函数、批归一化、Dropout正则化以及预训练与迁移学习等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;泛化能力&lt;/strong&gt;：深度网络通过学习更具普适性的特征表示，能够在未见过的数据上保持良好性能。这种泛化能力源于深度架构能够捕获数据中的本质规律而非仅仅记忆训练样本，这也是深度学习在实际应用中取得成功的关键因素。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/D47cZT7DvATC1VGwWjUFgw"&gt;一般来说，三层神经网络可以逼近任何一个非线性函数，为什么还需要深度神经网络？为什么深度学习模型能够自动提取多层次特征？｜深度学习&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;深度学习&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025-01-04&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>