<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>系统工程 on Linguista</title><link>https://linguista.cn/tags/%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B/</link><description>Recent content in 系统工程 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sun, 24 Aug 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>AGI的未来是系统工程而非模型训练</title><link>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/agi-future-systems-engineering-not-models/</link><pubDate>Sun, 24 Aug 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/info/tldrcards/henrinotes_2025_p4/agi-future-systems-engineering-not-models/</guid><description>&lt;h1 id="agi的未来是系统工程而非模型训练"&gt;AGI的未来是系统工程而非模型训练&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文核心观点是通往人工通用智能的道路不在于继续扩大语言模型的规模，而在于构建由模型、记忆、上下文和确定性工作流组成的工程化系统。当前主流大模型已接近能力极限，单纯依靠堆算力和扩参数已无法带来质的突破。AGI的本质是一个系统工程问题，需要分布式系统、上下文管理、记忆服务、确定性与概率性结合的工作流，以及多模型协作的架构创新。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先指出了现有大语言模型已达到能力平台期。日常使用者能明显感受到其局限性：虽然能生成高质量文本，但在跨会话保持上下文、持久记忆、复杂多步推理等方面表现不佳。这种技术发展轨迹与半导体行业类似，当主频提升遇到物理极限后，行业转向多核架构带来新一轮创新。AI领域也正处于类似拐点，继续做大模型的边际收益正在递减。&lt;/p&gt;
&lt;p&gt;未来的关键问题不再是如何让模型更大，而是如何让系统更智能。这意味着需要从模型训练转向系统工程，关注模型之间的协作、信息流动和可靠性。AGI的突破不在于更大的Transformer，而在于能编排数百专用模型、跨会话保持上下文、围绕概率性组件执行确定性工作流，并在生产级规模下实现容错操作的分布式系统。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;上下文管理基础设施&lt;/strong&gt;：现有模型的注意力跨度仅为数千token，而人类的上下文跨度可以覆盖数年甚至一生。这种差距不仅是数量级的，更是质量上的。理想的上下文管理系统需要具备按需检索和过滤相关信息、构建并维护可持续演化的世界模型、跨领域桥接上下文、处理冲突信息等能力。这需要从简单的向量检索升级到可操作知识图谱，实现实时更新、查询和推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;记忆服务&lt;/strong&gt;：现有LLM没有真正的记忆，只能通过提示工程和上下文填充模拟记忆。AGI需要具备真正的记忆系统，能够在新证据出现时更新信念、跨多次体验整合信息形成一般性原则、忘记无关细节但避免灾难性遗忘、生成关于信息可靠性和来源的元知识。这不仅是数据库持久化，更是类人记忆系统，使用越多越牢固，久未使用则逐渐遗忘，遇新理解时能重组记忆结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;确定性工作流与概率性组件结合&lt;/strong&gt;：AGI的突破点在于用确定性框架包裹概率性组件。类似编译器的设计理念，整体流程可预测，但某些步骤可用启发式或概率优化。理想系统应根据问题特征将任务路由到合适的专用求解器、执行多步工作流并具备回滚与恢复能力、在接受概率性结果前进行确定性校验、以可预测方式组合各能力同时保留生成式模型的灵活性。不确定性应成为系统设计的一级公民。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;专用模型的模块化协作&lt;/strong&gt;：未来不是单一模型包打天下，而是数百数千个专用模型协同工作。语言模型在语言任务上表现优异，但在符号运算、视觉空间推理、时间规划、持续目标行为等方面表现不佳。应构建能够将问题路由到最适合的专用模型的系统、整合不同模型输出为统一解决方案、保持各组件兼容性允许独立进化、在个别模型失效时优雅处理不影响整体系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;三阶段架构路线图&lt;/strong&gt;：第一阶段为基础层，包含上下文管理服务、记忆服务、工作流引擎和Agent协调层；第二阶段为能力层，包含专用模型控制、符号推理引擎、规划与目标管理、跨模态整合；第三阶段为涌现层，真正的AGI将从上述各组件的协同作用中涌现，而非单一模型突破。系统能力将超越各部分总和，依赖于精心设计的架构。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.vincirufus.com/posts/agi-is-engineering-problem/"&gt;AGI is an Engineering Problem&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Vinci Rufus&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;未注明&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战</title><link>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</guid><description>&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;OpenAI内部访谈深入回顾了GPT-4.5的预训练全过程。团队以实现10倍智能提升为目标，经历了长达两年的研发，涉及ML与系统团队的深度协同设计、多集群训练中的硬件故障应对、大规模调试困境，以及从计算受限向数据受限的关键转变。访谈验证了Scaling Laws的持续有效性，并揭示了数据效率和容错机制等未来核心挑战。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/6nJZopACRuQ?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scaling Laws&lt;/strong&gt;：描述模型性能随计算量、数据量和参数规模增长而可预测提升的经验规律，是GPT-4.5训练规划和风险评估的核心方法论&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（Data Efficiency）&lt;/strong&gt;：指模型从有限数据中提取知识的能力，GPT-4.5研发标志着OpenAI从计算受限进入数据受限的新阶段，当前算法与人类学习效率仍有十万到百万倍的差距&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协同设计（Co-design）&lt;/strong&gt;：ML算法团队与系统基础设施团队在架构、通信、计算和存储等层面进行深度联合设计，确保软硬件在目标规模下高效协同工作&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;困惑度（Perplexity）&lt;/strong&gt;：衡量语言模型预测能力的核心指标，数值越低表示模型压缩和泛化能力越强，OpenAI以内部代码库的困惑度作为模型智能水平的关键标尺&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;风险解除（Derisking）&lt;/strong&gt;：在正式大规模训练前，从已知稳定配置出发逐层引入新特性并验证其跨规模扩展性的系统化实验流程，用于避免大规模训练中的不可预见失败&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视频链接：&lt;a href="https://www.youtube.com/watch?v=6nJZopACRuQ"&gt;Pre-Training GPT-4.5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;官方频道：&lt;a href="https://www.youtube.com/@OpenAI"&gt;OpenAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战-1"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;p&gt;近期，OpenAI 举办了一场特别的内部访谈，由 CEO Sam Altman 主持，邀请了参与 GPT-4.5 研发的核心技术成员——负责预训练数据与机器学习的 Alex Paino、首席系统架构师 Amin Tootoonchian 以及负责数据效率与算法的 Daniel Selsam，共同回顾了这款备受瞩目的大模型从构思到训练完成的艰辛历程。与以往发布新产品不同，这次讨论旨在揭示 GPT-4.5 背后深层次的研究投入、技术挑战与关键学习，回应社区对其超越 GPT-4 的优异表现及其成因的广泛兴趣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、 宏伟目标与规模化挑战的起点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GPT-4.5 项目的启动可追溯至访谈发生时的两年前，其核心目标被设定为实现相较于 GPT-4 约 &lt;code&gt;10x&lt;/code&gt; 的智能提升。这一雄心壮志的设定是基于对即将上线的新一代大规模计算集群能力的预判。团队预见到，这将是一项需要投入大量人力、时间及计算资源的庞大工程。正如 Amin Tootoonchian 所述，大型模型的训练过程是一个从概念诞生之初就需要机器学习（ML）与系统（Systems）团队深度协作，直至模型精确定义、训练启动并最终完成的复杂流程。而 GPT-4.5 的研发，更是集合了比以往模型（如 GPT-4）更多的人力，代表了一次在规模和复杂度上都截然不同的尝试。最终，团队认为所交付的模型在有效算力投入和智能水平上达到了预设的 &lt;code&gt;10x&lt;/code&gt; 目标。&lt;/p&gt;</description></item></channel></rss>