<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>预训练 on Linguista</title><link>https://linguista.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/</link><description>Recent content in 预训练 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/index.xml" rel="self" type="application/rss+xml"/><item><title>深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战</title><link>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/chat-notes/openai-gpt-4-5-pretraining-deep-dive/</guid><description>&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;OpenAI内部访谈深入回顾了GPT-4.5的预训练全过程。团队以实现10倍智能提升为目标，经历了长达两年的研发，涉及ML与系统团队的深度协同设计、多集群训练中的硬件故障应对、大规模调试困境，以及从计算受限向数据受限的关键转变。访谈验证了Scaling Laws的持续有效性，并揭示了数据效率和容错机制等未来核心挑战。&lt;/p&gt;
&lt;div class="video-card group relative w-full overflow-hidden rounded-2xl border border-border bg-surface shadow-sm transition hover:shadow-md "&gt;
 &lt;div style="aspect-ratio: 16/9;" class="w-full relative bg-black/5 dark:bg-white/5"&gt;
 &lt;iframe
 src="https://www.youtube-nocookie.com/embed/6nJZopACRuQ?rel=0&amp;amp;modestbranding=1&amp;amp;playsinline=1"
 title="深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战"
 class="absolute inset-0 w-full h-full border-0"
 loading="lazy"
 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
 allowfullscreen&gt;
 &lt;/iframe&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scaling Laws&lt;/strong&gt;：描述模型性能随计算量、数据量和参数规模增长而可预测提升的经验规律，是GPT-4.5训练规划和风险评估的核心方法论&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据效率（Data Efficiency）&lt;/strong&gt;：指模型从有限数据中提取知识的能力，GPT-4.5研发标志着OpenAI从计算受限进入数据受限的新阶段，当前算法与人类学习效率仍有十万到百万倍的差距&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协同设计（Co-design）&lt;/strong&gt;：ML算法团队与系统基础设施团队在架构、通信、计算和存储等层面进行深度联合设计，确保软硬件在目标规模下高效协同工作&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;困惑度（Perplexity）&lt;/strong&gt;：衡量语言模型预测能力的核心指标，数值越低表示模型压缩和泛化能力越强，OpenAI以内部代码库的困惑度作为模型智能水平的关键标尺&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;风险解除（Derisking）&lt;/strong&gt;：在正式大规模训练前，从已知稳定配置出发逐层引入新特性并验证其跨规模扩展性的系统化实验流程，用于避免大规模训练中的不可预见失败&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视频链接：&lt;a href="https://www.youtube.com/watch?v=6nJZopACRuQ"&gt;Pre-Training GPT-4.5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;官方频道：&lt;a href="https://www.youtube.com/@OpenAI"&gt;OpenAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="深度揭秘gpt-45预训练scaling-laws的极限探索与系统工程挑战-1"&gt;深度揭秘GPT-4.5预训练：Scaling Laws的极限探索与系统工程挑战&lt;/h1&gt;
&lt;p&gt;近期，OpenAI 举办了一场特别的内部访谈，由 CEO Sam Altman 主持，邀请了参与 GPT-4.5 研发的核心技术成员——负责预训练数据与机器学习的 Alex Paino、首席系统架构师 Amin Tootoonchian 以及负责数据效率与算法的 Daniel Selsam，共同回顾了这款备受瞩目的大模型从构思到训练完成的艰辛历程。与以往发布新产品不同，这次讨论旨在揭示 GPT-4.5 背后深层次的研究投入、技术挑战与关键学习，回应社区对其超越 GPT-4 的优异表现及其成因的广泛兴趣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、 宏伟目标与规模化挑战的起点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GPT-4.5 项目的启动可追溯至访谈发生时的两年前，其核心目标被设定为实现相较于 GPT-4 约 &lt;code&gt;10x&lt;/code&gt; 的智能提升。这一雄心壮志的设定是基于对即将上线的新一代大规模计算集群能力的预判。团队预见到，这将是一项需要投入大量人力、时间及计算资源的庞大工程。正如 Amin Tootoonchian 所述，大型模型的训练过程是一个从概念诞生之初就需要机器学习（ML）与系统（Systems）团队深度协作，直至模型精确定义、训练启动并最终完成的复杂流程。而 GPT-4.5 的研发，更是集合了比以往模型（如 GPT-4）更多的人力，代表了一次在规模和复杂度上都截然不同的尝试。最终，团队认为所交付的模型在有效算力投入和智能水平上达到了预设的 &lt;code&gt;10x&lt;/code&gt; 目标。&lt;/p&gt;</description></item><item><title>通往智能之巅 Anthropic预训练负责人对AI未来路径的深度洞察</title><link>https://linguista.cn/curated/henrinotes-2025-p1/anthropic-pretraining-future-ai-path-insights/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/curated/henrinotes-2025-p1/anthropic-pretraining-future-ai-path-insights/</guid><description>&lt;h1 id="通往智能之巅anthropic预训练负责人对ai未来路径的深度洞察"&gt;通往智能之巅：Anthropic预训练负责人对AI未来路径的深度洞察&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于Anthropic预训练团队负责人Nick Joseph的深度访谈，系统性地解析了AI发展的底层逻辑与未来路径。从预训练的核心哲学出发，探讨缩放定律如何驱动AI性能提升，揭示超大规模计算背后的工程挑战，并深入分析合成数据与AI对齐等前沿议题，为理解人工智能的未来发展提供了独特的视角。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;Nick Joseph的分享首先聚焦于预训练的本质——这并非简单的热身，而是构建智能模型的基石。通过&amp;quot;预测下一个词元&amp;quot;这一看似简单的任务，模型在海量数据中学习语言的语法、语义、逻辑乃至世界知识。更关键的是&amp;quot;缩放定律&amp;quot;的发现，它量化了计算量、数据量和模型参数量与模型性能之间的可预测关系，形成了一个推动AI飞速发展的正向反馈循环：训练更强大的模型→开发更有用的产品→获得更多收入→投入更多计算资源→训练更强大的模型。&lt;/p&gt;
&lt;p&gt;然而，理论的优雅与实践的复杂性之间横亘着巨大鸿沟。当模型规模达到数千甚至上万个GPU时，分布式框架的构建成为决定成败的关键。Anthropic团队甚至需要反向工程云服务提供商的硬件布局，通过聚类算法优化网络延迟。更具挑战的是硬件本身的不可靠性——在高规模训练环境中，GPU可能出错，电源供应可能不稳定，调试这些&amp;quot;计算机自身&amp;quot;的错误需要工程师具备从应用层直抵硬件层的深度洞察力。&lt;/p&gt;
&lt;p&gt;在数据维度，当互联网文本数据逐渐饱和时，数据枯竭的担忧浮现。Nick Joseph指出，互联网的&amp;quot;大小&amp;quot;本身难以量化，尤其是包含大量动态生成内容的&amp;quot;无限&amp;quot;页面。合成数据成为有前景的策略，但如何避免模型陷入&amp;quot;模式崩溃&amp;quot;或重复自身错误，成为亟待解决的问题。与此同时，AI对齐作为Anthropic的核心关切，关乎赋予AI我们所期望的目标和价值观——在AI变得极其智能之前，必须确保我们能够&amp;quot;操纵&amp;quot;它，使其行动符合人类意图。&lt;/p&gt;
&lt;p&gt;展望未来，Nick Joseph认为AI领域最大的挑战并非颠覆性新范式，而是根植于&amp;quot;深层工程&amp;quot;中的复杂性。那些&amp;quot;难以解决的bug&amp;quot;可能导致数月的研发停滞，追踪这些问题需要能够从抽象的ML原理下钻到字节级网络协议的罕见能力。因此，驱动AI前沿发展的最稀缺资源，是那些具备强大工程能力、能够解决最底层复杂系统问题的工程师。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;缩放定律&lt;/strong&gt;：这是AI发展的核心驱动力，量化了计算量、数据量和模型参数量如何以可预测的方式降低模型损失并提升性能。它不仅是一个技术发现，更形成了一个经济循环，推动着整个AI行业的飞速进化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分布式训练框架&lt;/strong&gt;：当模型规模达到数千GPU时，如何有效协调这些芯片协同工作成为关键。这需要数据并行、流水线并行、模型分片等策略的组合运用，甚至需要对硬件布局进行反向工程优化，体现了理论与工程实践的巨大鸿沟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;合成数据&lt;/strong&gt;：通过现有智能模型生成数据来训练新模型的有前景策略，但面临&amp;quot;模式崩溃&amp;quot;和重复自身错误的风险。如何识别、过滤甚至利用日益增多的LLM生成内容，成为重要的研究方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI对齐&lt;/strong&gt;：关乎赋予AI人类所期望的目标和价值观，如同&amp;quot;在汽车上安装方向盘&amp;quot;。这不仅包括控制模型的&amp;quot;个性&amp;quot;，更涉及如何让AGI的价值观能够被民主化地塑造和管理，是AI发展的核心伦理维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深层工程&lt;/strong&gt;：AI领域最大的挑战在于那些根植于工程复杂性问题中的&amp;quot;难以解决的bug&amp;quot;。解决这些问题需要工程师具备从抽象ML原理到字节级网络协议的全栈洞察力，这是驱动AI前沿发展的最稀缺能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=YFeb3yAxtjE"&gt;Anthropic Pretraining Lead - Future of AI Path Insights&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;Nick Joseph（Anthropic预训练团队负责人）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;来源&lt;/td&gt;
 &lt;td&gt;Y Combinator&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;整理&lt;/td&gt;
 &lt;td&gt;基于宝玉的分享提示词整理&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>