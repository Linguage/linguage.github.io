<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI评估 on Linguista</title><link>https://linguista.cn/tags/ai%E8%AF%84%E4%BC%B0/</link><description>Recent content in AI评估 on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/ai%E8%AF%84%E4%BC%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>下半场——人工智能从解决问题转向定义问题</title><link>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</link><pubDate>Mon, 26 Jan 2026 00:00:00 +0800</pubDate><guid>https://linguista.cn/rosetta/technology/the-second-half-of-ai/</guid><description>&lt;h1 id="下半场人工智能从解决问题转向定义问题"&gt;下半场——人工智能从解决问题转向定义问题&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文提出人工智能正处于中场休息阶段。上半场的核心是开发新的训练方法和模型，而下半场的焦点将从解决问题转向定义问题，评估将变得比训练更重要。作者通过强化学习的视角，阐述了语言预训练、规模化和推理行动三大要素如何构成一套通用配方，使得RL终于具备了泛化能力，从而彻底改变了AI研究的游戏规则。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;强化学习泛化&lt;/strong&gt;：指RL通过语言预训练获得先验知识，结合推理行动机制，能够跨任务迁移而非局限于单一环境&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReAct范式&lt;/strong&gt;：将推理作为一种特殊行动加入RL环境的行动空间，使智能体在决策前进行语言化思考，从而提升泛化能力&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI评估转向&lt;/strong&gt;：下半场的核心命题不再是能否训练模型解决某个问题，而是应该训练AI做什么以及如何衡量真正的进展&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配方标准化&lt;/strong&gt;：大规模语言预训练加规模化加推理行动的组合已将基准提升过程工业化，使得针对特定任务的方法创新价值大幅降低&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;先验知识&lt;/strong&gt;：在RL三要素中，语言预训练提供的先验被证明可能比算法和环境更为关键，这颠覆了传统RL研究的优先级排序&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心观点：我们正处于人工智能的中场休息。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原文连接：&lt;a href="https://ysymyth.github.io/The-Second-Half/"&gt;The Second Half&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;作者：&lt;a href="https://ysymyth.github.io/"&gt;Shunyu Yao&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;几十年来，人工智能的发展主要围绕着开发新的训练方法和模型。这确实取得了成效：从在国际象棋和围棋比赛中击败世界冠军，到在SAT和律师资格考试中超越大多数人类，再到赢得IMO和IOI金牌。在这些载入史册的里程碑——深蓝（DeepBlue）、AlphaGo、GPT-4以及o系列模型——背后，是人工智能方法的根本性创新：搜索、深度强化学习（deep RL）、规模化（scaling）和推理（reasoning）。随着时间的推移，一切都在变得更好。&lt;/p&gt;
&lt;p&gt;那么，现在突然发生了什么变化？&lt;/p&gt;
&lt;p&gt;三个词概括：强化学习（RL）终于奏效了。更准确地说：强化学习终于具备了泛化能力。在经历了数次重大的弯路和一系列里程碑式的积累之后，我们终于找到了一种行之有效的配方，能够利用语言和推理来解决各种各样的强化学习任务。哪怕就在一年前，如果你告诉大多数人工智能研究者，单一的配方就能处理软件工程、创意写作、IMO级别的数学、键鼠操作以及长篇问答——他们会嘲笑你在痴人说梦。这些任务中的每一项都极其困难，许多研究者整个博士生涯都只专注于其中一个狭窄的领域。&lt;/p&gt;
&lt;p&gt;然而，这确实发生了。&lt;/p&gt;
&lt;p&gt;那么接下来会发生什么？人工智能的下半场——从现在开始——将把焦点从&lt;strong&gt;解决问题&lt;/strong&gt;转向&lt;strong&gt;定义问题&lt;/strong&gt;。在这个新时代，&lt;strong&gt;评估&lt;/strong&gt;变得比&lt;strong&gt;训练&lt;/strong&gt;更重要。我们不再仅仅问：“我们能否训练一个模型来解决X问题？”，而是问：“我们应该训练人工智能去做什么？以及我们如何衡量真正的进展？” 要在下半场取得成功，我们需要及时转变思维模式和技能组合，也许更接近于一个产品经理。&lt;/p&gt;
&lt;h2 id="上半场"&gt;上半场&lt;/h2&gt;
&lt;p&gt;要理解上半场，看看它的赢家就知道了。你认为迄今为止最具影响力的人工智能论文是哪些？&lt;/p&gt;
&lt;p&gt;我试着回答了斯坦福224N课程里的这个问题，答案并不令人意外：Transformer、AlexNet、GPT-3等等。这些论文有什么共同点？它们提出了一些根本性的突破，用以训练出更好的模型。而且，它们都通过在某些基准（benchmarks）上展示出（显著的）改进而成功发表。&lt;/p&gt;
&lt;p&gt;不过，还有一个潜在的共同点：这些“赢家”都是&lt;strong&gt;训练方法或模型&lt;/strong&gt;，而不是&lt;strong&gt;基准或任务&lt;/strong&gt;。即使是 arguably 最具影响力的基准 ImageNet，其引用量也不到 AlexNet 的三分之一。方法与基准的对比在其他地方甚至更为悬殊——例如，Transformer 的主要基准是 WMT’14，其研讨会报告的引用量约为1300次，而 Transformer 的引用量超过了16万次。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/first_half.png" alt="上半场"&gt;&lt;/p&gt;
&lt;p&gt;这描绘了上半场的游戏规则：专注于构建新的模型和方法，而评估和基准是次要的（尽管对于论文发表体系的运作是必要的）。&lt;/p&gt;
&lt;p&gt;为什么会这样？一个重要原因是，在人工智能的上半场，&lt;strong&gt;方法比任务更难、更令人兴奋&lt;/strong&gt;。从零开始创建一个新的算法或模型架构——想想反向传播算法、卷积网络（AlexNet）或 GPT-3 中使用的 Transformer 等突破——需要非凡的洞察力和工程能力。相比之下，为人工智能定义任务通常感觉更直接：我们只需将人类已经在做的任务（如翻译、图像识别或下棋）转化为基准。这并不需要太多的洞察力，甚至工程量也不大。&lt;/p&gt;
&lt;p&gt;方法也往往比单个任务更通用、适用范围更广，这使得它们尤为宝贵。例如，Transformer 架构最终推动了计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）等许多领域的进步——远远超出了它最初证明自己的那个单一数据集（WMT’14 翻译）。一个优秀的、简洁且通用的新方法可以在许多不同的基准上取得进展（hillclimb），因此其影响往往超越单个任务。&lt;/p&gt;
&lt;p&gt;这场游戏已经持续了几十年，催生了改变世界的想法和突破，这些都体现在各个领域基准性能的不断提升上。那为什么游戏规则会发生改变呢？因为这些想法和突破的积累，在&lt;strong&gt;创造一个解决任务的有效配方&lt;/strong&gt;方面，产生了质的变化。&lt;/p&gt;
&lt;h2 id="配方"&gt;配方&lt;/h2&gt;
&lt;p&gt;这个配方是什么？不出所料，它的成分包括：大规模语言预训练、规模（数据和计算能力），以及推理和行动（reasoning and acting）的思想。这些听起来可能像是你在旧金山每天都能听到的流行词，但为什么称它们为“配方”？&lt;/p&gt;
&lt;p&gt;我们可以通过**强化学习（RL）**的视角来理解这一点。RL 常被认为是人工智能的“终局之战”——毕竟，理论上 RL 保证能赢得游戏，而经验上也难以想象任何超人系统（如 AlphaGo）没有 RL 的参与。&lt;/p&gt;
&lt;p&gt;在 RL 中，有三个关键组成部分：&lt;strong&gt;算法（algorithm）、环境（environment）和先验（priors）&lt;/strong&gt;。很长一段时间里，RL 研究者主要关注算法（例如 REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO……）——即智能体学习方式的智力核心——而将环境和先验视为固定的或最简化的。例如，Sutton 和 Barto 的经典教科书几乎全是关于算法，而很少涉及环境或先验。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ysymyth.github.io/images/second_half/rl_book.png" alt="RL 教科书"&gt;&lt;/p&gt;
&lt;p&gt;然而，在深度强化学习时代，环境在经验上变得非常重要：一个算法的性能往往高度依赖于其开发和测试的环境。如果你忽略环境，你可能会构建出一个只在玩具环境中表现出色的“最优”算法。那么，为什么我们不先弄清楚我们真正想要解决的环境，然后再找到最适合该环境的算法呢？&lt;/p&gt;
&lt;p&gt;这正是 OpenAI 最初的计划。它构建了 &lt;a href="https://openai.com/index/openai-gym-beta/"&gt;gym&lt;/a&gt;，一个包含各种游戏的标准 RL 环境，然后是 &lt;a href="https://openai.com/index/universe/"&gt;World of Bits 和 Universe 项目&lt;/a&gt;，试图将互联网或计算机变成一个游戏。这计划听起来不错，对吧？一旦我们将所有数字世界都变成环境，用智能 RL 算法解决它，我们就拥有了数字通用人工智能（digital AGI）。&lt;/p&gt;</description></item></channel></rss>