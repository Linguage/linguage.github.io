<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CUDA on Linguista</title><link>https://linguista.cn/tags/cuda/</link><description>Recent content in CUDA on Linguista</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 10 Jan 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://linguista.cn/tags/cuda/index.xml" rel="self" type="application/rss+xml"/><item><title>GPU与CUDA简介</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/gpu-cuda-introduction-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/gpu-cuda-introduction-guide/</guid><description>&lt;h1 id="gpu与cuda简介"&gt;GPU与CUDA简介&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文是一篇关于GPU与CUDA基础知识的系统介绍，从计算机基础概念出发，逐步深入到GPU硬件架构和CUDA编程模型。文章首先讲解了计算机基础知识，包括IO设备、CPU与内存、并行与并发、编译器以及堆与栈等核心概念。接着详细介绍GPU的基础知识，包括GPU作为计算设备的特性、流多处理器执行单元、GPU产品架构分类以及GPU内存层级结构。最后重点阐述CUDA编程平台和模型，涵盖CUDA编程基础、线程模型、内存管理和nvcc编译器等内容。整篇文章为理解GPU并行计算和CUDA编程提供了完整的知识框架。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章采用递进式的知识结构，从计算机基础到GPU专业知识，再到CUDA编程实践，形成了完整的学习路径。第一部分计算机基础知识为后续内容奠定基础，帮助读者理解CPU、内存、并行并发等核心概念。第二部分GPU基础知识介绍GPU的独特架构和计算优势，特别是流多处理器SM的设计使GPU能够大规模并行执行任务。第三部分CUDA编程基础是文章的核心内容，详细解释了CUDA的线程模型、内存管理和编译流程。&lt;/p&gt;
&lt;p&gt;文章强调GPU与CPU的根本区别在于并行计算能力。一个CPU核心在同一时间点只能执行一个任务，而一个GPU的流多处理器SM可以同时执行多达32个任务。这种设计使GPU在处理大规模并行计算任务时具有显著优势，特别适合深度学习、科学计算等领域。CUDA作为英伟达推出的通用并行计算平台和编程模型，为开发者提供了使用GPU进行并行计算的标准化方法。&lt;/p&gt;
&lt;p&gt;文章还指出计算机知识体系的特点是会抽象出很多概念帮助读者理解，但真实运作方式往往更加复杂。计算机领域从业者喜欢&amp;quot;起名&amp;quot;来突出产品亮点，这在GPU和CUDA的术语体系中表现得尤为明显。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;流多处理器SM&lt;/strong&gt;：GPU的基本执行单元，是GPU实现大规模并行计算的关键。一个SM在同一时间点最多可以执行32个任务，而一个CPU Core只能执行1个任务。SM中包含多个CUDA Core作为实际运算单元，这种设计使GPU能够同时处理大量并行任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUDA线程模型&lt;/strong&gt;：CUDA编程的核心概念，采用嵌套式结构。多个线程构成线程块，多个线程块构成网格。Grid和Block可以是一维、二维或三维数组结构。在核函数中，通过gridDim、blockIdx、blockDim、threadIdx等对象来管理和协调线程的执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU内存层级&lt;/strong&gt;：GPU的内存系统采用多级缓存结构，从高到低依次是寄存器文件、L1 Cache、L2 Cache和DRAM。寄存器文件和L1 Cache属于片上内存，存在于SM上；L2 Cache和DRAM属于片外内存。当寄存器内存不足时会发生溢出，溢出部分称为本地内存。共享内存存在于L1 Cache之上，供同一个block中的所有线程访问，用于减少访问高带宽内存HBM的次数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主机与设备&lt;/strong&gt;：在GPU编程中，将计算机称为主机，GPU称为设备。两者之间是异步执行关系，通过PCIe总线连接。CUDA编程需要明确区分主机函数和设备函数，使用__host__和__device__修饰符来标识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nvcc编译器&lt;/strong&gt;：CUDA程序的专用编译器，编译时将代码分为主机代码和设备代码。主机代码的编译过程与gcc类似，设备代码会先转换成PTX伪汇编代码，再编译成cubin形式的目标代码。这种分离式编译设计使得CUDA能够充分利用GPU的并行计算能力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://zhuanlan.zhihu.com/p/686772546"&gt;GPU 与 CUDA 简介 - 知乎&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;知乎作者&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-01-10&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>MI300X、H100与H200训练性能对比：CUDA护城河依然坚固</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/mi300x-h100-h200-training-benchmark-cuda-moat/</guid><description>&lt;h1 id="mi300xh100与h200训练性能对比cuda护城河依然坚固"&gt;MI300X、H100与H200训练性能对比：CUDA护城河依然坚固&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文基于SemiAnalysis团队为期五个月的独立基准测试研究，全面对比了AMD MI300X与NVIDIA H100/H200在AI模型训练场景下的实际性能表现。测试涵盖GEMM计算性能、HBM内存带宽、单节点与多节点训练吞吐量、集体通信效率等核心指标。研究结果表明，尽管MI300X在纸面规格上具有一定优势，但由于软件栈的诸多问题，其实际训练性能显著落后于NVIDIA产品。NVIDIA的CUDA生态系统护城河依然坚固，而AMD在开箱即用体验、软件稳定性、性能优化等方面仍需大量改进工作。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;本文首先介绍了研究的背景和方法论，SemiAnalysis团队与NVIDIA和AMD进行了深度互动，进行了为期五个月的独立测试，旨在反映真实用户在实际使用中可能遇到的情况。研究不仅关注纯性能指标，还特别重视用户体验和可用性，这是评估AI基础设施产品的重要维度。&lt;/p&gt;
&lt;p&gt;文章的核心发现部分揭示了AMD和NVIDIA产品在实际应用中的巨大差距。尽管MI300X的总拥有成本较低，但在按TCO计算的训练性能方面，使用AMD公共稳定版软件的表现不如H100/H200。关键问题在于AMD的软件体验存在诸多缺陷，导致MI300X的实际训练性能远低于其纸面规格。在BF16精度下，H100/H200的GEMM性能约为720 TFLOP/s，而MI300X仅为约620 TFLOP/s，比其市场宣传的1307 TFLOP/s低得多，比H100/H200慢14%。在FP8精度下，差距更为明显，H100/H200达到约1280 TFLOP/s，MI300X仅为约990 TFLOP/s，慢22%。&lt;/p&gt;
&lt;p&gt;文章深入分析了性能差距的技术原因，从GEMM性能、集体通信操作、网络拓扑等多个维度进行了详细对比。特别值得关注的是，研究指出了当前流行的GEMM基准测试方法存在严重缺陷，许多测试未正确清除L2缓存，且仅取最大性能值而非迭代过程中的中位数或平均值。在多节点训练场景下，H100的性能优势更加明显，这主要得益于NVIDIA的NVLink拓扑、InfiniBand SHARP技术以及高度优化的NCCL集体通信库。&lt;/p&gt;
&lt;p&gt;最后，文章对AMD提出了具体的改进建议，包括增加软件工程资源投入、改进GEMM库的启发式模型、减少对环境标志的依赖、加强内部测试流程等。作者指出，AMD许多库是基于NVIDIA开源或生态系统库分叉而来，这种策略长期来看不利于建立独立健康的软件生态系统。文章还提供了不同网络配置下GPU集群的成本和性能分析，为读者提供了全面的TCO视角。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GEMM性能与Transformer训练&lt;/strong&gt;：通用矩阵乘法是衡量Transformer架构模型训练性能的核心指标，因为现代深度学习模型的计算负载中，矩阵乘法操作占据了主导地位。研究显示，AMD MI300X在BF16和FP8精度下的GEMM性能均显著低于NVIDIA H100/H200，这是导致其实际训练性能落后于纸面规格的关键原因之一。值得注意的是，AMD的torch.matmul和F.Linear API在性能上存在差异，这是因为底层使用了不同的GEMM库，这种不一致性增加了用户的优化负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;集体通信操作与多节点训练&lt;/strong&gt;：在大规模分布式训练中，all_reduce、all_gather、reduce_scatter等集体通信操作的效率直接影响整体训练性能。NVIDIA的NCCL库经过多年优化，在单节点和多节点环境下都表现出色，特别是在配合NVLink和InfiniBand SHARP技术时，能够实现近乎线性的扩展性能。相比之下，AMD的RCCL库性能明显不足，在多节点场景下的差距尤为突出，这严重限制了MI300X在大规模训练任务中的竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUDA生态系统护城河&lt;/strong&gt;：NVIDIA的真正优势不仅体现在硬件性能上，更重要的是其软件生态系统的完整性。从开箱即用的PyTorch体验、高度优化的cuBLASLt库、智能的算法启发式模型，到完善的开发工具和文档，NVIDIA为用户提供了无缝的开发和部署体验。反观AMD，用户需要处理大量环境标志配置、手动调优、依赖库从源代码构建等复杂操作，这种体验差异构成了CUDA生态系统的深层次护城河。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总拥有成本与实际训练性能&lt;/strong&gt;：虽然MI300X的硬件价格较低，总拥有成本具有优势，但如果按TCO计算的实际训练性能来评估，使用AMD公共稳定版软件的性价比并不如NVIDIA产品。文章强调，即使使用AMD的定制开发构建能够改善性能，但等到这些构建成熟可用时，NVIDIA的下一代产品可能已经上市。这揭示了AI基础设施评估中一个重要原则：不能仅看硬件规格和价格，必须综合考虑软件成熟度、开发效率、时间成本等因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;软件质量与用户体验&lt;/strong&gt;：本文反复强调的一个核心观点是，AI训练硬件的竞争本质上是软件生态系统的竞争。AMD存在大量内部测试不足导致的软件问题，如PyTorch原生Flash Attention内核性能极低、注意力层与torch.compile兼容性问题、PYTORCH_TUNABLE_OPS标志导致的内存泄漏和程序崩溃等。这些看似细微的软件缺陷累积起来，会严重影响用户体验，降低开发效率，最终影响产品的市场竞争力。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/#executive-recommendation-to-amd"&gt;MI300X vs H100 vs H200 Benchmark Part 1: Training – CUDA Moat Still Alive&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;SemiAnalysis&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2024-12-22&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item><item><title>NVIDIA GPU核心详解：CUDA、Tensor与光线追踪核心架构</title><link>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-gpu-cores-cuda-tensor-raytracing-guide/</link><pubDate>Fri, 10 Jan 2025 00:00:00 +0800</pubDate><guid>https://linguista.cn/infos/tldrcards/henrinotes_2025_p3/nvidia-gpu-cores-cuda-tensor-raytracing-guide/</guid><description>&lt;h1 id="nvidia-gpu核心详解cudatensor与光线追踪核心架构"&gt;NVIDIA GPU核心详解：CUDA、Tensor与光线追踪核心架构&lt;/h1&gt;
&lt;h2 id="摘要"&gt;摘要&lt;/h2&gt;
&lt;p&gt;本文全面解析了NVIDIA GPU的三种核心架构：CUDA Cores作为并行计算的基石，负责处理大规模浮点运算和整数运算；Tensor Cores专为深度学习设计，通过混合精度计算实现AI任务的高效加速；Ray-Tracing Cores则是光线追踪渲染技术的专用核心。三者通过共享硬件资源和统一编程模型实现协同工作，在深度学习、游戏渲染和科学计算等领域发挥各自优势，共同推动GPU性能的持续突破。&lt;/p&gt;
&lt;h2 id="内容框架与概述"&gt;内容框架与概述&lt;/h2&gt;
&lt;p&gt;文章首先介绍了NVIDIA GPU Core在人工智能生态中的核心地位，指出现代NVIDIA GPU通过精心设计的多类型核心架构，实现了在计算性能、人工智能和图形渲染等领域的跨越式发展。这三种核心各司其职又紧密协作，构成了GPU强大性能的硬件基础。&lt;/p&gt;
&lt;p&gt;CUDA Cores作为GPU中最基础的处理单元，采用大规模并行设计，能够同时运行数百万个线程，在图像视频处理、科学计算和实时物理模拟等场景中表现卓越。其优势在于核心数量远超传统CPU，通过简化指令流水线实现高吞吐量计算。&lt;/p&gt;
&lt;p&gt;Tensor Cores则是NVIDIA针对AI工作负载的专用解决方案，首次在Volta架构中引入。相比传统CUDA Cores，Tensor Cores在矩阵运算方面具有显著性能优势，特别是在FP16和INT8混合精度计算中，可实现10倍以上的运算加速，成为深度学习训练和推理任务的核心驱动力。&lt;/p&gt;
&lt;p&gt;Ray-Tracing Cores代表了图形渲染技术的革命性突破，从Turing架构开始引入，专门用于加速光线追踪计算。通过硬件加速光线与场景的交互检测、路径追踪和动态光影渲染，实现了高分辨率游戏和虚拟现实场景中的实时光线追踪效果。&lt;/p&gt;
&lt;h2 id="核心概念及解读"&gt;核心概念及解读&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;CUDA Cores（并行计算单元）&lt;/strong&gt;：作为GPU的计算基础，CUDA Cores采用SIMT（单指令多线程）架构，能够同时执行大量相同操作。其核心价值在于将传统CPU的串行计算模式转变为大规模并行计算模式，特别适合图像处理、科学计算等可并行化的任务。在现代GPU中，CUDA Cores数量通常达到数千个，通过共享内存和寄存器实现高效的数据交换和任务调度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tensor Cores（张量计算单元）&lt;/strong&gt;：这是NVIDIA对AI计算需求的创新回应，专门为神经网络的矩阵乘法和累加运算而设计。与传统标量计算不同，Tensor Cores采用矩阵运算的并行化思路，一个指令周期内可完成4×4矩阵的乘加运算。在深度学习训练中，前向传播和反向传播都涉及大量矩阵运算，Tensor Cores的专用设计使其在这些场景下的性能达到CUDA Cores的数倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ray-Tracing Cores（光线追踪单元）&lt;/strong&gt;：光线追踪技术通过模拟光线在三维空间中的传播路径来生成逼真的光影效果，但计算复杂度极高。Ray-Tracing Cores通过专用硬件加速光线与三角形求交、包围盒层次结构遍历等关键计算，使得实时光线追踪成为可能。这不仅是图形渲染的技术突破，也为科学可视化、虚拟仿真等领域提供了强大的渲染工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SM（Streaming Multiprocessor）架构&lt;/strong&gt;：这是三种核心协同工作的硬件基础。每个SM模块同时集成CUDA Cores、Tensor Cores和Ray-Tracing Cores，共享L1缓存、寄存器文件和内存接口。NVIDIA的统一编程模型让开发者能够灵活调度三种核心的资源，根据任务类型自动分配最合适的计算单元，实现硬件资源的优化配置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;混合精度计算&lt;/strong&gt;：Tensor Cores的核心技术特征，通过在计算过程中使用FP16或INT8等低精度格式，在保持模型精度的同时显著提升计算速度和内存效率。这种精度与性能的平衡策略，使得大规模深度学习模型的训练和推理在时间和成本上都变得可行，推动了AI技术的快速普及。&lt;/p&gt;
&lt;h2 id="原文信息"&gt;原文信息&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;字段&lt;/th&gt;
 &lt;th&gt;内容&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;原文&lt;/td&gt;
 &lt;td&gt;&lt;a href="https://mp.weixin.qq.com/s/hIMv0OtLOWhH1_H613ieEw"&gt;一文读懂 NVIDIA GPU Core&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;作者&lt;/td&gt;
 &lt;td&gt;技术文章&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;发表日期&lt;/td&gt;
 &lt;td&gt;2025&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;此文档由 AI 自动整理&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>