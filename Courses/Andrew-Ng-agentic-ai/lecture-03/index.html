<!doctype html>
<html lang="zh-CN" class="skin-gold">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1355&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Linguista</title>
  <script>
    (function(){
      try{
        var key = 'theme';
        var stored = window.localStorage ? localStorage.getItem(key) : null;
        var prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        var theme = (stored === 'dark' || stored === 'light') ? stored : (prefersDark ? 'dark' : null);
        if (theme){
          document.documentElement.dataset.theme = theme;
        } else {
          delete document.documentElement.dataset.theme;
        }
      }catch(err){   }
    })();
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Lora:wght@500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/styles.css?v=20251015-02" />
  <link rel="stylesheet" href="/overrides.css?v=20251015-02" />
  <link rel="stylesheet" href="/css/search.css?v=20251015-02" />
  <link rel="stylesheet" href="/linkcard.css?v=20251015-02" />
  <link rel="stylesheet" href="/css/typography-mlsys.css?v=20251015-02" />
  
  <link rel="stylesheet" href="/css/skin-gold.css?v=20251015-02" />
  
  
  
  
  <style>
    :root{
      
      
      
      
      
      
    }
  </style>
  
  
  
  

<link rel="icon" href="/favicon_io/favicon.ico" sizes="any">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png">
<link rel="apple-touch-icon" href="/favicon_io/apple-touch-icon.png">
<link rel="manifest" href="/favicon_io/site.webmanifest">

  
  
<meta property="og:site_name" content="Linguista">
<meta property="og:type" content="article">
<meta property="og:title" content="Linguista"><meta property="og:description" content="Andrew Ng: Agentic AI Module 3: Tool Use 3.1 What are tools? 0:00 In this module, you learn about tool use by LLMs, and that means letting your LLM decide when it might want to request to have a function called to take some action, or gather some information, or do something else. 0:12 Just as we as humans can do a lot more with tools than we can with just our bare hands, LLMs too can also do a lot more with access to tools. 0:23 But rather than using hammers and spanners and pliers, when we give tools, that is, functions, for an LLM to request a call, that&#39;s what lets it do a lot more. 0:33 Let&#39;s take a look. 0:34 If you were to ask an LLM that&#39;s been trained maybe many months ago, what time is it right now? 0:40 Well, that trained model does not know exactly what time it is, and so hopefully it responds, sorry, I do not have access to the current time. 0:48 But if you were to write a function and give the LLM access to this function, then that lets it respond with a more useful answer. 0:56 When we let LLMs call functions, or more precisely, let an LLM request to call functions, that&#39;s what we mean by tool use, and the tools are just functions that we provide to the LLM that it can request to call. 1:08 In detail, this is how tool use works. 1:11 In this example, I&#39;m going to give the getCurrentTime function that I showed on the previous slide to the LLM. 1:18 When you then prompt it, what time is it, the LLM can decide to call the getCurrentTime function. 1:23 That will return the current time, which is then fed back to the LLM in the conversational history, and finally the LLM can output is, say, 3.20pm. 1:33 So the sequence of steps is, there&#39;s the input prompt. 1:36 The LLM, in this case, looks at the set of tools, which is just one tool in this example, but looks at the set of tools available, and it will decide in this example to call the tool. 1:47 The tool is a function that then returns a value, that value is fed back to the LLM, and then finally the LLM generates its output. 1:54 Now, one important aspect of tool use is, we can leave it up to the LLM to decide whether or not to use any of the tools. 2:03 So for the same setup, if I was asking it, how much caffeine is in green tea, the LLM doesn&#39;t need to know the current time to answer this, and so it can generate an answer directly, 2:12 green tea typically has this much caffeine, and it does so without invoking the getCurrentTime function. 2:18 In my slides, I&#39;m going to use this notation with this dashed box on top of the LLM to indicate that we&#39;re providing a set of tools to the LLM for the LLM to choose to use when it deems appropriate. 2:30 This is as opposed to some examples you saw in earlier videos, where I, as a developer, had hard-coded in, for example, that I will always do a web search at this point in the research agent. 2:41 In contrast, the getCurrentTime function call is not hard-coded in, it&#39;s up to the LLM to decide whether or not it wants to request a call to the getCurrentTime function. 2:51 And again, we&#39;re going to use this dashed box notation to indicate when we&#39;re giving one or more tools to the LLM for the LLM to decide what tools, if any, it wants to call. 3:02 Here are some more examples of when tool use may help an LLM-based app generate better answers. 3:08 If you were to ask it, can you find some Italian restaurants near Mountain View, California? 3:12 If it has a web search tool, then an LLM might elect to call a web search engine for a query, restaurants near Mountain View, California, and use the results that fetches to generate the output. 3:23 Or if you are running a retail store and you want to be able to answer questions like, show me customers who bought white sunglasses, if your LLM is given access to a query database tool, then it might look up the table of sales for what entries had a pair of white sunglasses sold and then use that to then generate the output. 3:44 Finally, if you wanted to do an interest rate calculation, if I were to deposit $500 and after 10 years, an interest rate of 5%, what would I have? 3:53 If you happen to have an interest calculation tool, then it could invoke the interest calculation function to calculate that. 4:01 Or it turns out, one thing you see later is letting an LLM write code, like just write a mathematical expression like this, and then to evaluate it, that would be another way to let an LLM calculate the right answer. 4:16 So as a developer, it&#39;ll be up to you to think through what are the sorts of things you want an application to really do, and then to create the functions or the tools that are needed to make them available to the LLM to let it use the appropriate tools to complete the sorts of tasks that maybe a restaurant recommender or a retail question answer or a finance assistant may want to do. 4:39 So depending on your application, you may have to implement and make different tools available to your LLM. 4:46 So far, most of the examples we&#39;ve gone through made only one tool or one function available to the LLM. 4:52 But there are many use cases where you want to make multiple tools or multiple functions available for the LLM for it to choose which of any to call. 4:59 For example, if you&#39;re building a calendar assistant agent, you might then want it to be able to fulfill requests like, please find a free slot on Thursday in my calendar and make an appointment with Alice. 5:11 So in this example, we might make available to the LLM a tool or a function to make an appointment, that is, to send a calendar invite, to check the calendar to see when I might be free, as well as to delete the appointment if it ever wants to cancel an existing calendar entry. 5:26 And so given the set of instructions, the LLM would first decide that of the different tools available, probably the first one it should use is check calendar. 5:35 So call a check calendar function that will return when I am free on Thursday. 5:40 Based on that information, which is fed back to the LLM, it can then decide that the next step is to pick a slot, let&#39;s say 3 p.m., and then to call the make appointment function to send a calendar invite to Alice, as well as to add it to my calendar. 5:56 The output of that, which hopefully is a confirmation that the calendar entry was sent out successfully, is fed back to the LLM, and then lastly, the LLM might tell me your appointment is set up with Alice at 3 p.m. Thursday. 6:08 Being able to give your LLM access to tools is a pretty big deal. It will make your applications much more powerful. 6:15 In the next video, we&#39;ll take a look at how to write functions, how to create tools to then make them available to your LLM. Let&#39;s go on to the next video. 3.2 Creating a tool 0:01 The process of how an LLM decides to call a function maybe seems a little bit mysterious 0:05 initially because an LLM is just trained to generate output text or output text tokens. 0:11 So how does that work? In this video, I&#39;d like to walk through with you step-by-step 0:15 what the process of getting an LLM to be able to get a function called really looks like. 0:21 Let&#39;s take a look. 0:22 So tools are just codes or functions that an LLM can request to be executed, 0:27 like this getCurrentTime function that we saw from the previous video. 0:31 Now, today&#39;s leading LLMs are all trained directly to use tools, 0:36 but I want to walk through with you what it would look like if you had to write prompts yourself 0:41 to tell it when to use tools, and this is what we had to do in an earlier era 0:46 before LLMs were trained directly to use tools. 0:49 And even though we don&#39;t do it exactly this way anymore, 0:51 this will hopefully give you a better understanding of the process, 0:54 and we&#39;ll walk through the more modern syntax in the next video. 0:57 If you&#39;ve implemented this function to getCurrentTime, 1:00 then in order to give this tool to the LLM, you might write a prompt like this. 1:05 You may tell it, LLM, you have access to a tool called getCurrentTime. 1:09 To use it, I want you to print out the following text. 1:12 Print out all caps function and then print out getCurrentTime. 1:15 And if I ever see this text, all caps function and then getCurrentTime, 1:19 that&#39;s when I know you want me to call the getCurrentTime function for you. 1:23 When a user asks, what time is it? 1:25 The LLM will then realize it needs to call or request to get called the getCurrentTime function. 1:29 And so the LLM will then output what it was told. 1:32 It&#39;ll output all caps function: getCurrentTime. 1:35 Now, I then have to have written code to look at the output of the LLM 1:40 to see if there is this all caps function. 1:42 And if so, then I need to pull out the argument of this getCurrentTime 1:47 to figure out what function the LLM wants to call. 1:49 And then I need to write code to actually call the getCurrentTime function 1:53 and then pull out the output, which is, let&#39;s say, 8 a.m. 1:57 And then it is the developer written code, my code, 2:00 that has to take 8 a.m. and feed that time, 8 a.m., 2:04 back into the LLM as part of this conversational history. 2:07 And the conversational history, of course, includes the initial user prompt, 2:10 the fact that the request is a function call, and so on. 2:13 And lastly, the LLM, knowing what had happened earlier, 2:17 that the user asks a question, requests a function call, 2:19 and then also that I call the function and return 8 a.m. 2:23 Finally, the LLM can look at all this and generate the final response, 2:26 which is, it is 8 a.m. 2:28 So to be clear, in order to call a function, 2:31 the LLM doesn&#39;t call the function directly. 2:34 It instead outputs something in a specific format like this 2:38 that tells me that I need to call the function for the LLM 2:41 and then tell the LLM what was the output of the function I requested. 2:45 In this example, we had given the LLM only a single function, 2:49 but you can imagine if we gave it three or four functions, 2:52 we could tell it to output functions in all caps, 2:55 then the name of the function it wants called, 2:57 and maybe even some arguments of these functions. 3:00 In fact, now let&#39;s take a look at a slightly more complex example 3:03 where the getCurrentTime function accepts an argument for the time zone 3:08 at which you want the current time. 3:10 For this second example, I&#39;ve written a function 3:14 that gets the current time in a specified time zone, 3:16 where here the time zone is the input argument 3:19 to the getCurrentTime function. 3:22 So to let the LLM use this tool to answer questions 3:25 like maybe, what time is it in New Zealand? 3:27 Because my answer is there, so before I call her up, 3:29 I do look up what time it is in New Zealand. 3:31 To let the LLM use this tool, you might modify the system prompt 3:35 to say you can use the getCurrentTime tool for a specific time zone. 3:39 To use it, I&#39;ll put the following, getCurrentTime, 3:41 and then, you know, include the time zone. 3:43 And this is an abbreviated prompt. 3:45 In practice, you might put more details than this into the prompt 3:48 to tell it what is the function, how to use it, and so on. 3:50 In this example, the LLM will then realize 3:53 it needs to fetch the time in New Zealand, 3:56 and so it will generate output like this, 3:58 function: getCurrentTime Pacific/Auckland. 4:02 This is the New Zealand time zone 4:04 because Auckland is a major city in New Zealand. 4:06 Then I have to write code to search for whether or not 4:10 this function all caps appeared in the LLM output, 4:13 and if so, then I need to pull out the function to call. 4:17 Lastly, I will then call getCurrentTime 4:19 with the specified arguments, which is generated by the LLM, 4:22 which is Pacific/Auckland, and maybe returns is 4 a.m. 4:25 Then as usual, I feed this to the LLM and the LLM outputs. 4:28 It is 4 a.m. in New Zealand. 4:30 To summarize, here&#39;s the process for getting LLM to use tools. 4:34 First, you have to provide the tool to the LLM, 4:37 implement the function, and then tell the LLM that it is available. 4:40 When the LLM decides to call a tool, 4:42 it then generates a specific output that lets you know 4:45 that you need to call the function for the LLM. 4:48 Then you call the function, get its output, 4:51 take the output of the function you just called, 4:53 and give that output back to the LLM, 4:55 and the LLM then uses that to go on to whatever it decides to do next, 4:59 which in our examples in this video was to just generate the final output, 5:03 but sometimes it may even decide that the next step 5:05 is to go call yet another tool, and the process continues. 5:09 Now, it turns out that this all-caps function syntax is a little bit clunky. 5:13 This is what we used to do before LLMs were trained natively 5:17 or to know by themselves how to request that tools be called. 5:21 With modern LLMs, you don&#39;t need to tell it to output all-caps function, 5:25 then search for all-caps function, and so on. 5:27 Instead, LLMs are trained to use a specific syntax 5:31 to request very clearly when it wants a tool called. 5:34 In the next video, I want to share with you 5:36 what the modern syntax actually looks like 5:38 for letting LLMs request to have tools be called. 5:42 Let&#39;s go on to the next video. 3.3 Tool syntax 0:01 Let&#39;s take a look at how to write code to have your LLM get tools called. 0:04 Here&#39;s our old getCurrentTime function without the time zone argument. 0:09 Let me show you how to use the AI Suite open source library in order to have your LLM call 0:14 tools. By the way, technically, as you saw from the last video, the LLM doesn&#39;t call the tool. 0:20 The LLM just requests that you call the tool. But among developers building agentic workflows, 0:25 many of us will occasionally just say the LLM calls the tool, even though it&#39;s not technically 0:30 what happens, but because it&#39;s just a shorter way to say it. 0:34 This syntax here is very similar to the OpenAI syntax for calling these LLMs, except that here, 0:41 I&#39;m using the AI Suite library, which is an open source package that some friends and I had worked 0:46 on that makes it easy to call multiple LLM providers. So the code syntax, and if this 0:53 looks like a lot to you, don&#39;t worry about it. You&#39;ll see more of this in the code labs. 0:57 But very briefly, this is very similar to the OpenAI syntax, where you say response equals 1:02 client check, completions create, then select the model, which in this case, we&#39;ll use the 1:07 OpenAI model GPT-4o, messages equals messages, assuming you&#39;ve put into an array here the 1:13 messages you want to pass the LLM, and it will say tools equals, then a list of the tools you want 1:19 the LLM to have access to. And in this case, there&#39;s just one tool, which is get current time, 1:23 and then don&#39;t worry too much about the max turns parameter. This is included because 1:28 after a tool call returns, the LLM might decide to call another tool, and after that tool call 1:33 returns, the LLM might decide to call yet another tool. So max turns is just a ceiling on how many 1:38 times you want the LLM to request one tool after another before you stop to just break out of a 1:44 possible infinite loop. In practice, you almost never hit this limit unless your code is doing 1:49 something unusually ambitious. So I wouldn&#39;t worry about the max turns parameter. I usually just set it to 1:54 five, but in practice, it doesn&#39;t matter that much. And it turns out that with AISuite, the function 2:00 get current time is automatically described to the LLM in an appropriate way to enable the LLM to 2:06 know when to call it. So rather than you needing to manually write a long prompt to tell the LLM, 2:12 once get current time, this syntax in AISuite does that automatically. And to make it seem not 2:18 too mysterious, the way it does that, it actually looks at the dot string associated with get current 2:23 time with this comments in get current time in order to figure out how to describe this function 2:30 to the LLM. So to illustrate how this works, here&#39;s the function again, and here&#39;s the 2:36 snippet of code using AISuite to call the LLM. Behind the scenes, what this will do is create a 2:43 JSON schema that describes the function in detail. And this over here on the right is what is actually 2:50 passed to the LLM. And specifically, it will pull the name of the function, which is get current time, 2:55 and then also a description of the function, which is pulled out from the doc string to tell the LLM 3:01 what this function does, which lets it decide when to call it. There&#39;s some APIs which require that 3:06 you manually construct this JSON schema and then pass this JSON schema to the LLM, but the AISuite 3:13 package does this automatically for you. To go through a slightly more complex example, if you 3:18 have this more complex get current time tool that also has an input time zone parameter, then AISuite 3:25 will create this more complex JSON schema where, as before, it pulls out the name of the function, 3:30 which is get current time, pulls out the description from the doc string, and then also identifies 3:35 what are the parameters and describes them to the LLM based on the documentation here shown on the 3:41 left, so that when it&#39;s generating the function arguments to call the tool, it knows that it 3:47 should be something like America/New York or Pacific/Auckland or some other time zone. 3:53 And so if you execute this code snippet here on the lower left, it will use the OpenAI 3:59 GPT-4o model, see if the LLM wants the function called, and if so, it&#39;ll call the function, 4:04 get the output from the function, feed that back to the LLM, and do that up to a maximum of five 4:10 turns and then return the response. Note that if the LLM requests to call the get current time 4:17 function, AISuite or this client, it will call the get current time for you, so you don&#39;t need 4:23 to explicitly do it yourself. All that is done in this single function call that you have to write. 4:29 Just note that there are some other implementations of LLM interfaces where you have to do that step 4:36 manually, but with this particular package, this is all wrapped into this client chat completions 4:41 create function call. So you now know how to get an LLM to call functions, and I hope that you enjoy 4:49 playing with this in the labs, and it&#39;s actually really amazing when you provide a few functions 4:54 to LLM and LLM decides to go and take action in the world, go and get more information 4:59 to fulfill your requests. If you haven&#39;t played with this before, I think you&#39;ll find this to be 5:04 really cool. It turns out that of all the tools you can give an LLM, there&#39;s one that&#39;s a bit special, 5:10 which is a code execution tool. It turns out to be really powerful. If you can tell an LLM, 5:16 you can write code, and I will have a tool to execute that code for you, because code can do 5:21 a lot of things, and we give an LLM the flexibility to write code and have code executed. That turns 5:28 out to be an incredibly powerful tool to give to LLMs. So code execution is special. Let&#39;s go 5:35 on to the next video to talk about the code execution tool for LLMs. 3.4 Code execution 0:05 In a few agentic applications I&#39;ve worked on, I gave the LLM the option to write code to then 0:06 carry out the task I wanted it to. And I&#39;ve been a few times now, I&#39;ve been really surprised and 0:10 delighted by the cleverness of the code solutions it generated in order to solve various tasks for 0:17 me. So if you haven&#39;t used code execution much, I think you might be surprised and delighted at 0:23 what this will let your LLM applications do. Let&#39;s take a look. Let&#39;s take an example of 0:29 building an application that can input math word problems and solve them for you. So you might 0:36 create tools that add numbers, subtract numbers, multiply numbers, and divide numbers. And if 0:40 someone says, please add 13.2 plus 18.9, then it triggers the add tool and then it gets you the 0:46 right answer. But what if someone now types in, what is the square root of two? Well, one thing 0:50 you could do is write a new tool for a square root, but then maybe some new thing is needed to 0:56 carry out exponentiation. And in fact, if you look at the number of buttons on your modern 1:02 scientific calculator, are you going to create a separate tool for every one of these buttons and 1:06 the many more things that we would want to do in math calculation? So instead of trying to implement 1:12 one tool after another, a different approach is to let it write and execute code. To tell the LLM 1:19 to write code, you might write a prompt like this. Write code to solve the user&#39;s query. Return your 1:24 answer as Python code delimited with execute Python and closing execute Python tags. So given a query 1:31 like what is the square root of two, the LLM might generate outputs like this. You can then use 1:37 pattern matching, for example, a regular expression to look for the start and end execute Python tags 1:44 and extract the code in between. So here you get these two lines of code shown in the green box, 1:50 and you can then execute this code for the LLM and get the output, in this case, 1.4142 and so on. 1:57 Lastly, this numerical answer is then passed back to the LLM and it can write a nicely formatted 2:04 answer to the original question. There are a few different ways you can carry out the code 2:08 execution step for the LLM. One is to use Python&#39;s exec function. This is a built-in Python function 2:15 which will execute whatever code you pass in. And this is very powerful for your LLM to really 2:21 write code and get you to execute that code, although there are some security implications 2:26 which we&#39;ll see later in this video. And then there are also some tools that will let you run the code 2:31 in a safer sandbox environment. And of course, square root of two is a relatively simple example. 2:38 An LLM can also accurately write code to, for example, do interest calculations and solve much harder 2:45 math calculations than this. One refinement to this idea, which you sort of saw in our section 2:52 on reflection, is that if code execution fails, so if for some reason the LLM had generated code 2:58 that wasn&#39;t quite correct, then passing that error message back to the LLM to let it reflect and 3:04 maybe revise this code and try another one or two times. That can sometimes also allow it to get a 3:10 more accurate answer. Now, running arbitrary code that an LLM generates does have a small chance of 3:17 causing something bad to happen. Recently, one of my team members was using a highly agentic coder 3:24 and it actually chose to remove star.py within a project directory. So this is actually a real 3:30 example. And eventually that agentic coder did apologize. It said, yes, that&#39;s actually right, 3:35 that was an incredibly stupid mistake. I guess I was glad that this agentic coder was really sorry, 3:40 but I already deleted a bunch of Python files. Unfortunately, the team member had it backed 3:44 up on GitHub repo, so there was no real harm done, but it would have been not great if this 3:50 arbitrary code, which made the mistake of deleting a bunch of files, had been executed 3:55 without the backup. So the best practice for code execution is to run it inside a sandbox 4:00 environment. In practice, the risk for any single line of code is not that high. So if I&#39;m being 4:07 candid, many developers will execute code from the LLM without too much checking. But if you want to 4:12 be a bit safer, then the best practice is to create a sandbox so that if an LLM generates bad 4:19 code, there&#39;s a lower risk of data loss or leakage of sensitive data and so on. So sandbox 4:26 environments like Docker or E2B as a lightweight sandbox environment can reduce the risk of 4:33 arbitrary codes being executed in a way that damages your system or your environment. 4:39 It turns out that code execution is so important that a lot of trainers of LLMs actually do special 4:46 work to make sure that code execution works well on their applications. But I hope that as you add 4:52 this as one more tool for you to potentially offer to LLMs or let you make your applications 4:58 much more powerful. So far and what we&#39;ve discussed, you have to create tools and make them 5:05 available one at a time to your LLM. It turns out that many different teams are building similar 5:11 tools and having to do all this work of building functions and making them available to the OMs. 5:18 But there is recently a new standard called MCP, Model Context Protocol, that&#39;s making it much 5:24 easier for developers to get access to a huge set of tools for LLMs to use. This is an important 5:31 protocol that more and more teams are using to develop LLM based applications. Let&#39;s go learn 5:37 about MCP in the next video. 3.5 MCP 0:00 MCP, the Model Context Protocol, was a standard proposed by Anthropic but now adopted by many 0:07 other companies and by many developers as a way to give an LLM access to more context and to 0:13 more tools. There are a lot of developers developing around the MCP ecosystem and so 0:18 learning about this will give you a lot more access to resources for your applications. 0:24 Let&#39;s take a look. This is the pain points that MCP attempts to solve. If one developer is writing 0:31 an application that wants to integrate with data from Slack and Google Drive and GitHub or access 0:37 data from a Postgres database, then they might have to write code to wrap around Slack APIs 0:42 to have functions to provide to the application, write code to wrap around Google Drive APIs to 0:47 parse the application, and similarly for these other tools or data sources. Then what has been 0:54 happening in the developer community is if a different team is building a different application, 0:59 then they too will integrate by themselves with Slack and Google Drive and GitHub and so on. 1:04 So many developers were all building custom wrappers around these types of data sources. 1:10 And so if there are M applications being developed and there are N tools out there, 1:16 the total amount of work done by the community was M times N. What MCP did was propose a standard 1:24 for applications to get access to tools and data sources so that the total work that needs to be 1:29 done by the community is now M plus N rather than M times N. The initial design of MCP focused a lot 1:38 on how to give more context to an LLM or how to fetch data. So a lot of the initial tools were 1:44 ones that would just fetch data. And if you read the MCP documentation, that refers to these as 1:51 resources. But MCP gives access to both data as well as the more general functions that an 1:57 application may want to call. And it turns out that there are many MCP clients. These are the 2:04 applications that want access to tools or to data as well as service, which are often the software 2:11 wrappers that then give access to data in Slack or GitHub or Google Drive or allows you to take 2:16 actions at these different types of resources. So today there&#39;s a rapidly growing list of MCP 2:23 clients that consume the tools or the resources as well as MCP service that provide the tools and 2:28 the resources. And I hope that you find it useful to build your own MCP client. Your application 2:35 maybe one day will be an MCP client. And if you want to provide resources to other developers, 2:40 maybe you can build your own MCP server someday. Let me show you a quick example of using an MCP 2:46 client. This is a cloud desktop app and it has been connected to a GitHub MCP server. So when 2:54 I enter this query, summarize the readme.md from the GitHub repo at this URL, this is actually 3:00 an AI suite repo. Then this application, which is an MCP client, uses the GitHub MCP server with 3:07 the request, please get the file readme.md from the repo AI suite from this repo. And then it 3:14 gets this response, which is pretty long. All this is then fed back to the LLMs context and the LLM 3:21 then generates the summary of the markdown file. Now let me enter another request, which is let me 3:27 enter what are the latest pull requests. This in turn causes the LLMs to use the MCP server to make 3:36 a different request, to list the pull request. This is another tool provided by GitHub&#39;s MCP 3:42 server. And so it makes this request with repo AI suite, sort, going to update it, list 20, and so on. 3:50 And then it gives this response, which is fed back to the LLM and the LLM then writes this nice 3:55 text summary of the latest pull request for this repo. MCP is an important standard. If you want to 4:01 learn more about it, DeepLearning.ai also has a short course that goes much deeper into just the MCP 4:08 protocol that you can check out after finishing the course, if you&#39;re interested. I hope this 4:13 video gives you a brief overview of why it&#39;s useful and also why many developers are now building to 4:20 this standard. This brings us to the last video on tool use. And I hope that by giving your own access 4:27 to tools, you build and build agentic applications that are much more powerful. In the next module, 4:34 we&#39;ll talk about evaluations and error analysis. It turns out that one of the things I&#39;ve seen 4:41 that distinguishes people that can execute agentic workflows really well versus teams that are not 4:47 as efficient at it is your ability to drive a disciplined evaluation process. In the next 4:55 set of videos, which I think is maybe the most important module of this entire course, 5:00 I hope to share with you some of the best practices of how to use evals to drive 5:05 development of agentic workflows. Look forward to seeing you in the next module. "><meta property="og:url" content="http://localhost:1355/courses/andrew-ng-agentic-ai/lecture-03/"><meta property="og:image" content="http://localhost:1355/img/Linguista_imresizer.png"><meta property="og:image:width" content="400">
  <meta property="og:image:height" content="400">
<meta name="twitter:card" content="summary"><meta name="twitter:site" content="@linguista2025"><meta name="twitter:creator" content="@linguista2025"><meta name="twitter:title" content="Linguista"><meta name="twitter:description" content="Andrew Ng: Agentic AI Module 3: Tool Use 3.1 What are tools? 0:00 In this module, you learn about tool use by LLMs, and that means letting your LLM decide when it might want to request to have a function called to take some action, or gather some information, or do something else. 0:12 Just as we as humans can do a lot more with tools than we can with just our bare hands, LLMs too can also do a lot more with access to tools. 0:23 But rather than using hammers and spanners and pliers, when we give tools, that is, functions, for an LLM to request a call, that&#39;s what lets it do a lot more. 0:33 Let&#39;s take a look. 0:34 If you were to ask an LLM that&#39;s been trained maybe many months ago, what time is it right now? 0:40 Well, that trained model does not know exactly what time it is, and so hopefully it responds, sorry, I do not have access to the current time. 0:48 But if you were to write a function and give the LLM access to this function, then that lets it respond with a more useful answer. 0:56 When we let LLMs call functions, or more precisely, let an LLM request to call functions, that&#39;s what we mean by tool use, and the tools are just functions that we provide to the LLM that it can request to call. 1:08 In detail, this is how tool use works. 1:11 In this example, I&#39;m going to give the getCurrentTime function that I showed on the previous slide to the LLM. 1:18 When you then prompt it, what time is it, the LLM can decide to call the getCurrentTime function. 1:23 That will return the current time, which is then fed back to the LLM in the conversational history, and finally the LLM can output is, say, 3.20pm. 1:33 So the sequence of steps is, there&#39;s the input prompt. 1:36 The LLM, in this case, looks at the set of tools, which is just one tool in this example, but looks at the set of tools available, and it will decide in this example to call the tool. 1:47 The tool is a function that then returns a value, that value is fed back to the LLM, and then finally the LLM generates its output. 1:54 Now, one important aspect of tool use is, we can leave it up to the LLM to decide whether or not to use any of the tools. 2:03 So for the same setup, if I was asking it, how much caffeine is in green tea, the LLM doesn&#39;t need to know the current time to answer this, and so it can generate an answer directly, 2:12 green tea typically has this much caffeine, and it does so without invoking the getCurrentTime function. 2:18 In my slides, I&#39;m going to use this notation with this dashed box on top of the LLM to indicate that we&#39;re providing a set of tools to the LLM for the LLM to choose to use when it deems appropriate. 2:30 This is as opposed to some examples you saw in earlier videos, where I, as a developer, had hard-coded in, for example, that I will always do a web search at this point in the research agent. 2:41 In contrast, the getCurrentTime function call is not hard-coded in, it&#39;s up to the LLM to decide whether or not it wants to request a call to the getCurrentTime function. 2:51 And again, we&#39;re going to use this dashed box notation to indicate when we&#39;re giving one or more tools to the LLM for the LLM to decide what tools, if any, it wants to call. 3:02 Here are some more examples of when tool use may help an LLM-based app generate better answers. 3:08 If you were to ask it, can you find some Italian restaurants near Mountain View, California? 3:12 If it has a web search tool, then an LLM might elect to call a web search engine for a query, restaurants near Mountain View, California, and use the results that fetches to generate the output. 3:23 Or if you are running a retail store and you want to be able to answer questions like, show me customers who bought white sunglasses, if your LLM is given access to a query database tool, then it might look up the table of sales for what entries had a pair of white sunglasses sold and then use that to then generate the output. 3:44 Finally, if you wanted to do an interest rate calculation, if I were to deposit $500 and after 10 years, an interest rate of 5%, what would I have? 3:53 If you happen to have an interest calculation tool, then it could invoke the interest calculation function to calculate that. 4:01 Or it turns out, one thing you see later is letting an LLM write code, like just write a mathematical expression like this, and then to evaluate it, that would be another way to let an LLM calculate the right answer. 4:16 So as a developer, it&#39;ll be up to you to think through what are the sorts of things you want an application to really do, and then to create the functions or the tools that are needed to make them available to the LLM to let it use the appropriate tools to complete the sorts of tasks that maybe a restaurant recommender or a retail question answer or a finance assistant may want to do. 4:39 So depending on your application, you may have to implement and make different tools available to your LLM. 4:46 So far, most of the examples we&#39;ve gone through made only one tool or one function available to the LLM. 4:52 But there are many use cases where you want to make multiple tools or multiple functions available for the LLM for it to choose which of any to call. 4:59 For example, if you&#39;re building a calendar assistant agent, you might then want it to be able to fulfill requests like, please find a free slot on Thursday in my calendar and make an appointment with Alice. 5:11 So in this example, we might make available to the LLM a tool or a function to make an appointment, that is, to send a calendar invite, to check the calendar to see when I might be free, as well as to delete the appointment if it ever wants to cancel an existing calendar entry. 5:26 And so given the set of instructions, the LLM would first decide that of the different tools available, probably the first one it should use is check calendar. 5:35 So call a check calendar function that will return when I am free on Thursday. 5:40 Based on that information, which is fed back to the LLM, it can then decide that the next step is to pick a slot, let&#39;s say 3 p.m., and then to call the make appointment function to send a calendar invite to Alice, as well as to add it to my calendar. 5:56 The output of that, which hopefully is a confirmation that the calendar entry was sent out successfully, is fed back to the LLM, and then lastly, the LLM might tell me your appointment is set up with Alice at 3 p.m. Thursday. 6:08 Being able to give your LLM access to tools is a pretty big deal. It will make your applications much more powerful. 6:15 In the next video, we&#39;ll take a look at how to write functions, how to create tools to then make them available to your LLM. Let&#39;s go on to the next video. 3.2 Creating a tool 0:01 The process of how an LLM decides to call a function maybe seems a little bit mysterious 0:05 initially because an LLM is just trained to generate output text or output text tokens. 0:11 So how does that work? In this video, I&#39;d like to walk through with you step-by-step 0:15 what the process of getting an LLM to be able to get a function called really looks like. 0:21 Let&#39;s take a look. 0:22 So tools are just codes or functions that an LLM can request to be executed, 0:27 like this getCurrentTime function that we saw from the previous video. 0:31 Now, today&#39;s leading LLMs are all trained directly to use tools, 0:36 but I want to walk through with you what it would look like if you had to write prompts yourself 0:41 to tell it when to use tools, and this is what we had to do in an earlier era 0:46 before LLMs were trained directly to use tools. 0:49 And even though we don&#39;t do it exactly this way anymore, 0:51 this will hopefully give you a better understanding of the process, 0:54 and we&#39;ll walk through the more modern syntax in the next video. 0:57 If you&#39;ve implemented this function to getCurrentTime, 1:00 then in order to give this tool to the LLM, you might write a prompt like this. 1:05 You may tell it, LLM, you have access to a tool called getCurrentTime. 1:09 To use it, I want you to print out the following text. 1:12 Print out all caps function and then print out getCurrentTime. 1:15 And if I ever see this text, all caps function and then getCurrentTime, 1:19 that&#39;s when I know you want me to call the getCurrentTime function for you. 1:23 When a user asks, what time is it? 1:25 The LLM will then realize it needs to call or request to get called the getCurrentTime function. 1:29 And so the LLM will then output what it was told. 1:32 It&#39;ll output all caps function: getCurrentTime. 1:35 Now, I then have to have written code to look at the output of the LLM 1:40 to see if there is this all caps function. 1:42 And if so, then I need to pull out the argument of this getCurrentTime 1:47 to figure out what function the LLM wants to call. 1:49 And then I need to write code to actually call the getCurrentTime function 1:53 and then pull out the output, which is, let&#39;s say, 8 a.m. 1:57 And then it is the developer written code, my code, 2:00 that has to take 8 a.m. and feed that time, 8 a.m., 2:04 back into the LLM as part of this conversational history. 2:07 And the conversational history, of course, includes the initial user prompt, 2:10 the fact that the request is a function call, and so on. 2:13 And lastly, the LLM, knowing what had happened earlier, 2:17 that the user asks a question, requests a function call, 2:19 and then also that I call the function and return 8 a.m. 2:23 Finally, the LLM can look at all this and generate the final response, 2:26 which is, it is 8 a.m. 2:28 So to be clear, in order to call a function, 2:31 the LLM doesn&#39;t call the function directly. 2:34 It instead outputs something in a specific format like this 2:38 that tells me that I need to call the function for the LLM 2:41 and then tell the LLM what was the output of the function I requested. 2:45 In this example, we had given the LLM only a single function, 2:49 but you can imagine if we gave it three or four functions, 2:52 we could tell it to output functions in all caps, 2:55 then the name of the function it wants called, 2:57 and maybe even some arguments of these functions. 3:00 In fact, now let&#39;s take a look at a slightly more complex example 3:03 where the getCurrentTime function accepts an argument for the time zone 3:08 at which you want the current time. 3:10 For this second example, I&#39;ve written a function 3:14 that gets the current time in a specified time zone, 3:16 where here the time zone is the input argument 3:19 to the getCurrentTime function. 3:22 So to let the LLM use this tool to answer questions 3:25 like maybe, what time is it in New Zealand? 3:27 Because my answer is there, so before I call her up, 3:29 I do look up what time it is in New Zealand. 3:31 To let the LLM use this tool, you might modify the system prompt 3:35 to say you can use the getCurrentTime tool for a specific time zone. 3:39 To use it, I&#39;ll put the following, getCurrentTime, 3:41 and then, you know, include the time zone. 3:43 And this is an abbreviated prompt. 3:45 In practice, you might put more details than this into the prompt 3:48 to tell it what is the function, how to use it, and so on. 3:50 In this example, the LLM will then realize 3:53 it needs to fetch the time in New Zealand, 3:56 and so it will generate output like this, 3:58 function: getCurrentTime Pacific/Auckland. 4:02 This is the New Zealand time zone 4:04 because Auckland is a major city in New Zealand. 4:06 Then I have to write code to search for whether or not 4:10 this function all caps appeared in the LLM output, 4:13 and if so, then I need to pull out the function to call. 4:17 Lastly, I will then call getCurrentTime 4:19 with the specified arguments, which is generated by the LLM, 4:22 which is Pacific/Auckland, and maybe returns is 4 a.m. 4:25 Then as usual, I feed this to the LLM and the LLM outputs. 4:28 It is 4 a.m. in New Zealand. 4:30 To summarize, here&#39;s the process for getting LLM to use tools. 4:34 First, you have to provide the tool to the LLM, 4:37 implement the function, and then tell the LLM that it is available. 4:40 When the LLM decides to call a tool, 4:42 it then generates a specific output that lets you know 4:45 that you need to call the function for the LLM. 4:48 Then you call the function, get its output, 4:51 take the output of the function you just called, 4:53 and give that output back to the LLM, 4:55 and the LLM then uses that to go on to whatever it decides to do next, 4:59 which in our examples in this video was to just generate the final output, 5:03 but sometimes it may even decide that the next step 5:05 is to go call yet another tool, and the process continues. 5:09 Now, it turns out that this all-caps function syntax is a little bit clunky. 5:13 This is what we used to do before LLMs were trained natively 5:17 or to know by themselves how to request that tools be called. 5:21 With modern LLMs, you don&#39;t need to tell it to output all-caps function, 5:25 then search for all-caps function, and so on. 5:27 Instead, LLMs are trained to use a specific syntax 5:31 to request very clearly when it wants a tool called. 5:34 In the next video, I want to share with you 5:36 what the modern syntax actually looks like 5:38 for letting LLMs request to have tools be called. 5:42 Let&#39;s go on to the next video. 3.3 Tool syntax 0:01 Let&#39;s take a look at how to write code to have your LLM get tools called. 0:04 Here&#39;s our old getCurrentTime function without the time zone argument. 0:09 Let me show you how to use the AI Suite open source library in order to have your LLM call 0:14 tools. By the way, technically, as you saw from the last video, the LLM doesn&#39;t call the tool. 0:20 The LLM just requests that you call the tool. But among developers building agentic workflows, 0:25 many of us will occasionally just say the LLM calls the tool, even though it&#39;s not technically 0:30 what happens, but because it&#39;s just a shorter way to say it. 0:34 This syntax here is very similar to the OpenAI syntax for calling these LLMs, except that here, 0:41 I&#39;m using the AI Suite library, which is an open source package that some friends and I had worked 0:46 on that makes it easy to call multiple LLM providers. So the code syntax, and if this 0:53 looks like a lot to you, don&#39;t worry about it. You&#39;ll see more of this in the code labs. 0:57 But very briefly, this is very similar to the OpenAI syntax, where you say response equals 1:02 client check, completions create, then select the model, which in this case, we&#39;ll use the 1:07 OpenAI model GPT-4o, messages equals messages, assuming you&#39;ve put into an array here the 1:13 messages you want to pass the LLM, and it will say tools equals, then a list of the tools you want 1:19 the LLM to have access to. And in this case, there&#39;s just one tool, which is get current time, 1:23 and then don&#39;t worry too much about the max turns parameter. This is included because 1:28 after a tool call returns, the LLM might decide to call another tool, and after that tool call 1:33 returns, the LLM might decide to call yet another tool. So max turns is just a ceiling on how many 1:38 times you want the LLM to request one tool after another before you stop to just break out of a 1:44 possible infinite loop. In practice, you almost never hit this limit unless your code is doing 1:49 something unusually ambitious. So I wouldn&#39;t worry about the max turns parameter. I usually just set it to 1:54 five, but in practice, it doesn&#39;t matter that much. And it turns out that with AISuite, the function 2:00 get current time is automatically described to the LLM in an appropriate way to enable the LLM to 2:06 know when to call it. So rather than you needing to manually write a long prompt to tell the LLM, 2:12 once get current time, this syntax in AISuite does that automatically. And to make it seem not 2:18 too mysterious, the way it does that, it actually looks at the dot string associated with get current 2:23 time with this comments in get current time in order to figure out how to describe this function 2:30 to the LLM. So to illustrate how this works, here&#39;s the function again, and here&#39;s the 2:36 snippet of code using AISuite to call the LLM. Behind the scenes, what this will do is create a 2:43 JSON schema that describes the function in detail. And this over here on the right is what is actually 2:50 passed to the LLM. And specifically, it will pull the name of the function, which is get current time, 2:55 and then also a description of the function, which is pulled out from the doc string to tell the LLM 3:01 what this function does, which lets it decide when to call it. There&#39;s some APIs which require that 3:06 you manually construct this JSON schema and then pass this JSON schema to the LLM, but the AISuite 3:13 package does this automatically for you. To go through a slightly more complex example, if you 3:18 have this more complex get current time tool that also has an input time zone parameter, then AISuite 3:25 will create this more complex JSON schema where, as before, it pulls out the name of the function, 3:30 which is get current time, pulls out the description from the doc string, and then also identifies 3:35 what are the parameters and describes them to the LLM based on the documentation here shown on the 3:41 left, so that when it&#39;s generating the function arguments to call the tool, it knows that it 3:47 should be something like America/New York or Pacific/Auckland or some other time zone. 3:53 And so if you execute this code snippet here on the lower left, it will use the OpenAI 3:59 GPT-4o model, see if the LLM wants the function called, and if so, it&#39;ll call the function, 4:04 get the output from the function, feed that back to the LLM, and do that up to a maximum of five 4:10 turns and then return the response. Note that if the LLM requests to call the get current time 4:17 function, AISuite or this client, it will call the get current time for you, so you don&#39;t need 4:23 to explicitly do it yourself. All that is done in this single function call that you have to write. 4:29 Just note that there are some other implementations of LLM interfaces where you have to do that step 4:36 manually, but with this particular package, this is all wrapped into this client chat completions 4:41 create function call. So you now know how to get an LLM to call functions, and I hope that you enjoy 4:49 playing with this in the labs, and it&#39;s actually really amazing when you provide a few functions 4:54 to LLM and LLM decides to go and take action in the world, go and get more information 4:59 to fulfill your requests. If you haven&#39;t played with this before, I think you&#39;ll find this to be 5:04 really cool. It turns out that of all the tools you can give an LLM, there&#39;s one that&#39;s a bit special, 5:10 which is a code execution tool. It turns out to be really powerful. If you can tell an LLM, 5:16 you can write code, and I will have a tool to execute that code for you, because code can do 5:21 a lot of things, and we give an LLM the flexibility to write code and have code executed. That turns 5:28 out to be an incredibly powerful tool to give to LLMs. So code execution is special. Let&#39;s go 5:35 on to the next video to talk about the code execution tool for LLMs. 3.4 Code execution 0:05 In a few agentic applications I&#39;ve worked on, I gave the LLM the option to write code to then 0:06 carry out the task I wanted it to. And I&#39;ve been a few times now, I&#39;ve been really surprised and 0:10 delighted by the cleverness of the code solutions it generated in order to solve various tasks for 0:17 me. So if you haven&#39;t used code execution much, I think you might be surprised and delighted at 0:23 what this will let your LLM applications do. Let&#39;s take a look. Let&#39;s take an example of 0:29 building an application that can input math word problems and solve them for you. So you might 0:36 create tools that add numbers, subtract numbers, multiply numbers, and divide numbers. And if 0:40 someone says, please add 13.2 plus 18.9, then it triggers the add tool and then it gets you the 0:46 right answer. But what if someone now types in, what is the square root of two? Well, one thing 0:50 you could do is write a new tool for a square root, but then maybe some new thing is needed to 0:56 carry out exponentiation. And in fact, if you look at the number of buttons on your modern 1:02 scientific calculator, are you going to create a separate tool for every one of these buttons and 1:06 the many more things that we would want to do in math calculation? So instead of trying to implement 1:12 one tool after another, a different approach is to let it write and execute code. To tell the LLM 1:19 to write code, you might write a prompt like this. Write code to solve the user&#39;s query. Return your 1:24 answer as Python code delimited with execute Python and closing execute Python tags. So given a query 1:31 like what is the square root of two, the LLM might generate outputs like this. You can then use 1:37 pattern matching, for example, a regular expression to look for the start and end execute Python tags 1:44 and extract the code in between. So here you get these two lines of code shown in the green box, 1:50 and you can then execute this code for the LLM and get the output, in this case, 1.4142 and so on. 1:57 Lastly, this numerical answer is then passed back to the LLM and it can write a nicely formatted 2:04 answer to the original question. There are a few different ways you can carry out the code 2:08 execution step for the LLM. One is to use Python&#39;s exec function. This is a built-in Python function 2:15 which will execute whatever code you pass in. And this is very powerful for your LLM to really 2:21 write code and get you to execute that code, although there are some security implications 2:26 which we&#39;ll see later in this video. And then there are also some tools that will let you run the code 2:31 in a safer sandbox environment. And of course, square root of two is a relatively simple example. 2:38 An LLM can also accurately write code to, for example, do interest calculations and solve much harder 2:45 math calculations than this. One refinement to this idea, which you sort of saw in our section 2:52 on reflection, is that if code execution fails, so if for some reason the LLM had generated code 2:58 that wasn&#39;t quite correct, then passing that error message back to the LLM to let it reflect and 3:04 maybe revise this code and try another one or two times. That can sometimes also allow it to get a 3:10 more accurate answer. Now, running arbitrary code that an LLM generates does have a small chance of 3:17 causing something bad to happen. Recently, one of my team members was using a highly agentic coder 3:24 and it actually chose to remove star.py within a project directory. So this is actually a real 3:30 example. And eventually that agentic coder did apologize. It said, yes, that&#39;s actually right, 3:35 that was an incredibly stupid mistake. I guess I was glad that this agentic coder was really sorry, 3:40 but I already deleted a bunch of Python files. Unfortunately, the team member had it backed 3:44 up on GitHub repo, so there was no real harm done, but it would have been not great if this 3:50 arbitrary code, which made the mistake of deleting a bunch of files, had been executed 3:55 without the backup. So the best practice for code execution is to run it inside a sandbox 4:00 environment. In practice, the risk for any single line of code is not that high. So if I&#39;m being 4:07 candid, many developers will execute code from the LLM without too much checking. But if you want to 4:12 be a bit safer, then the best practice is to create a sandbox so that if an LLM generates bad 4:19 code, there&#39;s a lower risk of data loss or leakage of sensitive data and so on. So sandbox 4:26 environments like Docker or E2B as a lightweight sandbox environment can reduce the risk of 4:33 arbitrary codes being executed in a way that damages your system or your environment. 4:39 It turns out that code execution is so important that a lot of trainers of LLMs actually do special 4:46 work to make sure that code execution works well on their applications. But I hope that as you add 4:52 this as one more tool for you to potentially offer to LLMs or let you make your applications 4:58 much more powerful. So far and what we&#39;ve discussed, you have to create tools and make them 5:05 available one at a time to your LLM. It turns out that many different teams are building similar 5:11 tools and having to do all this work of building functions and making them available to the OMs. 5:18 But there is recently a new standard called MCP, Model Context Protocol, that&#39;s making it much 5:24 easier for developers to get access to a huge set of tools for LLMs to use. This is an important 5:31 protocol that more and more teams are using to develop LLM based applications. Let&#39;s go learn 5:37 about MCP in the next video. 3.5 MCP 0:00 MCP, the Model Context Protocol, was a standard proposed by Anthropic but now adopted by many 0:07 other companies and by many developers as a way to give an LLM access to more context and to 0:13 more tools. There are a lot of developers developing around the MCP ecosystem and so 0:18 learning about this will give you a lot more access to resources for your applications. 0:24 Let&#39;s take a look. This is the pain points that MCP attempts to solve. If one developer is writing 0:31 an application that wants to integrate with data from Slack and Google Drive and GitHub or access 0:37 data from a Postgres database, then they might have to write code to wrap around Slack APIs 0:42 to have functions to provide to the application, write code to wrap around Google Drive APIs to 0:47 parse the application, and similarly for these other tools or data sources. Then what has been 0:54 happening in the developer community is if a different team is building a different application, 0:59 then they too will integrate by themselves with Slack and Google Drive and GitHub and so on. 1:04 So many developers were all building custom wrappers around these types of data sources. 1:10 And so if there are M applications being developed and there are N tools out there, 1:16 the total amount of work done by the community was M times N. What MCP did was propose a standard 1:24 for applications to get access to tools and data sources so that the total work that needs to be 1:29 done by the community is now M plus N rather than M times N. The initial design of MCP focused a lot 1:38 on how to give more context to an LLM or how to fetch data. So a lot of the initial tools were 1:44 ones that would just fetch data. And if you read the MCP documentation, that refers to these as 1:51 resources. But MCP gives access to both data as well as the more general functions that an 1:57 application may want to call. And it turns out that there are many MCP clients. These are the 2:04 applications that want access to tools or to data as well as service, which are often the software 2:11 wrappers that then give access to data in Slack or GitHub or Google Drive or allows you to take 2:16 actions at these different types of resources. So today there&#39;s a rapidly growing list of MCP 2:23 clients that consume the tools or the resources as well as MCP service that provide the tools and 2:28 the resources. And I hope that you find it useful to build your own MCP client. Your application 2:35 maybe one day will be an MCP client. And if you want to provide resources to other developers, 2:40 maybe you can build your own MCP server someday. Let me show you a quick example of using an MCP 2:46 client. This is a cloud desktop app and it has been connected to a GitHub MCP server. So when 2:54 I enter this query, summarize the readme.md from the GitHub repo at this URL, this is actually 3:00 an AI suite repo. Then this application, which is an MCP client, uses the GitHub MCP server with 3:07 the request, please get the file readme.md from the repo AI suite from this repo. And then it 3:14 gets this response, which is pretty long. All this is then fed back to the LLMs context and the LLM 3:21 then generates the summary of the markdown file. Now let me enter another request, which is let me 3:27 enter what are the latest pull requests. This in turn causes the LLMs to use the MCP server to make 3:36 a different request, to list the pull request. This is another tool provided by GitHub&#39;s MCP 3:42 server. And so it makes this request with repo AI suite, sort, going to update it, list 20, and so on. 3:50 And then it gives this response, which is fed back to the LLM and the LLM then writes this nice 3:55 text summary of the latest pull request for this repo. MCP is an important standard. If you want to 4:01 learn more about it, DeepLearning.ai also has a short course that goes much deeper into just the MCP 4:08 protocol that you can check out after finishing the course, if you&#39;re interested. I hope this 4:13 video gives you a brief overview of why it&#39;s useful and also why many developers are now building to 4:20 this standard. This brings us to the last video on tool use. And I hope that by giving your own access 4:27 to tools, you build and build agentic applications that are much more powerful. In the next module, 4:34 we&#39;ll talk about evaluations and error analysis. It turns out that one of the things I&#39;ve seen 4:41 that distinguishes people that can execute agentic workflows really well versus teams that are not 4:47 as efficient at it is your ability to drive a disciplined evaluation process. In the next 4:55 set of videos, which I think is maybe the most important module of this entire course, 5:00 I hope to share with you some of the best practices of how to use evals to drive 5:05 development of agentic workflows. Look forward to seeing you in the next module. "><meta name="twitter:image" content="http://localhost:1355/img/Linguista_imresizer.png">
<link rel="canonical" href="http://localhost:1355/courses/andrew-ng-agentic-ai/lecture-03/">

  
  
  <script>
    window.MathJax = {
      loader: { load: ['[tex]/ams'] },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        packages: { '[+]': ['ams'] },
        macros: {
          unicodeInt: ['\\mathop{\\vcenter{\\mathchoice{\\huge\\unicode{#1}}{\\unicode{#1}}{\\unicode{#1}}{\\unicode{#1}}}}\\nolimits', 1],
          oiint: '\\unicodeInt{x222F}',
          oiiint: '\\unicodeInt{x2230}'
        }
      },
      options: {
        skipHtmlTags: ['script','noscript','style','textarea','pre','code']
      }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  
  
</head>
<body>
  
  
  
  
  
<header class="site-header">
  
  <div class="container header-inner">
    
    <div class="brand">
      
        
          <a href="/" class="brand-avatar-link" aria-label="Linguista">
            <picture>
              <img src="/img/Avatar5518800_2.jpg" class="logo-avatar" alt="Linguista" width="32" height="32" loading="lazy" decoding="async" onerror="this.classList.add('avatar-fallback');" />
            </picture>
          </a>
        
      
      
      <button class="mobile-nav-btn mobile-nav-btn--header" aria-controls="navDrawer" aria-expanded="false" aria-label="打开导航">☰</button>
      <span class="brand-text">Linguista</span>
    </div>
    
    
    <nav class="nav">
      
      
      
        
          
            <a href="/labs/" class="nav-link ">工坊</a>
          
        
          
            <a href="/essays/" class="nav-link ">经典</a>
          
        
          
            <a href="/paul_graham/" class="nav-link ">Paul Graham</a>
          
        
      
      
    </nav>
    
    
    <div class="actions">
      
        
      
      <button id="searchToggleBtn" class="btn ghost search-toggle-btn" aria-label="打开搜索" title="搜索" type="button">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
          <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" />
          <line x1="16.65" y1="16.65" x2="21" y2="21" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
        </svg>
      </button>
      <div class="site-search" role="search">
        <button id="searchBackBtn" class="mobile-search-back" aria-label="返回" title="返回" type="button">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M15 18l-6-6 6-6" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
          </svg>
        </button>
        <span class="site-search-icon" aria-hidden="true">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2" />
            <line x1="16.65" y1="16.65" x2="21" y2="21" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
          </svg>
        </span>
        <input id="globalSearchInput"
               class="site-search-input"
               type="search"
               placeholder="搜索文章、标签、摘要…（按 / 快速聚焦）"
               aria-label="站内搜索"
               autocomplete="off"
               spellcheck="false" />
        <div id="globalSearchPopover" class="site-search-popover" hidden aria-live="polite"></div>
      </div>
      <a href="/post/" class="btn ghost posts-btn">Posts</a>
      <button id="themeToggle" class="btn ghost theme-toggle-btn" aria-label="切换主题" title="切换主题">🌙</button>
    </div>
    
    
  </div>

  
</header>


<aside id="navDrawer" class="nav-drawer" hidden>
  <div class="nav-drawer-inner">
    
    
    <div class="nav-drawer-header">
      <div class="nav-drawer-title">Menu</div>
      <button id="navCloseBtn" class="nav-drawer-close" aria-label="Close menu">×</button>
    </div>
    
      <nav class="nav-drawer-list" aria-label="Mobile Primary">
        
        
          <a class="nav-drawer-item" href="/labs/">工坊</a>
          
        
          <a class="nav-drawer-item" href="/essays/">经典</a>
          
        
          <a class="nav-drawer-item" href="/paul_graham/">Paul Graham</a>
          
        
      </nav>
    

    
    
    
    
      <div class="nav-drawer-folder">
        <div class="nav-drawer-subtitle">Section</div>
        <nav class="docs-nav in-drawer" aria-label="Current Section">
          






  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/lecture/">Lectures</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh/">中文讲义</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/guides/">Guides</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/insights/">Insights</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/blog/">Blog</a>
    
  




  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/agenticai/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link is-active" href="/courses/andrew-ng-agentic-ai/lecture-03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/overview/">吴恩达课程：Agentic AI</a>
    
  



        </nav>
      </div>
    
  </div>
</aside>

  
  
  
  <main class="container">
    



<nav aria-label="Breadcrumb" class="breadcrumbs" style="margin:14px 0 8px;">
  <button id="mobileNavBtn" class="mobile-nav-btn" aria-controls="navDrawer" aria-expanded="false" aria-label="打开导航">☰</button>
  
  
  
    
    
      
      
      
      
      <a href="/" class="crumb-anc">Linguista</a>
    
      <span class="crumb-sep">/</span>
      
      
      
      <a href="/courses/" class="crumb-anc">Courses</a>
    
    <span class="crumb-sep">/</span>
  
  
  
  
  
  <a href="/courses/andrew-ng-agentic-ai/" class="crumb-sec">Andrew Ng: Agentic AI</a>
  
  <span class="crumb-sep">/</span>
  <span class="crumb-current"></span>
</nav>



<div class="docs-layout">
  
  <aside class="docs-sidebar" aria-label="Sections">
    
    <div class="docs-side-title">
      Documentation
    </div>
    
    
    
    
    
    
    
    
    <nav class="docs-nav">
      
      
      
      






  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/lecture/">Lectures</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh/">中文讲义</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/guides/">Guides</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/insights/">Insights</a>
    
  
    
    
    
    
      
      <a class="depth-0 section-link " href="/courses/andrew-ng-agentic-ai/blog/">Blog</a>
    
  




  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/agenticai/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/blog-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/guide-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/insights-lec05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecnotes-zh-05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-01/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-02/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link is-active" href="/courses/andrew-ng-agentic-ai/lecture-03/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-04/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/lecture-05/"></a>
    
  
    
    
    
      
      <a class="depth-0 file-link " href="/courses/andrew-ng-agentic-ai/overview/">吴恩达课程：Agentic AI</a>
    
  



      
      
    </nav>
    
    
    
    
  </aside>

  
  <article class="docs-content">
    
    <div class="prose">
      
      
      
      
      
      

      
      
      
      
      
      

      
      <h1 id="andrew-ng-agentic-ai">Andrew Ng: Agentic AI</h1>
<h1 id="module-3-tool-use">Module 3: Tool Use</h1>
<h2 id="31-what-are-tools">3.1 What are tools?</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-gdscript3" data-lang="gdscript3"><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>In this module, you learn about <span style="color:#66d9ef">tool</span> use by LLMs, <span style="color:#f92672">and</span> that means letting your LLM decide when it might want to request to have a function called to take some action, <span style="color:#f92672">or</span> gather some information, <span style="color:#f92672">or</span> <span style="color:#66d9ef">do</span> something <span style="color:#66d9ef">else</span><span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>Just as we as humans can <span style="color:#66d9ef">do</span> a lot more with tools than we can with just our bare hands, LLMs too can also <span style="color:#66d9ef">do</span> a lot more with access to tools<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>But rather than using hammers <span style="color:#f92672">and</span> spanners <span style="color:#f92672">and</span> pliers, when we give tools, that is, functions, <span style="color:#66d9ef">for</span> an LLM to request a call, that<span style="color:#e6db74">&#39;s what lets it do a lot more.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">33</span>
</span></span><span style="display:flex;"><span>Let<span style="color:#e6db74">&#39;s take a look.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>If you were to ask an LLM that<span style="color:#e6db74">&#39;s been trained maybe many months ago, what time is it right now?</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>Well, that trained model does <span style="color:#f92672">not</span> know exactly what time it is, <span style="color:#f92672">and</span> so hopefully it responds, sorry, I <span style="color:#66d9ef">do</span> <span style="color:#f92672">not</span> have access to the current time<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">48</span>
</span></span><span style="display:flex;"><span>But <span style="color:#66d9ef">if</span> you were to write a function <span style="color:#f92672">and</span> give the LLM access to this function, then that lets it respond with a more useful answer<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">56</span>
</span></span><span style="display:flex;"><span>When we let LLMs call functions, <span style="color:#f92672">or</span> more precisely, let an LLM request to call functions, that<span style="color:#e6db74">&#39;s what we mean by tool use, and the tools are just functions that we provide to the LLM that it can request to call.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>In detail, this is how <span style="color:#66d9ef">tool</span> use works<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>In this example, I<span style="color:#e6db74">&#39;m going to give the getCurrentTime function that I showed on the previous slide to the LLM.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>When you then prompt it, what time is it, the LLM can decide to call the getCurrentTime function<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>That will <span style="color:#66d9ef">return</span> the current time, which is then fed back to the LLM <span style="color:#f92672">in</span> the conversational history, <span style="color:#f92672">and</span> finally the LLM can output is, say, <span style="color:#ae81ff">3.20</span>pm<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">33</span>
</span></span><span style="display:flex;"><span>So the sequence of steps is, there<span style="color:#e6db74">&#39;s the input prompt.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>The LLM, <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>, looks at the set of tools, which is just one <span style="color:#66d9ef">tool</span> <span style="color:#f92672">in</span> this example, but looks at the set of tools available, <span style="color:#f92672">and</span> it will decide <span style="color:#f92672">in</span> this example to call the <span style="color:#66d9ef">tool</span><span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">47</span>
</span></span><span style="display:flex;"><span>The <span style="color:#66d9ef">tool</span> is a function that then returns a value, that value is fed back to the LLM, <span style="color:#f92672">and</span> then finally the LLM generates its output<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span>Now, one important aspect of <span style="color:#66d9ef">tool</span> use is, we can leave it up to the LLM to decide whether <span style="color:#f92672">or</span> <span style="color:#f92672">not</span> to use any of the tools<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">03</span>
</span></span><span style="display:flex;"><span>So <span style="color:#66d9ef">for</span> the same setup, <span style="color:#66d9ef">if</span> I was asking it, how much caffeine is <span style="color:#f92672">in</span> green tea, the LLM doesn<span style="color:#e6db74">&#39;t need to know the current time to answer this, and so it can generate an answer directly,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>green tea typically has this much caffeine, <span style="color:#f92672">and</span> it does so without invoking the getCurrentTime function<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>In my slides, I<span style="color:#e6db74">&#39;m going to use this notation with this dashed box on top of the LLM to indicate that we&#39;</span>re providing a set of tools to the LLM <span style="color:#66d9ef">for</span> the LLM to choose to use when it deems appropriate<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>This is as opposed to some examples you saw <span style="color:#f92672">in</span> earlier videos, where I, as a developer, had hard<span style="color:#f92672">-</span>coded <span style="color:#f92672">in</span>, <span style="color:#66d9ef">for</span> example, that I will always <span style="color:#66d9ef">do</span> a web search at this point <span style="color:#f92672">in</span> the research agent<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>In contrast, the getCurrentTime function call is <span style="color:#f92672">not</span> hard<span style="color:#f92672">-</span>coded <span style="color:#f92672">in</span>, it<span style="color:#e6db74">&#39;s up to the LLM to decide whether or not it wants to request a call to the getCurrentTime function.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">51</span>
</span></span><span style="display:flex;"><span>And again, we<span style="color:#e6db74">&#39;re going to use this dashed box notation to indicate when we&#39;</span>re giving one <span style="color:#f92672">or</span> more tools to the LLM <span style="color:#66d9ef">for</span> the LLM to decide what tools, <span style="color:#66d9ef">if</span> any, it wants to call<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">02</span>
</span></span><span style="display:flex;"><span>Here are some more examples of when <span style="color:#66d9ef">tool</span> use may help an LLM<span style="color:#f92672">-</span>based app generate better answers<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>If you were to ask it, can you find some Italian restaurants near Mountain View, California<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>If it has a web search <span style="color:#66d9ef">tool</span>, then an LLM might elect to call a web search engine <span style="color:#66d9ef">for</span> a query, restaurants near Mountain View, California, <span style="color:#f92672">and</span> use the results that fetches to generate the output<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>Or <span style="color:#66d9ef">if</span> you are running a retail store <span style="color:#f92672">and</span> you want to be able to answer questions like, show me customers who bought white sunglasses, <span style="color:#66d9ef">if</span> your LLM is given access to a query database <span style="color:#66d9ef">tool</span>, then it might look up the table of sales <span style="color:#66d9ef">for</span> what entries had a pair of white sunglasses sold <span style="color:#f92672">and</span> then use that to then generate the output<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">44</span>
</span></span><span style="display:flex;"><span>Finally, <span style="color:#66d9ef">if</span> you wanted to <span style="color:#66d9ef">do</span> an interest rate calculation, <span style="color:#66d9ef">if</span> I were to deposit <span style="color:#f92672">$</span><span style="color:#ae81ff">500</span> <span style="color:#f92672">and</span> after <span style="color:#ae81ff">10</span> years, an interest rate of <span style="color:#ae81ff">5</span><span style="color:#f92672">%</span>, what would I have<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span>If you happen to have an interest calculation <span style="color:#66d9ef">tool</span>, then it could invoke the interest calculation function to calculate that<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">01</span>
</span></span><span style="display:flex;"><span>Or it turns out, one thing you see later is letting an LLM write code, like just write a mathematical expression like this, <span style="color:#f92672">and</span> then to evaluate it, that would be another way to let an LLM calculate the right answer<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>So as a developer, it<span style="color:#e6db74">&#39;ll be up to you to think through what are the sorts of things you want an application to really do, and then to create the functions or the tools that are needed to make them available to the LLM to let it use the appropriate tools to complete the sorts of tasks that maybe a restaurant recommender or a retail question answer or a finance assistant may want to do.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">39</span>
</span></span><span style="display:flex;"><span>So depending on your application, you may have to implement <span style="color:#f92672">and</span> make different tools available to your LLM<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>So far, most of the examples we<span style="color:#e6db74">&#39;ve gone through made only one tool or one function available to the LLM.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">52</span>
</span></span><span style="display:flex;"><span>But there are many use cases where you want to make multiple tools <span style="color:#f92672">or</span> multiple functions available <span style="color:#66d9ef">for</span> the LLM <span style="color:#66d9ef">for</span> it to choose which of any to call<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">59</span>
</span></span><span style="display:flex;"><span>For example, <span style="color:#66d9ef">if</span> you<span style="color:#e6db74">&#39;re building a calendar assistant agent, you might then want it to be able to fulfill requests like, please find a free slot on Thursday in my calendar and make an appointment with Alice.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>So <span style="color:#f92672">in</span> this example, we might make available to the LLM a <span style="color:#66d9ef">tool</span> <span style="color:#f92672">or</span> a function to make an appointment, that is, to send a calendar invite, to check the calendar to see when I might be free, as well as to delete the appointment <span style="color:#66d9ef">if</span> it ever wants to cancel an existing calendar entry<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">26</span>
</span></span><span style="display:flex;"><span>And so given the set of instructions, the LLM would first decide that of the different tools available, probably the first one it should use is check calendar<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>So call a check calendar function that will <span style="color:#66d9ef">return</span> when I am free on Thursday<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>Based on that information, which is fed back to the LLM, it can then decide that the next step is to pick a slot, let<span style="color:#e6db74">&#39;s say 3 p.m., and then to call the make appointment function to send a calendar invite to Alice, as well as to add it to my calendar.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">56</span>
</span></span><span style="display:flex;"><span>The output of that, which hopefully is a confirmation that the calendar entry was sent out successfully, is fed back to the LLM, <span style="color:#f92672">and</span> then lastly, the LLM might tell me your appointment is set up with Alice at <span style="color:#ae81ff">3</span> p<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span> Thursday<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">6</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>Being able to give your LLM access to tools is a pretty big deal<span style="color:#f92672">.</span> It will make your applications much more powerful<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">6</span>:<span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>In the next video, we<span style="color:#e6db74">&#39;ll take a look at how to write functions, how to create tools to then make them available to your LLM. Let&#39;</span>s go on to the next video<span style="color:#f92672">.</span>
</span></span></code></pre></div><h2 id="32-creating-a-tool">3.2 Creating a tool</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-gdscript3" data-lang="gdscript3"><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">01</span>
</span></span><span style="display:flex;"><span>The process of how an LLM decides to call a function maybe seems a little bit mysterious
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>initially because an LLM is just trained to generate output text <span style="color:#f92672">or</span> output text tokens<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>So how does that work<span style="color:#960050;background-color:#1e0010">?</span> In this video, I<span style="color:#e6db74">&#39;d like to walk through with you step-by-step</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>what the process of getting an LLM to be able to get a function called really looks like<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">21</span>
</span></span><span style="display:flex;"><span>Let<span style="color:#e6db74">&#39;s take a look.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">22</span>
</span></span><span style="display:flex;"><span>So tools are just codes <span style="color:#f92672">or</span> functions that an LLM can request to be executed,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">27</span>
</span></span><span style="display:flex;"><span>like this getCurrentTime function that we saw from the previous video<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>Now, today<span style="color:#e6db74">&#39;s leading LLMs are all trained directly to use tools,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>but I want to walk through with you what it would look like <span style="color:#66d9ef">if</span> you had to write prompts yourself
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>to tell it when to use tools, <span style="color:#f92672">and</span> this is what we had to <span style="color:#66d9ef">do</span> <span style="color:#f92672">in</span> an earlier era
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>before LLMs were trained directly to use tools<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">49</span>
</span></span><span style="display:flex;"><span>And even though we don<span style="color:#e6db74">&#39;t do it exactly this way anymore,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">51</span>
</span></span><span style="display:flex;"><span>this will hopefully give you a better understanding of the process,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> we<span style="color:#e6db74">&#39;ll walk through the more modern syntax in the next video.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span>If you<span style="color:#e6db74">&#39;ve implemented this function to getCurrentTime,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>then <span style="color:#f92672">in</span> order to give this <span style="color:#66d9ef">tool</span> to the LLM, you might write a prompt like this<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>You may tell it, LLM, you have access to a <span style="color:#66d9ef">tool</span> called getCurrentTime<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">09</span>
</span></span><span style="display:flex;"><span>To use it, I want you to print out the following text<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>Print out all caps function <span style="color:#f92672">and</span> then print out getCurrentTime<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>And <span style="color:#66d9ef">if</span> I ever see this text, all caps function <span style="color:#f92672">and</span> then getCurrentTime,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>that<span style="color:#e6db74">&#39;s when I know you want me to call the getCurrentTime function for you.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>When a user asks, what time is it<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>The LLM will then realize it needs to call <span style="color:#f92672">or</span> request to get called the getCurrentTime function<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">29</span>
</span></span><span style="display:flex;"><span>And so the LLM will then output what it was told<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>It<span style="color:#e6db74">&#39;ll output all caps function: getCurrentTime.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>Now, I then have to have written code to look at the output of the LLM
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>to see <span style="color:#66d9ef">if</span> there is this all caps function<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>And <span style="color:#66d9ef">if</span> so, then I need to pull out the argument of this getCurrentTime
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">47</span>
</span></span><span style="display:flex;"><span>to figure out what function the LLM wants to call<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">49</span>
</span></span><span style="display:flex;"><span>And then I need to write code to actually call the getCurrentTime function
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then pull out the output, which is, let<span style="color:#e6db74">&#39;s say, 8 a.m.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span>And then it is the developer written code, my code,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>that has to take <span style="color:#ae81ff">8</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span> <span style="color:#f92672">and</span> feed that time, <span style="color:#ae81ff">8</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span>,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>back into the LLM as part of this conversational history<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">07</span>
</span></span><span style="display:flex;"><span>And the conversational history, of course, includes the initial user prompt,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>the fact that the request is a function call, <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>And lastly, the LLM, knowing what had happened earlier,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>that the user asks a question, requests a function call,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then also that I call the function <span style="color:#f92672">and</span> <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">8</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>Finally, the LLM can look at all this <span style="color:#f92672">and</span> generate the final response,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">26</span>
</span></span><span style="display:flex;"><span>which is, it is <span style="color:#ae81ff">8</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>So to be clear, <span style="color:#f92672">in</span> order to call a function,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>the LLM doesn<span style="color:#e6db74">&#39;t call the function directly.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>It instead outputs something <span style="color:#f92672">in</span> a specific format like this
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">38</span>
</span></span><span style="display:flex;"><span>that tells me that I need to call the function <span style="color:#66d9ef">for</span> the LLM
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then tell the LLM what was the output of the function I requested<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">45</span>
</span></span><span style="display:flex;"><span>In this example, we had given the LLM only a single function,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">49</span>
</span></span><span style="display:flex;"><span>but you can imagine <span style="color:#66d9ef">if</span> we gave it three <span style="color:#f92672">or</span> four functions,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">52</span>
</span></span><span style="display:flex;"><span>we could tell it to output functions <span style="color:#f92672">in</span> all caps,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span>then the name of the function it wants called,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> maybe even some arguments of these functions<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>In fact, now let<span style="color:#e6db74">&#39;s take a look at a slightly more complex example</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">03</span>
</span></span><span style="display:flex;"><span>where the getCurrentTime function accepts an argument <span style="color:#66d9ef">for</span> the time zone
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>at which you want the current time<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>For this second example, I<span style="color:#e6db74">&#39;ve written a function</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>that gets the current time <span style="color:#f92672">in</span> a specified time zone,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>where here the time zone is the input argument
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>to the getCurrentTime function<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">22</span>
</span></span><span style="display:flex;"><span>So to let the LLM use this <span style="color:#66d9ef">tool</span> to answer questions
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>like maybe, what time is it <span style="color:#f92672">in</span> New Zealand<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">27</span>
</span></span><span style="display:flex;"><span>Because my answer is there, so before I call her up,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">29</span>
</span></span><span style="display:flex;"><span>I <span style="color:#66d9ef">do</span> look up what time it is <span style="color:#f92672">in</span> New Zealand<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>To let the LLM use this <span style="color:#66d9ef">tool</span>, you might modify the system prompt
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>to say you can use the getCurrentTime <span style="color:#66d9ef">tool</span> <span style="color:#66d9ef">for</span> a specific time zone<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">39</span>
</span></span><span style="display:flex;"><span>To use it, I<span style="color:#e6db74">&#39;ll put the following, getCurrentTime,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then, you know, include the time zone<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">43</span>
</span></span><span style="display:flex;"><span>And this is an abbreviated prompt<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">45</span>
</span></span><span style="display:flex;"><span>In practice, you might put more details than this into the prompt
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">48</span>
</span></span><span style="display:flex;"><span>to tell it what is the function, how to use it, <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>In this example, the LLM will then realize
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span>it needs to fetch the time <span style="color:#f92672">in</span> New Zealand,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">56</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> so it will generate output like this,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">58</span>
</span></span><span style="display:flex;"><span>function: getCurrentTime Pacific<span style="color:#f92672">/</span>Auckland<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">02</span>
</span></span><span style="display:flex;"><span>This is the New Zealand time zone
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>because Auckland is a major city <span style="color:#f92672">in</span> New Zealand<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">06</span>
</span></span><span style="display:flex;"><span>Then I have to write code to search <span style="color:#66d9ef">for</span> whether <span style="color:#f92672">or</span> <span style="color:#f92672">not</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>this function all caps appeared <span style="color:#f92672">in</span> the LLM output,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> <span style="color:#66d9ef">if</span> so, then I need to pull out the function to call<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>Lastly, I will then call getCurrentTime
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>with the specified arguments, which is generated by the LLM,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">22</span>
</span></span><span style="display:flex;"><span>which is Pacific<span style="color:#f92672">/</span>Auckland, <span style="color:#f92672">and</span> maybe returns is <span style="color:#ae81ff">4</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>Then as usual, I feed this to the LLM <span style="color:#f92672">and</span> the LLM outputs<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>It is <span style="color:#ae81ff">4</span> a<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span> <span style="color:#f92672">in</span> New Zealand<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>To summarize, here<span style="color:#e6db74">&#39;s the process for getting LLM to use tools.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>First, you have to provide the <span style="color:#66d9ef">tool</span> to the LLM,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">37</span>
</span></span><span style="display:flex;"><span>implement the function, <span style="color:#f92672">and</span> then tell the LLM that it is available<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>When the LLM decides to call a <span style="color:#66d9ef">tool</span>,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>it then generates a specific output that lets you know
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">45</span>
</span></span><span style="display:flex;"><span>that you need to call the function <span style="color:#66d9ef">for</span> the LLM<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">48</span>
</span></span><span style="display:flex;"><span>Then you call the function, get its output,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">51</span>
</span></span><span style="display:flex;"><span>take the output of the function you just called,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> give that output back to the LLM,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> the LLM then uses that to go on to whatever it decides to <span style="color:#66d9ef">do</span> next,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">59</span>
</span></span><span style="display:flex;"><span>which <span style="color:#f92672">in</span> our examples <span style="color:#f92672">in</span> this video was to just generate the final output,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">03</span>
</span></span><span style="display:flex;"><span>but sometimes it may even decide that the next step
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>is to go call yet another <span style="color:#66d9ef">tool</span>, <span style="color:#f92672">and</span> the process continues<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">09</span>
</span></span><span style="display:flex;"><span>Now, it turns out that this all<span style="color:#f92672">-</span>caps function syntax is a little bit clunky<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>This is what we used to <span style="color:#66d9ef">do</span> before LLMs were trained natively
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">or</span> to know by themselves how to request that tools be called<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">21</span>
</span></span><span style="display:flex;"><span>With modern LLMs, you don<span style="color:#e6db74">&#39;t need to tell it to output all-caps function,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>then search <span style="color:#66d9ef">for</span> all<span style="color:#f92672">-</span>caps function, <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">27</span>
</span></span><span style="display:flex;"><span>Instead, LLMs are trained to use a specific syntax
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>to request very clearly when it wants a <span style="color:#66d9ef">tool</span> called<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>In the next video, I want to share with you
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>what the modern syntax actually looks like
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">38</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> letting LLMs request to have tools be called<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>Let<span style="color:#e6db74">&#39;s go on to the next video.</span>
</span></span></code></pre></div><h2 id="33-tool-syntax">3.3 Tool syntax</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-gdscript3" data-lang="gdscript3"><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">01</span>
</span></span><span style="display:flex;"><span>Let<span style="color:#e6db74">&#39;s take a look at how to write code to have your LLM get tools called.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>Here<span style="color:#e6db74">&#39;s our old getCurrentTime function without the time zone argument.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">09</span>
</span></span><span style="display:flex;"><span>Let me show you how to use the AI Suite open source library <span style="color:#f92672">in</span> order to have your LLM call
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>tools<span style="color:#f92672">.</span> By the way, technically, as you saw from the last video, the LLM doesn<span style="color:#e6db74">&#39;t call the tool.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>The LLM just requests that you call the <span style="color:#66d9ef">tool</span><span style="color:#f92672">.</span> But among developers building agentic workflows,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>many of us will occasionally just say the LLM calls the <span style="color:#66d9ef">tool</span>, even though it<span style="color:#e6db74">&#39;s not technically</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>what happens, but because it<span style="color:#e6db74">&#39;s just a shorter way to say it.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>This syntax here is very similar to the OpenAI syntax <span style="color:#66d9ef">for</span> calling these LLMs, except that here,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>I<span style="color:#e6db74">&#39;m using the AI Suite library, which is an open source package that some friends and I had worked</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>on that makes it easy to call multiple LLM providers<span style="color:#f92672">.</span> So the code syntax, <span style="color:#f92672">and</span> <span style="color:#66d9ef">if</span> this
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span>looks like a lot to you, don<span style="color:#e6db74">&#39;t worry about it. You&#39;</span>ll see more of this <span style="color:#f92672">in</span> the code labs<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span>But very briefly, this is very similar to the OpenAI syntax, where you say response equals
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">02</span>
</span></span><span style="display:flex;"><span>client check, completions create, then select the model, which <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>, we<span style="color:#e6db74">&#39;ll use the</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">07</span>
</span></span><span style="display:flex;"><span>OpenAI model GPT<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>o, messages equals messages, assuming you<span style="color:#e6db74">&#39;ve put into an array here the</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>messages you want to <span style="color:#66d9ef">pass</span> the LLM, <span style="color:#f92672">and</span> it will say tools equals, then a list of the tools you want
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>the LLM to have access to<span style="color:#f92672">.</span> And <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>, there<span style="color:#e6db74">&#39;s just one tool, which is get current time,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then don<span style="color:#e6db74">&#39;t worry too much about the max turns parameter. This is included because</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>after a <span style="color:#66d9ef">tool</span> call returns, the LLM might decide to call another <span style="color:#66d9ef">tool</span>, <span style="color:#f92672">and</span> after that <span style="color:#66d9ef">tool</span> call
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">33</span>
</span></span><span style="display:flex;"><span>returns, the LLM might decide to call yet another <span style="color:#66d9ef">tool</span><span style="color:#f92672">.</span> So max turns is just a ceiling on how many
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">38</span>
</span></span><span style="display:flex;"><span>times you want the LLM to request one <span style="color:#66d9ef">tool</span> after another before you stop to just <span style="color:#66d9ef">break</span> out of a
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">44</span>
</span></span><span style="display:flex;"><span>possible infinite loop<span style="color:#f92672">.</span> In practice, you almost never hit this limit unless your code is doing
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">49</span>
</span></span><span style="display:flex;"><span>something unusually ambitious<span style="color:#f92672">.</span> So I wouldn<span style="color:#e6db74">&#39;t worry about the max turns parameter. I usually just set it to</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span>five, but <span style="color:#f92672">in</span> practice, it doesn<span style="color:#e6db74">&#39;t matter that much. And it turns out that with AISuite, the function</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>get current time is automatically described to the LLM <span style="color:#f92672">in</span> an appropriate way to enable the LLM to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">06</span>
</span></span><span style="display:flex;"><span>know when to call it<span style="color:#f92672">.</span> So rather than you needing to manually write a long prompt to tell the LLM,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>once get current time, this syntax <span style="color:#f92672">in</span> AISuite does that automatically<span style="color:#f92672">.</span> And to make it seem <span style="color:#f92672">not</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>too mysterious, the way it does that, it actually looks at the dot string associated with get current
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>time with this comments <span style="color:#f92672">in</span> get current time <span style="color:#f92672">in</span> order to figure out how to describe this function
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>to the LLM<span style="color:#f92672">.</span> So to illustrate how this works, here<span style="color:#e6db74">&#39;s the function again, and here&#39;</span>s the
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>snippet of code using AISuite to call the LLM<span style="color:#f92672">.</span> Behind the scenes, what this will <span style="color:#66d9ef">do</span> is create a
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">43</span>
</span></span><span style="display:flex;"><span>JSON schema that describes the function <span style="color:#f92672">in</span> detail<span style="color:#f92672">.</span> And this over here on the right is what is actually
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>passed to the LLM<span style="color:#f92672">.</span> And specifically, it will pull the name of the function, which is get current time,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> then also a description of the function, which is pulled out from the doc string to tell the LLM
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">01</span>
</span></span><span style="display:flex;"><span>what this function does, which lets it decide when to call it<span style="color:#f92672">.</span> There<span style="color:#e6db74">&#39;s some APIs which require that</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">06</span>
</span></span><span style="display:flex;"><span>you manually construct this JSON schema <span style="color:#f92672">and</span> then <span style="color:#66d9ef">pass</span> this JSON schema to the LLM, but the AISuite
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>package does this automatically <span style="color:#66d9ef">for</span> you<span style="color:#f92672">.</span> To go through a slightly more complex example, <span style="color:#66d9ef">if</span> you
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>have this more complex get current time <span style="color:#66d9ef">tool</span> that also has an input time zone parameter, then AISuite
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>will create this more complex JSON schema where, as before, it pulls out the name of the function,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>which is get current time, pulls out the description from the doc string, <span style="color:#f92672">and</span> then also identifies
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>what are the parameters <span style="color:#f92672">and</span> describes them to the LLM based on the documentation here shown on the
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>left, so that when it<span style="color:#e6db74">&#39;s generating the function arguments to call the tool, it knows that it</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">47</span>
</span></span><span style="display:flex;"><span>should be something like America<span style="color:#f92672">/</span>New York <span style="color:#f92672">or</span> Pacific<span style="color:#f92672">/</span>Auckland <span style="color:#f92672">or</span> some other time zone<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">53</span>
</span></span><span style="display:flex;"><span>And so <span style="color:#66d9ef">if</span> you execute this code snippet here on the lower left, it will use the OpenAI
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">59</span>
</span></span><span style="display:flex;"><span>GPT<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>o model, see <span style="color:#66d9ef">if</span> the LLM wants the function called, <span style="color:#f92672">and</span> <span style="color:#66d9ef">if</span> so, it<span style="color:#e6db74">&#39;ll call the function,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>get the output from the function, feed that back to the LLM, <span style="color:#f92672">and</span> <span style="color:#66d9ef">do</span> that up to a maximum of five
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>turns <span style="color:#f92672">and</span> then <span style="color:#66d9ef">return</span> the response<span style="color:#f92672">.</span> Note that <span style="color:#66d9ef">if</span> the LLM requests to call the get current time
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>function, AISuite <span style="color:#f92672">or</span> this client, it will call the get current time <span style="color:#66d9ef">for</span> you, so you don<span style="color:#e6db74">&#39;t need</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>to explicitly <span style="color:#66d9ef">do</span> it yourself<span style="color:#f92672">.</span> All that is done <span style="color:#f92672">in</span> this single function call that you have to write<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">29</span>
</span></span><span style="display:flex;"><span>Just note that there are some other implementations of LLM interfaces where you have to <span style="color:#66d9ef">do</span> that step
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>manually, but with this particular package, this is all wrapped into this client chat completions
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>create function call<span style="color:#f92672">.</span> So you now know how to get an LLM to call functions, <span style="color:#f92672">and</span> I hope that you enjoy
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">49</span>
</span></span><span style="display:flex;"><span>playing with this <span style="color:#f92672">in</span> the labs, <span style="color:#f92672">and</span> it<span style="color:#e6db74">&#39;s actually really amazing when you provide a few functions</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span>to LLM <span style="color:#f92672">and</span> LLM decides to go <span style="color:#f92672">and</span> take action <span style="color:#f92672">in</span> the world, go <span style="color:#f92672">and</span> get more information
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">59</span>
</span></span><span style="display:flex;"><span>to fulfill your requests<span style="color:#f92672">.</span> If you haven<span style="color:#e6db74">&#39;t played with this before, I think you&#39;</span>ll find this to be
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>really cool<span style="color:#f92672">.</span> It turns out that of all the tools you can give an LLM, there<span style="color:#e6db74">&#39;s one that&#39;</span>s a bit special,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>which is a code execution <span style="color:#66d9ef">tool</span><span style="color:#f92672">.</span> It turns out to be really powerful<span style="color:#f92672">.</span> If you can tell an LLM,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>you can write code, <span style="color:#f92672">and</span> I will have a <span style="color:#66d9ef">tool</span> to execute that code <span style="color:#66d9ef">for</span> you, because code can <span style="color:#66d9ef">do</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">21</span>
</span></span><span style="display:flex;"><span>a lot of things, <span style="color:#f92672">and</span> we give an LLM the flexibility to write code <span style="color:#f92672">and</span> have code executed<span style="color:#f92672">.</span> That turns
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>out to be an incredibly powerful <span style="color:#66d9ef">tool</span> to give to LLMs<span style="color:#f92672">.</span> So code execution is special<span style="color:#f92672">.</span> Let<span style="color:#e6db74">&#39;s go</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>on to the next video to talk about the code execution <span style="color:#66d9ef">tool</span> <span style="color:#66d9ef">for</span> LLMs<span style="color:#f92672">.</span>
</span></span></code></pre></div><h2 id="34-code-execution">3.4 Code execution</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-gdscript3" data-lang="gdscript3"><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>In a few agentic applications I<span style="color:#e6db74">&#39;ve worked on, I gave the LLM the option to write code to then</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">06</span>
</span></span><span style="display:flex;"><span>carry out the task I wanted it to<span style="color:#f92672">.</span> And I<span style="color:#e6db74">&#39;ve been a few times now, I&#39;</span>ve been really surprised <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>delighted by the cleverness of the code solutions it generated <span style="color:#f92672">in</span> order to solve various tasks <span style="color:#66d9ef">for</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>me<span style="color:#f92672">.</span> So <span style="color:#66d9ef">if</span> you haven<span style="color:#e6db74">&#39;t used code execution much, I think you might be surprised and delighted at</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>what this will let your LLM applications <span style="color:#66d9ef">do</span><span style="color:#f92672">.</span> Let<span style="color:#e6db74">&#39;s take a look. Let&#39;</span>s take an example of
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">29</span>
</span></span><span style="display:flex;"><span>building an application that can input math word problems <span style="color:#f92672">and</span> solve them <span style="color:#66d9ef">for</span> you<span style="color:#f92672">.</span> So you might
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>create tools that add numbers, subtract numbers, multiply numbers, <span style="color:#f92672">and</span> divide numbers<span style="color:#f92672">.</span> And <span style="color:#66d9ef">if</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>someone says, please add <span style="color:#ae81ff">13.2</span> plus <span style="color:#ae81ff">18.9</span>, then it triggers the add <span style="color:#66d9ef">tool</span> <span style="color:#f92672">and</span> then it gets you the
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>right answer<span style="color:#f92672">.</span> But what <span style="color:#66d9ef">if</span> someone now types <span style="color:#f92672">in</span>, what is the square root of two<span style="color:#960050;background-color:#1e0010">?</span> Well, one thing
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>you could <span style="color:#66d9ef">do</span> is write a new <span style="color:#66d9ef">tool</span> <span style="color:#66d9ef">for</span> a square root, but then maybe some new thing is needed to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">56</span>
</span></span><span style="display:flex;"><span>carry out exponentiation<span style="color:#f92672">.</span> And <span style="color:#f92672">in</span> fact, <span style="color:#66d9ef">if</span> you look at the number of buttons on your modern
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">02</span>
</span></span><span style="display:flex;"><span>scientific calculator, are you going to create a separate <span style="color:#66d9ef">tool</span> <span style="color:#66d9ef">for</span> every one of these buttons <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">06</span>
</span></span><span style="display:flex;"><span>the many more things that we would want to <span style="color:#66d9ef">do</span> <span style="color:#f92672">in</span> math calculation<span style="color:#960050;background-color:#1e0010">?</span> So instead of trying to implement
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>one <span style="color:#66d9ef">tool</span> after another, a different approach is to let it write <span style="color:#f92672">and</span> execute code<span style="color:#f92672">.</span> To tell the LLM
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>to write code, you might write a prompt like this<span style="color:#f92672">.</span> Write code to solve the user<span style="color:#e6db74">&#39;s query. Return your</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span>answer as Python code delimited with execute Python <span style="color:#f92672">and</span> closing execute Python tags<span style="color:#f92672">.</span> So given a query
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>like what is the square root of two, the LLM might generate outputs like this<span style="color:#f92672">.</span> You can then use
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">37</span>
</span></span><span style="display:flex;"><span>pattern matching, <span style="color:#66d9ef">for</span> example, a regular expression to look <span style="color:#66d9ef">for</span> the start <span style="color:#f92672">and</span> end execute Python tags
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">44</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> extract the code <span style="color:#f92672">in</span> between<span style="color:#f92672">.</span> So here you get these two lines of code shown <span style="color:#f92672">in</span> the green box,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> you can then execute this code <span style="color:#66d9ef">for</span> the LLM <span style="color:#f92672">and</span> get the output, <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>, <span style="color:#ae81ff">1.4142</span> <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span>Lastly, this numerical answer is then passed back to the LLM <span style="color:#f92672">and</span> it can write a nicely formatted
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>answer to the original question<span style="color:#f92672">.</span> There are a few different ways you can carry out the code
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>execution step <span style="color:#66d9ef">for</span> the LLM<span style="color:#f92672">.</span> One is to use Python<span style="color:#e6db74">&#39;s exec function. This is a built-in Python function</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">15</span>
</span></span><span style="display:flex;"><span>which will execute whatever code you <span style="color:#66d9ef">pass</span> <span style="color:#f92672">in</span><span style="color:#f92672">.</span> And this is very powerful <span style="color:#66d9ef">for</span> your LLM to really
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">21</span>
</span></span><span style="display:flex;"><span>write code <span style="color:#f92672">and</span> get you to execute that code, although there are some security implications
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">26</span>
</span></span><span style="display:flex;"><span>which we<span style="color:#e6db74">&#39;ll see later in this video. And then there are also some tools that will let you run the code</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">in</span> a safer sandbox environment<span style="color:#f92672">.</span> And of course, square root of two is a relatively simple example<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">38</span>
</span></span><span style="display:flex;"><span>An LLM can also accurately write code to, <span style="color:#66d9ef">for</span> example, <span style="color:#66d9ef">do</span> interest calculations <span style="color:#f92672">and</span> solve much harder
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">45</span>
</span></span><span style="display:flex;"><span>math calculations than this<span style="color:#f92672">.</span> One refinement to this idea, which you sort of saw <span style="color:#f92672">in</span> our section
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">52</span>
</span></span><span style="display:flex;"><span>on reflection, is that <span style="color:#66d9ef">if</span> code execution fails, so <span style="color:#66d9ef">if</span> <span style="color:#66d9ef">for</span> some reason the LLM had generated code
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">58</span>
</span></span><span style="display:flex;"><span>that wasn<span style="color:#e6db74">&#39;t quite correct, then passing that error message back to the LLM to let it reflect and</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>maybe revise this code <span style="color:#f92672">and</span> try another one <span style="color:#f92672">or</span> two times<span style="color:#f92672">.</span> That can sometimes also allow it to get a
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>more accurate answer<span style="color:#f92672">.</span> Now, running arbitrary code that an LLM generates does have a small chance of
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>causing something bad to happen<span style="color:#f92672">.</span> Recently, one of my team members was using a highly agentic coder
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">and</span> it actually chose to remove star<span style="color:#f92672">.</span>py within a project directory<span style="color:#f92672">.</span> So this is actually a real
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>example<span style="color:#f92672">.</span> And eventually that agentic coder did apologize<span style="color:#f92672">.</span> It said, yes, that<span style="color:#e6db74">&#39;s actually right,</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>that was an incredibly stupid mistake<span style="color:#f92672">.</span> I guess I was glad that this agentic coder was really sorry,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>but I already deleted a bunch of Python files<span style="color:#f92672">.</span> Unfortunately, the team member had it backed
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">44</span>
</span></span><span style="display:flex;"><span>up on GitHub repo, so there was no real harm done, but it would have been <span style="color:#f92672">not</span> great <span style="color:#66d9ef">if</span> this
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>arbitrary code, which made the mistake of deleting a bunch of files, had been executed
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span>without the backup<span style="color:#f92672">.</span> So the best practice <span style="color:#66d9ef">for</span> code execution is to run it inside a sandbox
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>environment<span style="color:#f92672">.</span> In practice, the risk <span style="color:#66d9ef">for</span> any single line of code is <span style="color:#f92672">not</span> that high<span style="color:#f92672">.</span> So <span style="color:#66d9ef">if</span> I<span style="color:#e6db74">&#39;m being</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">07</span>
</span></span><span style="display:flex;"><span>candid, many developers will execute code from the LLM without too much checking<span style="color:#f92672">.</span> But <span style="color:#66d9ef">if</span> you want to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>be a bit safer, then the best practice is to create a sandbox so that <span style="color:#66d9ef">if</span> an LLM generates bad
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">19</span>
</span></span><span style="display:flex;"><span>code, there<span style="color:#e6db74">&#39;s a lower risk of data loss or leakage of sensitive data and so on. So sandbox</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">26</span>
</span></span><span style="display:flex;"><span>environments like Docker <span style="color:#f92672">or</span> E2B as a lightweight sandbox environment can reduce the risk of
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">33</span>
</span></span><span style="display:flex;"><span>arbitrary codes being executed <span style="color:#f92672">in</span> a way that damages your system <span style="color:#f92672">or</span> your environment<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">39</span>
</span></span><span style="display:flex;"><span>It turns out that code execution is so important that a lot of trainers of LLMs actually <span style="color:#66d9ef">do</span> special
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>work to make sure that code execution works well on their applications<span style="color:#f92672">.</span> But I hope that as you add
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">52</span>
</span></span><span style="display:flex;"><span>this as one more <span style="color:#66d9ef">tool</span> <span style="color:#66d9ef">for</span> you to potentially offer to LLMs <span style="color:#f92672">or</span> let you make your applications
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">58</span>
</span></span><span style="display:flex;"><span>much more powerful<span style="color:#f92672">.</span> So far <span style="color:#f92672">and</span> what we<span style="color:#e6db74">&#39;ve discussed, you have to create tools and make them</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>available one at a time to your LLM<span style="color:#f92672">.</span> It turns out that many different teams are building similar
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>tools <span style="color:#f92672">and</span> having to <span style="color:#66d9ef">do</span> all this work of building functions <span style="color:#f92672">and</span> making them available to the OMs<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>But there is recently a new standard called MCP, Model Context Protocol, that<span style="color:#e6db74">&#39;s making it much</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span>easier <span style="color:#66d9ef">for</span> developers to get access to a huge set of tools <span style="color:#66d9ef">for</span> LLMs to use<span style="color:#f92672">.</span> This is an important
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>protocol that more <span style="color:#f92672">and</span> more teams are using to develop LLM based applications<span style="color:#f92672">.</span> Let<span style="color:#e6db74">&#39;s go learn</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">37</span>
</span></span><span style="display:flex;"><span>about MCP <span style="color:#f92672">in</span> the next video<span style="color:#f92672">.</span>
</span></span></code></pre></div><h2 id="35-mcp">3.5 MCP</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-gdscript3" data-lang="gdscript3"><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>MCP, the Model Context Protocol, was a standard proposed by Anthropic but now adopted by many
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">07</span>
</span></span><span style="display:flex;"><span>other companies <span style="color:#f92672">and</span> by many developers as a way to give an LLM access to more context <span style="color:#f92672">and</span> to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>more tools<span style="color:#f92672">.</span> There are a lot of developers developing around the MCP ecosystem <span style="color:#f92672">and</span> so
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">18</span>
</span></span><span style="display:flex;"><span>learning about this will give you a lot more access to resources <span style="color:#66d9ef">for</span> your applications<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span>Let<span style="color:#e6db74">&#39;s take a look. This is the pain points that MCP attempts to solve. If one developer is writing</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">31</span>
</span></span><span style="display:flex;"><span>an application that wants to integrate with data from Slack <span style="color:#f92672">and</span> Google Drive <span style="color:#f92672">and</span> GitHub <span style="color:#f92672">or</span> access
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">37</span>
</span></span><span style="display:flex;"><span>data from a Postgres database, then they might have to write code to wrap around Slack APIs
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>to have functions to provide to the application, write code to wrap around Google Drive APIs to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">47</span>
</span></span><span style="display:flex;"><span>parse the application, <span style="color:#f92672">and</span> similarly <span style="color:#66d9ef">for</span> these other tools <span style="color:#f92672">or</span> data sources<span style="color:#f92672">.</span> Then what has been
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span>happening <span style="color:#f92672">in</span> the developer community is <span style="color:#66d9ef">if</span> a different team is building a different application,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">59</span>
</span></span><span style="display:flex;"><span>then they too will integrate by themselves with Slack <span style="color:#f92672">and</span> Google Drive <span style="color:#f92672">and</span> GitHub <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>So many developers were all building custom wrappers around these types of data sources<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>And so <span style="color:#66d9ef">if</span> there are M applications being developed <span style="color:#f92672">and</span> there are N tools out there,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>the total amount of work done by the community was M times N<span style="color:#f92672">.</span> What MCP did was propose a standard
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> applications to get access to tools <span style="color:#f92672">and</span> data sources so that the total work that needs to be
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">29</span>
</span></span><span style="display:flex;"><span>done by the community is now M plus N rather than M times N<span style="color:#f92672">.</span> The initial design of MCP focused a lot
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">38</span>
</span></span><span style="display:flex;"><span>on how to give more context to an LLM <span style="color:#f92672">or</span> how to fetch data<span style="color:#f92672">.</span> So a lot of the initial tools were
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">44</span>
</span></span><span style="display:flex;"><span>ones that would just fetch data<span style="color:#f92672">.</span> And <span style="color:#66d9ef">if</span> you read the MCP documentation, that refers to these as
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">51</span>
</span></span><span style="display:flex;"><span>resources<span style="color:#f92672">.</span> But MCP gives access to both data as well as the more general functions that an
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">57</span>
</span></span><span style="display:flex;"><span>application may want to call<span style="color:#f92672">.</span> And it turns out that there are many MCP clients<span style="color:#f92672">.</span> These are the
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">04</span>
</span></span><span style="display:flex;"><span>applications that want access to tools <span style="color:#f92672">or</span> to data as well as service, which are often the software
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>wrappers that then give access to data <span style="color:#f92672">in</span> Slack <span style="color:#f92672">or</span> GitHub <span style="color:#f92672">or</span> Google Drive <span style="color:#f92672">or</span> allows you to take
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>actions at these different types of resources<span style="color:#f92672">.</span> So today there<span style="color:#e6db74">&#39;s a rapidly growing list of MCP</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">23</span>
</span></span><span style="display:flex;"><span>clients that consume the tools <span style="color:#f92672">or</span> the resources as well as MCP service that provide the tools <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>the resources<span style="color:#f92672">.</span> And I hope that you find it useful to build your own MCP client<span style="color:#f92672">.</span> Your application
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">35</span>
</span></span><span style="display:flex;"><span>maybe one day will be an MCP client<span style="color:#f92672">.</span> And <span style="color:#66d9ef">if</span> you want to provide resources to other developers,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">40</span>
</span></span><span style="display:flex;"><span>maybe you can build your own MCP server someday<span style="color:#f92672">.</span> Let me show you a quick example of using an MCP
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">46</span>
</span></span><span style="display:flex;"><span>client<span style="color:#f92672">.</span> This is a cloud desktop app <span style="color:#f92672">and</span> it has been connected to a GitHub MCP server<span style="color:#f92672">.</span> So when
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">54</span>
</span></span><span style="display:flex;"><span>I enter this query, summarize the readme<span style="color:#f92672">.</span>md from the GitHub repo at this URL, this is actually
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>an AI suite repo<span style="color:#f92672">.</span> Then this application, which is an MCP client, uses the GitHub MCP server with
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">07</span>
</span></span><span style="display:flex;"><span>the request, please get the file readme<span style="color:#f92672">.</span>md from the repo AI suite from this repo<span style="color:#f92672">.</span> And then it
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">14</span>
</span></span><span style="display:flex;"><span>gets this response, which is pretty long<span style="color:#f92672">.</span> All this is then fed back to the LLMs context <span style="color:#f92672">and</span> the LLM
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">21</span>
</span></span><span style="display:flex;"><span>then generates the summary of the markdown file<span style="color:#f92672">.</span> Now let me enter another request, which is let me
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">27</span>
</span></span><span style="display:flex;"><span>enter what are the latest pull requests<span style="color:#f92672">.</span> This <span style="color:#f92672">in</span> turn causes the LLMs to use the MCP server to make
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>a different request, to list the pull request<span style="color:#f92672">.</span> This is another <span style="color:#66d9ef">tool</span> provided by GitHub<span style="color:#e6db74">&#39;s MCP</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>server<span style="color:#f92672">.</span> And so it makes this request with repo AI suite, sort, going to update it, list <span style="color:#ae81ff">20</span>, <span style="color:#f92672">and</span> so on<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>And then it gives this response, which is fed back to the LLM <span style="color:#f92672">and</span> the LLM then writes this nice
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">3</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span>text summary of the latest pull request <span style="color:#66d9ef">for</span> this repo<span style="color:#f92672">.</span> MCP is an important standard<span style="color:#f92672">.</span> If you want to
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">01</span>
</span></span><span style="display:flex;"><span>learn more about it, DeepLearning<span style="color:#f92672">.</span>ai also has a short course that goes much deeper into just the MCP
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">08</span>
</span></span><span style="display:flex;"><span>protocol that you can check out after finishing the course, <span style="color:#66d9ef">if</span> you<span style="color:#e6db74">&#39;re interested. I hope this</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span>video gives you a brief overview of why it<span style="color:#e6db74">&#39;s useful and also why many developers are now building to</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>this standard<span style="color:#f92672">.</span> This brings us to the last video on <span style="color:#66d9ef">tool</span> use<span style="color:#f92672">.</span> And I hope that by giving your own access
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">27</span>
</span></span><span style="display:flex;"><span>to tools, you build <span style="color:#f92672">and</span> build agentic applications that are much more powerful<span style="color:#f92672">.</span> In the next module,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">34</span>
</span></span><span style="display:flex;"><span>we<span style="color:#e6db74">&#39;ll talk about evaluations and error analysis. It turns out that one of the things I&#39;</span>ve seen
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">41</span>
</span></span><span style="display:flex;"><span>that distinguishes people that can execute agentic workflows really well versus teams that are <span style="color:#f92672">not</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">47</span>
</span></span><span style="display:flex;"><span>as efficient at it is your ability to drive a disciplined evaluation process<span style="color:#f92672">.</span> In the next
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">55</span>
</span></span><span style="display:flex;"><span>set of videos, which I think is maybe the most important module of this entire course,
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">00</span>
</span></span><span style="display:flex;"><span>I hope to share with you some of the best practices of how to use evals to drive
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">5</span>:<span style="color:#ae81ff">05</span>
</span></span><span style="display:flex;"><span>development of agentic workflows<span style="color:#f92672">.</span> Look forward to seeing you <span style="color:#f92672">in</span> the next module<span style="color:#f92672">.</span>
</span></span></code></pre></div>
      
    </div>
    
    <section class="comments" aria-label="Comments">
  <div id="giscus_thread" class="giscus"
    data-repo="Linguage/Giscus"
    data-repo-id="R_kgDOLxA-eA"
    data-category="Announcements"
    data-category-id="DIC_kwDOLxA-eM4Ce06R"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme-light="light"
    data-theme-dark="dark"
    data-lang="zh-CN">
  </div>
  <script>
    (function(){
      try{
        const el = document.getElementById('giscus_thread');
        if (!el) return;
        
        let saved = null;
        try{ const v = localStorage.getItem('theme'); if (v === 'light' || v === 'dark') saved = v; }catch(_){ saved = null; }
        const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        const effective = saved ? saved : (prefersDark ? 'dark' : 'light');
        const theme = (effective === 'dark') ? (el.dataset.themeDark || 'dark') : (el.dataset.themeLight || 'light');
        
        el.setAttribute('data-theme', theme);
        
        const s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', el.dataset.repo || '');
        s.setAttribute('data-repo-id', el.dataset.repoId || '');
        s.setAttribute('data-category', el.dataset.category || 'General');
        s.setAttribute('data-category-id', el.dataset.categoryId || '');
        s.setAttribute('data-mapping', el.dataset.mapping || 'pathname');
        s.setAttribute('data-strict', String(el.dataset.strict || '0'));
        s.setAttribute('data-reactions-enabled', String(el.dataset.reactionsEnabled || '1'));
        s.setAttribute('data-emit-metadata', String(el.dataset.emitMetadata || '0'));
        s.setAttribute('data-input-position', el.dataset.inputPosition || 'bottom');
        s.setAttribute('data-theme', theme);
        s.setAttribute('data-lang', el.dataset.lang || 'zh-CN');
        s.setAttribute('crossorigin','anonymous');
        s.async = true;
        el.parentNode.insertBefore(s, el.nextSibling);
      }catch(_){   }
    })();
  </script>
</section>
  </article>

  
  
  
  
    
    <aside class="docs-toc" aria-label="On this page">
      
      <div class="toc-title">On this page</div>
      
      <nav class="toc">
        
          <nav id="TableOfContents">
  <ul>
    <li><a href="#31-what-are-tools">3.1 What are tools?</a></li>
    <li><a href="#32-creating-a-tool">3.2 Creating a tool</a></li>
    <li><a href="#33-tool-syntax">3.3 Tool syntax</a></li>
    <li><a href="#34-code-execution">3.4 Code execution</a></li>
    <li><a href="#35-mcp">3.5 MCP</a></li>
  </ul>
</nav>
            
      </nav>
      
      
    </aside>
  
</div>



<button class="back-to-top" id="backToTop" aria-label="Back to top">↑</button>




  <button id="tocMiniBtn" class="toc-mini-btn" aria-controls="tocDrawer" aria-expanded="false" aria-label="打开目录">☰ 目录</button>
  <div id="tocDrawer" class="toc-drawer" hidden>
    <div class="toc-drawer-inner">
      <div class="toc-drawer-header">
        <span>目录</span>
        <button id="tocCloseBtn" class="toc-drawer-close" aria-label="关闭">×</button>
      </div>
      <nav class="toc">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#31-what-are-tools">3.1 What are tools?</a></li>
    <li><a href="#32-creating-a-tool">3.2 Creating a tool</a></li>
    <li><a href="#33-tool-syntax">3.3 Tool syntax</a></li>
    <li><a href="#34-code-execution">3.4 Code execution</a></li>
    <li><a href="#35-mcp">3.5 MCP</a></li>
  </ul>
</nav>
      </nav>
    </div>
  </div>






  </main>
  
  
  
  <footer class="site-footer">
  
  <div class="container footer-inner">
    
    <div>
      
        
          © 2025 Linguista
        
      
    </div>
    
    
    
    <nav class="footer-nav social-links" aria-label="Social links">
      
      
      <a class="social-icon" href="https://x.com/AztecaAlpaca" target="_blank" rel="noopener" aria-label="X / Twitter">
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/X_logo.jpg/500px-X_logo.jpg" alt="X logo" loading="lazy" decoding="async" />
      </a>
      
      
      <a class="social-icon" href="https://github.com/Linguage" target="_blank" rel="noopener" aria-label="GitHub">
        <svg viewBox="0 0 24 24" aria-hidden="true"><path fill="currentColor" fill-rule="evenodd" d="M12 2C6.48 2 2 6.58 2 12.26c0 4.52 2.87 8.35 6.84 9.71.5.1.68-.22.68-.49 0-.24-.01-.87-.01-1.71-2.78.62-3.37-1.37-3.37-1.37-.45-1.18-1.11-1.5-1.11-1.5-.91-.64.07-.63.07-.63 1 .07 1.53 1.05 1.53 1.05.89 1.56 2.34 1.11 2.91.85.09-.66.35-1.11.63-1.37-2.22-.26-4.56-1.14-4.56-5.07 0-1.12.39-2.03 1.03-2.74-.1-.26-.45-1.3.1-2.71 0 0 .84-.27 2.75 1.05A9.28 9.28 0 0 1 12 6.84c.85.01 1.71.12 2.51.35 1.9-1.32 2.74-1.05 2.74-1.05.55 1.41.2 2.45.1 2.71.64.71 1.03 1.62 1.03 2.74 0 3.94-2.34 4.8-4.57 5.05.36.32.68.95.68 1.92 0 1.38-.01 2.49-.01 2.83 0 .27.18.6.69.49A10.03 10.03 0 0 0 22 12.26C22 6.58 17.52 2 12 2Z" clip-rule="evenodd"/></svg>
      </a>
      
      
      <a class="social-icon" href="https://linguista.notion.site/linguista-hub" target="_blank" rel="noopener" aria-label="Notion">
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Notion-logo.svg/200px-Notion-logo.svg.png?20220918151013" alt="Notion logo" loading="lazy" decoding="async" />
      </a>
      
      
      <a class="social-icon bear-badge" href="https://linguista.bearblog.dev/" target="_blank" rel="noopener" aria-label="Bear Blog ʕ•ᴥ•ʔ">ʕ•ᴥ•ʔ</a>
      
    </nav>
    
  </div>
  
</footer>

  
  
  <script src="/script.js?v=20251015-02" defer></script>
  <script>
    window.SEARCH_INDEX_URL_LITE = '\/index-lite.json?v=20251015-02';
    window.SEARCH_INDEX_URL = '\/index.json?v=20251015-02';
  </script>
  <script src="/js/search-advanced.js?v=20251015-02" defer></script>
  
  <script defer src="/js/table-7char.js?v=20251015-02"></script>
  
  <script defer src="/js/linkcard.js?v=20251015-02"></script>
  
  
  
  
  
</body>
</html>
