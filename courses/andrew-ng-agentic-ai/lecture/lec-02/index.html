<!doctype html><html lang=zh-CN class=skin-gold><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>「Scripts」Module 2: Reflection Design Pattern • Linguista</title><script>(function(){try{var s="theme",e=window.localStorage?localStorage.getItem(s):null,n=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=e==="dark"||e==="light"?e:n?"dark":null;t?document.documentElement.dataset.theme=t:delete document.documentElement.dataset.theme}catch{}})()</script><link rel=preconnect href=https://fonts.loli.net><link rel=preconnect href=https://gstatic.loli.net crossorigin><link href="https://fonts.loli.net/css2?family=Inter:wght@400;500;600&family=Lora:wght@500;600&family=Josefin+Sans:wght@400;600;700&display=swap" rel=stylesheet><link rel=stylesheet href="/css/amp.css?v=20260121-08"><link rel=stylesheet href="/css/mobile-toc-fix.css?v=20260121-08"><style>:root{--bg:#ffffff;--surface:#ffffff;--text:#2b2b2b;--muted:#6c757d;--border:#e9ecef;--brand-text:var(--text, #2b2b2b);--brand-muted:var(--muted, #6c757d);--brand-surface:var(--surface, #ffffff);--brand-border:var(--border, #e9ecef);--brand-accent:var(--accent, #2E7D6B);--brand-accent-soft:rgba(46, 125, 107, 0.12);--brand-accent-soft-strong:rgba(46, 125, 107, 0.18);--gold:#e6db74;--gold-soft:rgba(230, 219, 116, 0.12);--gold-soft-strong:rgba(230, 219, 116, 0.2);--hero-pattern-color:#000000;--hero-pattern-opacity:0.2}html[data-theme=dark]{--bg:#000000;--surface:#111111;--text:#ffffff;--muted:#a1a1aa;--border:#333333;--hero-pattern-color:#ffffff;--hero-pattern-opacity:0.3}.container h1:first-of-type{color:var(--brand-accent)}.hero .hero-title{color:inherit!important}.hero .hero-title .hero-greeting{color:var(--brand-text,#111)!important}.hero .hero-title .hero-brand{color:var(--brand-accent,#0e6a85)!important}html[data-theme=dark] .hero .hero-title .hero-greeting{color:#f8f8f2}html[data-theme=dark] .hero .hero-title .hero-brand{color:var(--gold)}html[data-theme=dark] .container h1{color:#f8f8f2}html[data-theme=dark] .container,html[data-theme=dark] .hero{--brand-accent:var(--gold)}:is(.container,.prose) h2{position:relative;display:flex;flex-direction:column;align-items:center;justify-content:center;width:100%;margin:2.6rem auto 2.4rem;padding:0 0 .9rem;font-weight:600;text-align:center;color:var(--heading-h2-color,#14532d)}:is(.container,.prose) h2 .anchor-link{position:absolute;top:50%;right:clamp(16px,3vw,48px);transform:translateY(-50%);font-size:1.3rem;color:color-mix(in srgb,var(--heading-h2-color,#14532d) 70%,transparent 30%);text-decoration:none;opacity:0;transition:opacity .2s ease,color .2s ease}:is(.container,.prose) h2:hover .anchor-link,:is(.container,.prose) h2 .anchor-link:focus{opacity:1;color:var(--heading-h2-color,#14532d)}:is(.container,.prose) h2::after{content:"";position:absolute;left:50%;bottom:0;width:clamp(220px,38vw,520px);height:3px;transform:translateX(-50%);background:linear-gradient(90deg,color-mix(in srgb,var(--heading-h2-color,#14532d) 0%,transparent 100%) 0%,color-mix(in srgb,var(--heading-h2-color,#14532d) 22%,transparent 78%) 20%,color-mix(in srgb,var(--heading-h2-color,#14532d) 75%,transparent 25%) 50%,color-mix(in srgb,var(--heading-h2-color,#14532d) 22%,transparent 78%) 80%,color-mix(in srgb,var(--heading-h2-color,#14532d) 0%,transparent 100%) 100%);border-radius:999px}:is(.container,.prose) h2::before{content:"";position:absolute;left:50%;bottom:-8px;width:clamp(120px,28vw,360px);height:1px;transform:translateX(-50%);background:linear-gradient(90deg,color-mix(in srgb,var(--heading-h2-color,#14532d) 0%,transparent 100%) 0%,color-mix(in srgb,var(--heading-h2-color,#14532d) 35%,transparent 65%) 45%,color-mix(in srgb,var(--heading-h2-color,#14532d) 70%,transparent 30%) 50%,color-mix(in srgb,var(--heading-h2-color,#14532d) 35%,transparent 65%) 55%,color-mix(in srgb,var(--heading-h2-color,#14532d) 0%,transparent 100%) 100%);opacity:.85}:is(.container,.prose) h2 a{color:var(--brand-accent,#0e6a85)}:is(.container,.prose) h2 a:visited{color:var(--brand-accent-visited,#094c60)}html[data-theme=dark] :is(.container,.prose) h2{--heading-h2-color:rgba(230,219,116,1);color:var(--heading-h2-color)}html[data-theme=dark] .prose h2,html[data-theme=dark] .container h2{color:#e6db74}html[data-theme=dark] :is(.container,.prose) h2 a{color:var(--gold)}html[data-theme=dark] :is(.container,.prose) h2 a:visited{color:rgba(230,219,116,.85)}.container h3{border-left:4px solid var(--brand-accent);border-bottom:1px solid rgba(46,125,107,.25);padding-left:14px;padding-bottom:6px;margin-top:1.5rem;margin-bottom:.75rem;color:var(--brand-text)}.container h4{border-left:3px solid var(--brand-accent);border-bottom:1px solid rgba(46,125,107,.2);padding-left:12px;padding-bottom:4px;margin-top:1.25rem;margin-bottom:.5rem;color:var(--brand-text);font-weight:500}.container h5{border-left:2px solid var(--brand-accent);border-bottom:1px solid rgba(46,125,107,.15);padding-left:10px;padding-bottom:3px;margin-top:1rem;margin-bottom:.4rem;color:var(--brand-text);font-weight:500}.container h6{border-left:1px solid var(--brand-accent);border-bottom:1px solid rgba(46,125,107,.1);padding-left:8px;padding-bottom:2px;margin-top:.75rem;margin-bottom:.3rem;color:var(--brand-text);font-weight:500}.container blockquote{margin:30px 0;padding:20px 25px;border-left:4px solid var(--brand-muted);background-color:#f8f9fa;font-style:italic;border-radius:0 8px 8px 0}.container blockquote footer{margin-top:10px;font-size:.9em;color:var(--brand-muted)}.container .figure-caption,.container .caption,.container figure figcaption,.container .table-caption,.container table caption,.container .quarto-float-caption-top,.container .quarto-float-caption{text-align:left!important;caption-side:top!important;font-weight:500!important;margin-bottom:.75rem!important;margin-top:1rem!important;color:var(--brand-muted)!important;font-size:.9rem!important;line-height:1.4!important}.container table{border-collapse:collapse;width:100%;margin:1.2rem 0;font-size:.95rem}.container table th{background-color:#f8f9fa;font-weight:600;text-align:left;padding:12px 16px;border-bottom:2px solid var(--brand-border)}.container table td{text-align:left;padding:10px 16px;border-bottom:1px solid var(--brand-border);vertical-align:top}.container table tbody tr:nth-child(even){background-color:#f8f9fa}.container table tbody tr:hover{background-color:#f1f3f5}html[data-theme=dark] .container h2{border-color:rgba(230,219,116,.22);color:#f8f8f2}.container h2 a{color:var(--brand-accent,#0e6a85)}.container h2 a:visited{color:var(--brand-accent-visited,#094c60)}html[data-theme=dark] .container blockquote{background-color:rgba(230,219,116,8%);border-left-color:rgba(230,219,116,.6)}html[data-theme=dark] .docs-content .prose blockquote,html[data-theme=dark] .prose blockquote{background-color:rgba(230,219,116,8%);border-left-color:rgba(230,219,116,.6)}html[data-theme=dark] .container .figure-caption,html[data-theme=dark] .container .caption,html[data-theme=dark] .container figure figcaption,html[data-theme=dark] .container .table-caption,html[data-theme=dark] .container table caption,html[data-theme=dark] .container .quarto-float-caption-top,html[data-theme=dark] .container .quarto-float-caption{color:#c9ced6!important}html[data-theme=dark] .container table th{background-color:rgba(255,255,255,6%);border-bottom-color:rgba(230,219,116,.22)}html[data-theme=dark] .docs-content .prose table thead th,html[data-theme=dark] .prose table thead th{background-color:rgba(255,255,255,6%);border-bottom-color:rgba(230,219,116,.22)}html[data-theme=dark] .container table thead,html[data-theme=dark] .docs-content .prose table thead{background-color:rgba(255,255,255,4%)}html[data-theme=dark] .container table td{border-bottom-color:rgba(255,255,255,.12)}html[data-theme=dark] .container table tbody tr:nth-child(even){background-color:rgba(255,255,255,4%)}html[data-theme=dark] .docs-content .prose table tbody tr:nth-child(even),html[data-theme=dark] .prose table tbody tr:nth-child(even){background-color:rgba(255,255,255,4%)}html[data-theme=dark] .container table tbody tr:hover{background-color:rgba(255,255,255,8%)}html[data-theme=dark] .docs-content .prose table tbody tr:hover,html[data-theme=dark] .prose table tbody tr:hover{background-color:rgba(255,255,255,8%)}html[data-theme=dark] .container h1:first-of-type{color:var(--brand-color,var(--link-color,var(--gold)))}html[data-theme=dark] .container h3{border-left-color:var(--gold);border-bottom-color:rgba(230,219,116,.22);color:#f8f8f2}html[data-theme=dark] .container h4{border-left-color:var(--gold);border-bottom-color:rgba(230,219,116,.18);color:#f8f8f2}html[data-theme=dark] .container h5{border-left-color:var(--gold);border-bottom-color:rgba(230,219,116,.14);color:#f8f8f2}html[data-theme=dark] .container h6{border-left-color:var(--gold);border-bottom-color:rgba(230,219,116,.1);color:#f8f8f2}.prose a{color:var(--link-color,var(--brand-accent,#0e6a85))}.prose a:visited{color:var(--visited-color,var(--brand-accent-visited,#094c60))}html[data-theme=dark] .prose a{color:var(--gold)}html[data-theme=dark] .prose a:visited{color:rgba(230,219,116,.85)}html[data-theme=dark] .callout{background:rgba(255,255,255,6%);border-color:rgba(255,255,255,.18)}html[data-theme=dark] .callout.tip{background:rgba(230,219,116,8%);border-color:rgba(230,219,116,.22)}html[data-theme=dark] .callout.warn{background:rgba(253,151,31,.1);border-color:rgba(253,151,31,.28)}.search-hit-flash{animation:searchHitFlash 1.2s ease-out 1;outline:2px solid var(--accent-2,#0F6DDC);outline-offset:2px;scroll-margin-top:calc(var(--theme-header-height,64px) + 8px)}@keyframes searchHitFlash{0%{background-color:rgba(15,109,220,.18)}60%{background-color:rgba(15,109,220,8%)}100%{background-color:initial}}</style><style>:root{--theme-breadcrumbs-height:0px}</style><script>window.tailwind=window.tailwind||{},tailwind.config={darkMode:["class",'[data-theme="dark"]'],theme:{extend:{colors:{bg:"var(--bg)",surface:"var(--surface)",border:"var(--border)",text:"var(--text)",muted:"var(--muted)",accent:"var(--accent)","accent-2":"var(--accent-2)"},fontFamily:{sans:["Inter","system-ui","-apple-system","Segoe UI","Roboto","sans-serif"],serif:["Lora","Georgia","Times New Roman","serif"],josefin:['"Josefin Sans"',"sans-serif"]},maxWidth:{container:"var(--container)"},spacing:{header:"var(--theme-header-height)",footer:"var(--theme-footer-height)"}}},corePlugins:{preflight:!1},safelist:["group/link-card","sm:flex-row","sm:items-center","h-[84px]","w-[84px]","sm:h-[84px]","sm:w-[84px]","line-clamp-2"]}</script><script src="https://cdn.tailwindcss.com?plugins=aspect-ratio"></script><style>html.skin-gold{--gold:#e6db74;--deep-green:#0e6a85;--deep-green-visited:#094c60}html.skin-gold[data-theme=light],html.skin-gold:not([data-theme]){--bg:#ffffff;--surface:#ffffff;--text:#2b2b2b;--muted:#6c757d;--border:#e9ecef;--accent:var(--deep-green);--accent-2:var(--deep-green);--link-color:var(--deep-green);--visited-color:var(--deep-green-visited)}@media(prefers-color-scheme:light){html.skin-gold:not([data-theme]){--bg:#ffffff;--surface:#ffffff;--text:#2b2b2b;--muted:#6c757d;--border:#e9ecef;--accent:var(--deep-green);--accent-2:var(--deep-green);--link-color:var(--deep-green);--visited-color:var(--deep-green-visited)}}html.skin-gold[data-theme=dark]{--bg:#000000;--surface:#111111;--text:#ffffff;--muted:#a1a1aa;--border:#333333;--accent:var(--gold);--accent-2:var(--gold);--link-color:var(--gold);--visited-color:rgba(230,219,116,0.85);--tw-prose-body:#ffffff;--tw-prose-headings:var(--gold);--tw-prose-lead:#ffffff;--tw-prose-links:var(--gold);--tw-prose-bold:var(--gold);--tw-prose-counters:var(--gold);--tw-prose-bullets:var(--gold);--tw-prose-hr:var(--border);--tw-prose-quotes:#ffffff;--tw-prose-quote-borders:var(--gold);--tw-prose-captions:var(--muted);--tw-prose-code:var(--gold);--tw-prose-pre-code:#e5e7eb;--tw-prose-pre-bg:#111111;--tw-prose-th-borders:var(--border);--tw-prose-td-borders:var(--border)}@media(prefers-color-scheme:dark){html.skin-gold:not([data-theme]){--bg:#000000;--surface:#111111;--text:#ffffff;--muted:#a1a1aa;--border:#333333;--accent:var(--gold);--accent-2:var(--gold);--link-color:var(--gold);--visited-color:rgba(230,219,116,0.85);--tw-prose-body:#ffffff;--tw-prose-headings:var(--gold);--tw-prose-lead:#ffffff;--tw-prose-links:var(--gold);--tw-prose-bold:var(--gold);--tw-prose-counters:var(--gold);--tw-prose-bullets:var(--gold);--tw-prose-hr:var(--border);--tw-prose-quotes:#ffffff;--tw-prose-quote-borders:var(--gold);--tw-prose-captions:var(--muted);--tw-prose-code:var(--gold);--tw-prose-pre-code:#e5e7eb;--tw-prose-pre-bg:#111111;--tw-prose-th-borders:var(--border);--tw-prose-td-borders:var(--border)}}html.skin-gold .hero-title .hero-greeting{color:var(--text,#111)}html.skin-gold .hero-title .hero-brand{color:var(--link-color)}html.skin-gold[data-theme=dark] .hero-title .hero-greeting{color:var(--text)}html.skin-gold .prose a{color:var(--link-color)}html.skin-gold .prose a:visited{color:var(--visited-color)}html.skin-gold[data-theme=dark] .container blockquote{background-color:rgba(230,219,116,.1);border-left-color:var(--gold)}html.skin-gold[data-theme=dark] .container table th{border-bottom-color:rgba(230,219,116,.3)}</style><style>:root{}.footer-hidden{transform:translateY(100%);opacity:.85}.docs-nav a.is-active,.toc a.is-active,.nav-drawer a.is-active{background-color:var(--surface)!important;color:var(--text)!important;font-weight:500!important;box-shadow:0 1px 2px rgba(0,0,0,5%);border-color:var(--border)!important}</style><link rel=icon href=/favicon_io/favicon.ico sizes=any><link rel=icon type=image/png sizes=32x32 href=/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon_io/favicon-16x16.png><link rel=apple-touch-icon href=/favicon_io/apple-touch-icon.png><link rel=manifest href=/favicon_io/site.webmanifest><meta property="og:site_name" content="Linguista"><meta property="og:type" content="article"><meta property="og:title" content="「Scripts」Module 2: Reflection Design Pattern"><meta property="og:description" content="```"><meta property="og:url" content="https://linguista.cn/courses/andrew-ng-agentic-ai/lecture/lec-02/"><meta property="og:image" content="https://linguista.cn/img/Linguista_imresizer.png"><meta property="og:image:width" content="400"><meta property="og:image:height" content="400"><meta name=twitter:card content="summary"><meta name=twitter:site content="@linguista2025"><meta name=twitter:creator content="@linguista2025"><meta name=twitter:title content="「Scripts」Module 2: Reflection Design Pattern"><meta name=twitter:description content="```"><meta name=twitter:image content="https://linguista.cn/img/Linguista_imresizer.png"><link rel=canonical href=https://linguista.cn/courses/andrew-ng-agentic-ai/lecture/lec-02/><script>window.MathJax={loader:{load:["[tex]/ams"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,packages:{"[+]":["ams"]},macros:{unicodeInt:[`\\mathop{\\vcenter{\\mathchoice{\\huge\\unicode{#1}}{\\unicode{#1}}{\\unicode{#1}}{\\unicode{#1}}}}\\nolimits`,1],oiint:"\\unicodeInt{x222F}",oiiint:"\\unicodeInt{x2230}"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-99PGBGJH4S"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-99PGBGJH4S")</script></head><body class="bg-bg text-text font-sans antialiased pt-header min-h-screen flex flex-col" style=background-color:var(--bg);color:var(--text)><header class="site-header sticky top-0 z-50 bg-bg/80 backdrop-blur-md border-b border-border transition-colors duration-300"><div class="header-container container mx-auto px-4 flex items-center gap-6 h-16"><div class=header-before></div><div class="header-brand flex items-center gap-3 shrink-0"><button id=navToggleBtn class="nav-toggle-btn inline-flex items-center justify-center w-10 h-10 rounded-full hover:bg-black/5 dark:hover:bg-white/10 text-text transition-colors focus-visible:outline focus-visible:outline-2 focus-visible:outline-accent" aria-controls=navDrawer aria-expanded=false aria-label="Toggle menu">
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><line x1="4" y1="12" x2="20" y2="12"/><line x1="4" y1="6" x2="20" y2="6"/><line x1="4" y1="18" x2="20" y2="18"/></svg>
</button>
<a href=/ id=brand-logo class="flex items-center gap-2 font-josefin font-bold text-2xl tracking-tight text-accent hover:opacity-80 transition-opacity no-underline" style="font-family:josefin sans,sans-serif!important;color:var(--accent)!important"><span class=md:hidden>Ling</span>
<span class="hidden md:inline">Linguista</span>
<span id=comm-dot class="inline-block w-2 h-2 rounded-full bg-accent ml-1 animate-pulse" style=background-color:var(--accent);vertical-align:middle title="Browser Communication: Active"></span>
</a><script>console.log("%c Cascade Agent Connected ","background: #2E7D6B; color: #fff; border-radius: 3px; padding: 2px 5px;"),console.log("Visual Communication: The pulsing dot next to the logo indicates CSS variables are active.");const logo=document.getElementById("brand-logo");if(logo){const e=getComputedStyle(logo).color;console.log("Diagnostics - Logo Color:",e),console.log("Diagnostics - Theme Mode:",document.documentElement.dataset.theme||"auto/light"),console.log("Diagnostics - Font Family:",getComputedStyle(logo).fontFamily)}</script></div><div class="header-search flex-1 flex justify-center px-4"><div id=siteSearchContainer class="site-search relative flex items-center w-full max-w-md transition-all duration-300" role=search><button id=searchBackBtn class="mobile-search-back hidden mr-2 text-muted hover:text-text" aria-label=Cancel type=button>
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 18l-6-6 6-6"/></svg>
</button>
<span class="site-search-icon absolute left-3 text-muted pointer-events-none transition-opacity" aria-hidden=true><svg width="16" height="16" viewBox="0 0 24 24" fill="none"><circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2"/><line x1="16.65" y1="16.65" x2="21" y2="21" stroke="currentColor" stroke-width="2" stroke-linecap="round"/></svg>
</span><input id=globalSearchInput class="site-search-input w-full bg-black/5 dark:bg-white/5 border border-border rounded-full py-2 pl-10 pr-4 text-sm text-text placeholder-muted focus:bg-surface focus:border-accent focus:outline-none transition-all focus:shadow-md" type=search placeholder=全站搜索… aria-label=站内搜索 autocomplete=off spellcheck=false><div id=globalSearchPopover class="site-search-popover glass-popover absolute top-full left-0 right-0 mt-2 max-h-[80vh] overflow-y-auto border border-border rounded-xl shadow-2xl z-[100] text-text" style=display:none aria-live=polite></div></div></div><div class="header-actions flex items-center gap-2.5 shrink-0"><button id=themeToggle class="inline-flex items-center justify-center min-w-[44px] px-3 py-1.5 text-base rounded-full cursor-pointer border border-border bg-transparent text-text hover:bg-surface transition-colors" aria-label=切换主题 title=切换主题></button></div></div><style>#themeToggle::after{content:'◐'}html[data-theme=dark] #themeToggle::after{content:'◑'}.glass-popover{background-color:rgba(255,255,255,.9);backdrop-filter:blur(20px);-webkit-backdrop-filter:blur(20px)}html[data-theme=dark] .glass-popover{background-color:rgba(0,0,0,.9)}.site-search-popover mark{background-color:#facc15;color:#000!important;border-radius:2px;padding:0 2px;font-weight:600;box-shadow:0 0 0 1px rgba(234,179,8,.5)}html[data-theme=dark] .site-search-popover mark{background-color:#facc15;color:#000!important;box-shadow:0 0 0 1px rgba(234,179,8,.8)}@media(max-width:900px){body.mobile-searching .header-before,body.mobile-searching .header-brand,body.mobile-searching .header-actions{display:none!important}body.mobile-searching .header-container{padding-left:.5rem;padding-right:.5rem;gap:0}body.mobile-searching .header-search{padding-left:0;padding-right:0;flex:1 1 100%}body.mobile-searching #siteSearchContainer{max-width:100%;width:100%}body.mobile-searching #searchBackBtn{display:flex!important;align-items:center;justify-content:center}body.mobile-searching .site-search-icon{display:none}body.mobile-searching #globalSearchInput{padding-left:1rem;width:auto;flex:1}}html:not([data-theme=dark]) #navDrawer{background-color:rgba(255,255,255,.94)!important}html:not([data-theme=dark]) #navDrawer .nav-drawer-header,html:not([data-theme=dark]) #navDrawer .nav-drawer-footer{background-color:rgba(255,255,255,.78)!important}html[data-theme=dark] #navDrawer{background-color:#000!important}html[data-theme=dark] #navDrawer .nav-drawer-header,html[data-theme=dark] #navDrawer .nav-drawer-footer{background-color:#000!important;border-color:rgba(255,255,255,.1)!important}.site-search-popover::-webkit-scrollbar{width:6px}.site-search-popover::-webkit-scrollbar-track{background:0 0}.site-search-popover::-webkit-scrollbar-thumb{background-color:rgba(156,163,175,.5);border-radius:3px}html[data-theme=dark] #mobileTocPanel,html[data-theme=dark] #mobileTocPanel>div:first-child,html[data-theme=dark] #mobileTocToggleBtn{background-color:rgba(0,0,0,.9)!important;border-color:rgba(255,255,255,.15)!important;color:#f4f4f5!important}html[data-theme=dark] #mobileTocPanel h4,html[data-theme=dark] #mobileTocToggleBtn span,html[data-theme=dark] #mobileTocToggleBtn svg{color:#f4f4f5!important}html[data-theme=dark] #mobileTocToggleBtn>div:first-child>div{background-color:rgba(255,255,255,.1)!important}</style></header><div id=navOverlay class="fixed inset-0 z-[90] bg-black/40 backdrop-blur-sm opacity-0 pointer-events-none transition-opacity duration-300" aria-hidden=true></div><aside id=navDrawer class="fixed inset-y-0 left-0 z-[100] w-72 bg-white/92 dark:bg-black/85 backdrop-blur-md border-r border-border transform -translate-x-full transition-transform duration-300 flex flex-col shadow-2xl" aria-hidden=true><div class="nav-drawer-header flex items-center justify-between p-4 border-b border-border bg-white/75 dark:bg-black/60 backdrop-blur-sm"><a href=/ class="font-josefin font-bold text-xl tracking-tight text-accent hover:opacity-80 transition-opacity no-underline">Linguista</a>
<button id=navCloseBtn class="p-2 rounded-lg hover:bg-black/5 dark:hover:bg-white/5 text-muted transition-colors" aria-label="Close menu">
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6 6 18"/><path d="m6 6 12 12"/></svg></button></div><div class="flex-1 overflow-y-auto p-4 custom-scrollbar"><nav class="flex flex-col gap-1 mb-6" aria-label=Primary><div class="text-xs font-semibold text-muted uppercase tracking-wider mb-2 px-2">Menu</div><a class="flex items-center gap-2 px-3 py-2 rounded-lg text-sm font-medium text-text hover:bg-black/5 dark:hover:bg-white/5 transition-colors" href=/labs/>工坊
</a><a class="flex items-center gap-2 px-3 py-2 rounded-lg text-sm font-medium text-text hover:bg-black/5 dark:hover:bg-white/5 transition-colors" href=/essays/>经典
</a><a class="flex items-center gap-2 px-3 py-2 rounded-lg text-sm font-medium text-text hover:bg-black/5 dark:hover:bg-white/5 transition-colors" href=/courses/>课程资料
</a><a class="flex items-center gap-2 px-3 py-2 rounded-lg text-sm font-medium text-text hover:bg-black/5 dark:hover:bg-white/5 transition-colors" href=/bookmarks/>书签
</a><a class="flex items-center gap-2 px-3 py-2 rounded-lg text-sm font-medium text-text hover:bg-black/5 dark:hover:bg-white/5 transition-colors" href=/about/>关于我</a></nav><div class=nav-drawer-folder><div class="text-xs font-semibold text-muted uppercase tracking-wider mb-2 px-2">Directory</div><nav class="flex flex-col gap-1" aria-label="Current Section"><a class="block px-2.5 py-1.5 rounded-lg text-sm transition-colors border border-transparent text-muted hover:text-text hover:bg-black/5 dark:hover:bg-white/5" href=/courses/andrew-ng-agentic-ai/lecture/lec-01/>「Scripts」Module 1: Introduction to Agentic Workflows</a>
<a class="block px-2.5 py-1.5 rounded-lg text-sm transition-colors border border-transparent bg-surface text-text font-medium shadow-sm border-border" href=/courses/andrew-ng-agentic-ai/lecture/lec-02/>「Scripts」Module 2: Reflection Design Pattern</a>
<a class="block px-2.5 py-1.5 rounded-lg text-sm transition-colors border border-transparent text-muted hover:text-text hover:bg-black/5 dark:hover:bg-white/5" href=/courses/andrew-ng-agentic-ai/lecture/lec-03/>「Scripts」Module 3: Tool Use</a>
<a class="block px-2.5 py-1.5 rounded-lg text-sm transition-colors border border-transparent text-muted hover:text-text hover:bg-black/5 dark:hover:bg-white/5" href=/courses/andrew-ng-agentic-ai/lecture/lec-04/>「Scripts」Module 4: Practical Tips for Building Agentic AI</a>
<a class="block px-2.5 py-1.5 rounded-lg text-sm transition-colors border border-transparent text-muted hover:text-text hover:bg-black/5 dark:hover:bg-white/5" href=/courses/andrew-ng-agentic-ai/lecture/lec-05/>「Scripts」Module 5: Patterns for Highly Autonomous Agents</a></nav></div></div><div class="nav-drawer-footer p-4 border-t border-border bg-white/75 dark:bg-black/60 backdrop-blur-sm"><div class="flex flex-col gap-4"><nav class="flex items-center gap-2 flex-wrap" aria-label="Social links"><a class="inline-flex w-8 h-8 rounded-full items-center justify-center text-muted/80 bg-black/5 no-underline transition-all duration-250 hover:bg-accent hover:text-white hover:-translate-y-0.5 dark:bg-white/10 dark:text-gray-300 dark:hover:bg-gray-200 dark:hover:text-black" href=https://x.com/AztecaAlpaca target=_blank rel=noopener aria-label="X / Twitter"><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/X_logo.jpg/500px-X_logo.jpg alt="X logo" class="w-4.5 h-4.5 object-contain rounded" loading=lazy decoding=async>
</a><a class="inline-flex w-8 h-8 rounded-full items-center justify-center text-muted/80 bg-black/5 no-underline transition-all duration-250 hover:bg-accent hover:text-white hover:-translate-y-0.5 dark:bg-white/10 dark:text-gray-300 dark:hover:bg-gray-200 dark:hover:text-black" href=https://github.com/Linguage target=_blank rel=noopener aria-label=GitHub><svg viewBox="0 0 24 24" aria-hidden="true" class="w-4.5 h-4.5 block"><path fill="currentColor" fill-rule="evenodd" d="M12 2C6.48 2 2 6.58 2 12.26c0 4.52 2.87 8.35 6.84 9.71.5.1.68-.22.68-.49.0-.24-.01-.87-.01-1.71-2.78.62-3.37-1.37-3.37-1.37-.45-1.18-1.11-1.5-1.11-1.5-.91-.64.07-.63.07-.63 1 .07 1.53 1.05 1.53 1.05.89 1.56 2.34 1.11 2.91.85.09-.66.35-1.11.63-1.37-2.22-.26-4.56-1.14-4.56-5.07.0-1.12.39-2.03 1.03-2.74-.1-.26-.45-1.3.1-2.71.0.0.84-.27 2.75 1.05A9.28 9.28.0 0112 6.84c.85.01 1.71.12 2.51.35 1.9-1.32 2.74-1.05 2.74-1.05.55 1.41.2 2.45.1 2.71.64.71 1.03 1.62 1.03 2.74.0 3.94-2.34 4.8-4.57 5.05.36.32.68.95.68 1.92.0 1.38-.01 2.49-.01 2.83.0.27.18.6.69.49A10.03 10.03.0 0022 12.26C22 6.58 17.52 2 12 2z" clip-rule="evenodd"/></svg>
</a><a class="inline-flex w-8 h-8 rounded-full items-center justify-center text-muted/80 bg-black/5 no-underline transition-all duration-250 hover:bg-accent hover:text-white hover:-translate-y-0.5 dark:bg-white/10 dark:text-gray-300 dark:hover:bg-gray-200 dark:hover:text-black" href=https://linguista.notion.site/linguista-hub target=_blank rel=noopener aria-label=Notion><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Notion-logo.svg/200px-Notion-logo.svg.png?20220918151013 alt="Notion logo" class="w-4.5 h-4.5 object-contain rounded" loading=lazy decoding=async>
</a><a class="inline-flex w-auto min-w-[32px] px-3.5 h-8 rounded-full items-center justify-center text-sm font-semibold tracking-wide leading-none text-muted/80 bg-black/5 no-underline transition-all duration-250 hover:bg-accent hover:text-white hover:-translate-y-0.5 dark:bg-white/10 dark:text-gray-300 dark:hover:bg-gray-200 dark:hover:text-black" href=https://linguista.bearblog.dev/ target=_blank rel=noopener aria-label="Bear Blog ʕ•ᴥ•ʔ">ʕ•ᴥ•ʔ</a></nav><div class="text-xs text-muted">© 2026 Linguista</div></div></div></aside><main class="mx-auto max-w-container w-full px-4 flex-grow"><div class="grid grid-cols-1 lg:grid-cols-[200px_minmax(0,1fr)_220px] gap-8 py-8 pb-20 items-start max-w-7xl mx-auto"><div class="hidden lg:block" aria-hidden=true></div><article class="docs-content min-w-0"><div class=md-theme-amp-outer><div class=md-theme-amp><div class="text-muted text-sm mb-6"><span class=font-medium>Andrew Ng</span>
·
<span>2025-10-17</span></div><div class="link-card group relative my-3 w-full overflow-hidden rounded-xl border border-border bg-surface transition-shadow duration-300 hover:shadow-md" data-url=https://learn.deeplearning.ai/courses/agentic-ai/><a class="link-card__fallback block px-4 py-3 text-sm font-medium text-accent underline-offset-4 hover:underline" href=https://learn.deeplearning.ai/courses/agentic-ai/ target=_blank rel=noopener>https://learn.deeplearning.ai/courses/agentic-ai/</a></div><h1 id=module-2-reflection-design-pattern>Module 2: Reflection Design Pattern</h1><h2 id=21-reflection-to-improve-outputs-of-a-task>2.1 Reflection to improve outputs of a task</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>0:03
</span></span><span style=display:flex><span>The reflection design pattern is something I&#39;ve used in many applications, and it&#39;s surprisingly easy to implement.
</span></span><span style=display:flex><span>0:07
</span></span><span style=display:flex><span>Let&#39;s take a look.
</span></span><span style=display:flex><span>0:08
</span></span><span style=display:flex><span>Just as humans will sometimes reflect their own output and find a way to improve it, so can LLMs.
</span></span><span style=display:flex><span>0:14
</span></span><span style=display:flex><span>For example, I might write an email like this, and if I&#39;m typing quickly, I might end up with a first draft that&#39;s not great.
</span></span><span style=display:flex><span>0:21
</span></span><span style=display:flex><span>And if I read over it, I might say,
</span></span><span style=display:flex><span>0:24
</span></span><span style=display:flex><span>huh, next month isn&#39;t that clear for what dates Tommy might be free for dinner, and there&#39;s such a typo that I had, and also forgot to sign my name.
</span></span><span style=display:flex><span>0:33
</span></span><span style=display:flex><span>And this would let me revise the draft to be more specific in saying, hey, Tommy, are you free for dinner on the 5th to the 7th?
</span></span><span style=display:flex><span>0:40
</span></span><span style=display:flex><span>A similar process lets LLMs also improve their outputs.
</span></span><span style=display:flex><span>0:44
</span></span><span style=display:flex><span>You can prompt an LLM to write the first draft in email, and given email version 1, email v1, you can pass it to maybe the same model,
</span></span><span style=display:flex><span>0:53
</span></span><span style=display:flex><span>the same large language model, but with a different prompt, and tell it to reflect and write an improved second draft to then get you the final output, email v2.
</span></span><span style=display:flex><span>1:02
</span></span><span style=display:flex><span>Here, I have just hard-coded this workflow of prompting the LLMa once, and then prompting them again to reflect and improve, and that gives email v2.
</span></span><span style=display:flex><span>1:12
</span></span><span style=display:flex><span>It turns out that a similar process can be used to improve other types of outputs.
</span></span><span style=display:flex><span>1:18
</span></span><span style=display:flex><span>For example, if you are having an LLM write code, you might prompt an LLM to write code to do a certain task, and it may give you v1 of the code,
</span></span><span style=display:flex><span>1:28
</span></span><span style=display:flex><span>and then pass it to the same LLM or maybe a different LLM to ask it to check for bugs and write an improved second draft of the code.
</span></span><span style=display:flex><span>1:36
</span></span><span style=display:flex><span>Different LLMs have different strengths, and so sometimes I would choose different models for writing the first draft and for reflecting and trying to improve it.
</span></span><span style=display:flex><span>1:46
</span></span><span style=display:flex><span>For example, it turns out reasoning models, sometimes also called thinking models, are pretty good at finding bugs,
</span></span><span style=display:flex><span>1:53
</span></span><span style=display:flex><span>and so I&#39;ll sometimes write the first draft of the code by direct generation, but then use a reasoning model to check for bugs.
</span></span><span style=display:flex><span>2:00
</span></span><span style=display:flex><span>Now, rather than just having an LLM reflect on the code, it turns out that if you can get external feedback, meaning new information from outside the LLM,
</span></span><span style=display:flex><span>2:11
</span></span><span style=display:flex><span>reflection becomes much more powerful.
</span></span><span style=display:flex><span>2:14
</span></span><span style=display:flex><span>In the case of code, one thing you can do is just execute the code to see what the code does,
</span></span><span style=display:flex><span>2:20
</span></span><span style=display:flex><span>and by examining the output, including any error messages of the code, this is incredibly useful information for the LLM to reflect and to find a way to improve his code.
</span></span><span style=display:flex><span>2:30
</span></span><span style=display:flex><span>So in this example, the LLM generated the first draft of the code, but when I run it, it generates a syntax error.
</span></span><span style=display:flex><span>2:36
</span></span><span style=display:flex><span>When you pass this code output and error logs back into the LLM and ask it to reflect on the feedback and write a new draft,
</span></span><span style=display:flex><span>2:44
</span></span><span style=display:flex><span>this gives it a lot of very useful information to come up with a much better version 2 of the code.
</span></span><span style=display:flex><span>2:50
</span></span><span style=display:flex><span>So the reflection design pattern isn&#39;t magic.
</span></span><span style=display:flex><span>2:53
</span></span><span style=display:flex><span>It does not make an LLM always get everything right 100% of the time, but it can often give it maybe a modest bump in performance.
</span></span><span style=display:flex><span>3:01
</span></span><span style=display:flex><span>But one design consideration to keep in mind is reflection is much more powerful when there is new additional external information that you can ingest into the reflection process.
</span></span><span style=display:flex><span>3:13
</span></span><span style=display:flex><span>So in this example, if you can run the code and have that code output or error messages as an additional input to the reflection step,
</span></span><span style=display:flex><span>3:20
</span></span><span style=display:flex><span>that really lets the LLM reflect much more deeply and figure out what may be going wrong, if anything,
</span></span><span style=display:flex><span>3:26
</span></span><span style=display:flex><span>and results in a much better second version of the code than if there wasn&#39;t this external information that you can ingest.
</span></span><span style=display:flex><span>3:32
</span></span><span style=display:flex><span>So one thing to keep in mind, whenever reflection has an opportunity to get additional information, that makes it much more powerful.
</span></span><span style=display:flex><span>3:41
</span></span><span style=display:flex><span>Now with that, let&#39;s go on to the next video where I want to share with you a more systematic comparison of using reflection versus direct generation or something we sometimes call zero shot prompting.
</span></span><span style=display:flex><span>3:54
</span></span><span style=display:flex><span>Let&#39;s go on to the next video.
</span></span></code></pre></div><h2 id=22-why-not-just-direct-generation>2.2 why not just direct generation?</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-gdscript3 data-lang=gdscript3><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>Let<span style=color:#e6db74>&#39;s take a look at why we might prefer to use a reflection workflow rather than just</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>prompting an LLM once <span style=color:#f92672>and</span> having it directly generate the answer <span style=color:#f92672>and</span> be done with it<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>09</span>
</span></span><span style=display:flex><span>With direct generation, you just prompt the LLM with an instruction <span style=color:#f92672>and</span> let it generate an answer<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>So you can ask an LLM to write an essay about black holes <span style=color:#f92672>and</span> have it just generate the text,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>19</span>
</span></span><span style=display:flex><span><span style=color:#f92672>or</span> have it write the Python functions to calculate
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>22</span>
</span></span><span style=display:flex><span>compound interest <span style=color:#f92672>and</span> have it just write the code directly<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>25</span>
</span></span><span style=display:flex><span>The prompt examples you see here are also called zero<span style=color:#f92672>-</span>shot prompting<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>Let me explain what zero<span style=color:#f92672>-</span>shot means<span style=color:#f92672>.</span> In contrast to zero<span style=color:#f92672>-</span>shot prompting,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>a related approach is to include one <span style=color:#f92672>or</span> more examples of what you want the output to look like
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span><span style=color:#f92672>in</span> your prompt<span style=color:#f92672>.</span> And this is known as one<span style=color:#f92672>-</span>shot prompting, <span style=color:#66d9ef>if</span> <span style=color:#f92672>in</span> the prompt you include
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>one example of a desired input<span style=color:#f92672>-</span>output pair, <span style=color:#f92672>or</span> two<span style=color:#f92672>-</span>shot <span style=color:#f92672>or</span> few<span style=color:#f92672>-</span>shot prompting,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>depending on how many such examples you include <span style=color:#f92672>in</span> your prompt<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>53</span>
</span></span><span style=display:flex><span>And so zero<span style=color:#f92672>-</span>shot prompting refers to <span style=color:#66d9ef>if</span> you include zero examples <span style=color:#f92672>and</span> <span style=color:#66d9ef>if</span> you don<span style=color:#e6db74>&#39;t include</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>any examples of the desired outputs that you want<span style=color:#f92672>.</span> But don<span style=color:#e6db74>&#39;t worry if you aren&#39;</span>t yet familiar
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>with these terms<span style=color:#f92672>.</span> The important thing is that <span style=color:#f92672>in</span> the examples you see here, you<span style=color:#e6db74>&#39;re just prompting</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>the LLM to directly generate an answer <span style=color:#f92672>in</span> one go, which I<span style=color:#e6db74>&#39;m also calling zero-shot prompting</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>13</span>
</span></span><span style=display:flex><span>because we include zero examples<span style=color:#f92672>.</span> It turns out that multiple studies have
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>shown that reflection improves on the performance of direct generation on a variety of tasks<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>This diagram is adapted from the research paper by Madaan <span style=color:#f92672>and</span> others, <span style=color:#f92672>and</span> this shows a range of
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>30</span>
</span></span><span style=display:flex><span>different tasks being implemented with different models <span style=color:#f92672>and</span> with <span style=color:#f92672>and</span> without reflection<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>The way to read this diagram is to look at these pairs of adjacent light followed by
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>dark<span style=color:#f92672>-</span>colored bars, where the light bar shows zero<span style=color:#f92672>-</span>shot prompting <span style=color:#f92672>and</span> the dark bar
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>46</span>
</span></span><span style=display:flex><span>shows the same model but with reflection<span style=color:#f92672>.</span> And the colors blue, green, <span style=color:#f92672>and</span> red show
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>52</span>
</span></span><span style=display:flex><span>experiments run with different models, such as GPT<span style=color:#f92672>-</span><span style=color:#ae81ff>3.5</span> <span style=color:#f92672>and</span> GPT<span style=color:#f92672>-</span><span style=color:#ae81ff>4.</span> And what you see is,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> many different applications, the dark bar that is with reflection is quite a bit higher
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>than the light bar<span style=color:#f92672>.</span> But of course, your knowledge may vary depending on your specific application<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>Here are some more examples where reflection might be helpful<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>If you are generating structured data, such as an HTML table, sometimes it may have incorrect
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>formatting of the output<span style=color:#f92672>.</span> So a reflection prompt to validate the HTML code could be helpful<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>If it<span style=color:#e6db74>&#39;s basic HTML, this may not help that much, since LLMs are pretty good at basic HTML. But</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>especially <span style=color:#66d9ef>if</span> you have more complex structured outputs, like maybe a JSON data structure with
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>36</span>
</span></span><span style=display:flex><span>a lot of nesting, then reflection may be more likely to spot bugs<span style=color:#f92672>.</span> Or <span style=color:#66d9ef>if</span> you ask an LLM to
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>generate a sequence of steps that comprise a set of instructions to <span style=color:#66d9ef>do</span> something, such as how to
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>brew a perfect cup of tea, sometimes the LLM may miss steps <span style=color:#f92672>and</span> a reflection prompt to ask to check
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>instructions <span style=color:#66d9ef>for</span> coherence <span style=color:#f92672>and</span> completeness might help spot errors<span style=color:#f92672>.</span> Or something that I<span style=color:#e6db74>&#39;ve actually</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>56</span>
</span></span><span style=display:flex><span>worked on was using an LLM to generate domain names, but sometimes the names it generates has
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>01</span>
</span></span><span style=display:flex><span>an unintended meaning <span style=color:#f92672>or</span> may be really hard to pronounce<span style=color:#f92672>.</span> And so I<span style=color:#e6db74>&#39;ve used reflection prompts</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>06</span>
</span></span><span style=display:flex><span>to double check <span style=color:#66d9ef>if</span> the domain name has any problematic connotations <span style=color:#f92672>or</span> problematic meanings,
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span><span style=color:#f92672>or</span> <span style=color:#66d9ef>if</span> the name is hard to pronounce<span style=color:#f92672>.</span> And we actually used this at one of my team<span style=color:#e6db74>&#39;s AI fund</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>to help brainstorm domain names <span style=color:#66d9ef>for</span> startups that we<span style=color:#e6db74>&#39;re working on. I want to show you a couple of</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>examples of reflection prompts<span style=color:#f92672>.</span> For brainstorming domain names, you might ask it to review the
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>domain names you suggested, <span style=color:#f92672>and</span> then ask it to check <span style=color:#66d9ef>if</span> each name is easy to pronounce<span style=color:#f92672>.</span> Check
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>30</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> each name might mean something negative <span style=color:#f92672>in</span> English <span style=color:#f92672>or</span> other languages, <span style=color:#f92672>and</span> then output a
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>short list of only the names that satisfy these criteria<span style=color:#f92672>.</span> Or to improve an email, you can write a
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>reflection prompt to tell it to review the email first draft, check the tone, verify all fact states
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>46</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> promises are accurate<span style=color:#f92672>.</span> This would make sense <span style=color:#f92672>in</span> the context of the LLM having been fed a number
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>of facts <span style=color:#f92672>and</span> dates <span style=color:#f92672>and</span> so on <span style=color:#f92672>in</span> order to write the email drafts<span style=color:#f92672>.</span> All this would be provided as part
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>56</span>
</span></span><span style=display:flex><span>of the LLM context<span style=color:#f92672>.</span> And then based on any problems it may find, write the next draft of the email<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>So some tips <span style=color:#66d9ef>for</span> writing reflection prompts<span style=color:#f92672>.</span> It helps to clearly indicate that you want it to
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>review <span style=color:#f92672>or</span> to reflect on the first draft of the output<span style=color:#f92672>.</span> And <span style=color:#66d9ef>if</span> you can specify a clear set of
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>13</span>
</span></span><span style=display:flex><span>criteria, such as whether the domain name is easy to pronounce <span style=color:#f92672>and</span> whether it may have negative
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>connotations <span style=color:#f92672>or</span> <span style=color:#66d9ef>for</span> email, check the tone <span style=color:#f92672>and</span> verify the facts<span style=color:#f92672>.</span> Then that guides the LLM better
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>22</span>
</span></span><span style=display:flex><span><span style=color:#f92672>in</span> reflecting <span style=color:#f92672>and</span> critiquing on the criteria that you care the most about<span style=color:#f92672>.</span> I found that one of the
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>ways I<span style=color:#e6db74>&#39;ve learned to write better prompts is to read a lot of other prompts that other people</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>34</span>
</span></span><span style=display:flex><span>have written<span style=color:#f92672>.</span> Sometimes I<span style=color:#e6db74>&#39;ll actually download open source software and go and find the prompts</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span><span style=color:#f92672>in</span> a piece of software that I think is especially well done to just go <span style=color:#f92672>and</span> read the prompts that
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>the authors have written<span style=color:#f92672>.</span> So that I hope you have a sense of how to write a basic reflection prompt
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> that maybe you even try it out <span style=color:#f92672>in</span> your own work to see <span style=color:#66d9ef>if</span> it helps give you better performance<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>In the next video, I<span style=color:#e6db74>&#39;d like to share with you a fun example where we&#39;</span>ll start to look at
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>multi<span style=color:#f92672>-</span>modal inputs <span style=color:#f92672>and</span> outputs<span style=color:#f92672>.</span> We<span style=color:#e6db74>&#39;ll have an algorithm reflect</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>06</span>
</span></span><span style=display:flex><span>on an image being generated <span style=color:#f92672>or</span> a chart being generated<span style=color:#f92672>.</span> Let<span style=color:#e6db74>&#39;s go take a look.</span>
</span></span></code></pre></div><h2 id=23-chart-generation-workflow>2.3 chart generation workflow</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-gdscript3 data-lang=gdscript3><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>In the coding lab that you see <span style=color:#f92672>in</span> this module, you play with a chart generation workflow where
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>05</span>
</span></span><span style=display:flex><span>you use an agent to generate nice<span style=color:#f92672>-</span>looking diagrams<span style=color:#f92672>.</span> It turns out reflection can significantly improve
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span>the quality of this output<span style=color:#f92672>.</span> Let<span style=color:#e6db74>&#39;s take a look. In this example, I have data from a coffee machine</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>18</span>
</span></span><span style=display:flex><span>showing when different drinks, such as a latte coffee <span style=color:#f92672>or</span> hot chocolate <span style=color:#f92672>or</span> a cappuccino coffee
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> so on, were sold <span style=color:#f92672>and</span> <span style=color:#66d9ef>for</span> what price<span style=color:#f92672>.</span> And we want to have an agent create a plot
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>29</span>
</span></span><span style=display:flex><span>comparing Q1 <span style=color:#f92672>or</span> first quarter coffee sales <span style=color:#f92672>in</span> <span style=color:#ae81ff>2024</span> <span style=color:#f92672>and</span> <span style=color:#ae81ff>2025.</span> So one way to <span style=color:#66d9ef>do</span> it would be to
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>write a prompt that asks an LLM to create a plot comparing Q1 coffee sales <span style=color:#f92672>in</span> <span style=color:#ae81ff>2024</span> <span style=color:#f92672>and</span> <span style=color:#ae81ff>2025</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>using the data stored <span style=color:#f92672>in</span> a spreadsheet as a CSV file <span style=color:#f92672>or</span> comma<span style=color:#f92672>-</span>separated values as a spreadsheet
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>48</span>
</span></span><span style=display:flex><span>file<span style=color:#f92672>.</span> And an LLM might write Python code like this to generate the plot<span style=color:#f92672>.</span> And with this v1 of the code,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>55</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> you execute it, it may generate a plot like this<span style=color:#f92672>.</span> When I ran the code to the LLM output,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>01</span>
</span></span><span style=display:flex><span>it actually generated this the first time<span style=color:#f92672>.</span> And this is a stacked bar plot, which is <span style=color:#f92672>not</span> a very
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>easy way to visualize things <span style=color:#f92672>and</span> it just doesn<span style=color:#e6db74>&#39;t look a very good plot. But what you can do is then</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>give it v1 of the code as well as the plot that this code generated <span style=color:#f92672>and</span> feed it into a
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>19</span>
</span></span><span style=display:flex><span>multimodal model that is an LLM that can also accept image inputs <span style=color:#f92672>and</span> ask it to examine the
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>image that was generated by this code <span style=color:#f92672>and</span> then to critique the image, find a way to come up with
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>better visualization, <span style=color:#f92672>and</span> update the code to just generate a clearer, better plot<span style=color:#f92672>.</span> Multimodal LLMs
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>37</span>
</span></span><span style=display:flex><span>can use visual reasoning, so it can actually look visually at this figure to find ways to improve it<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>44</span>
</span></span><span style=display:flex><span>And when I did this, it actually generated a bar graph that isn<span style=color:#e6db74>&#39;t this stacked bar graph,</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>49</span>
</span></span><span style=display:flex><span>but a more regular bar graph that separates out the <span style=color:#ae81ff>2034</span> <span style=color:#f92672>and</span> <span style=color:#ae81ff>2035</span> coffee sales <span style=color:#f92672>in</span> what I thought
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>54</span>
</span></span><span style=display:flex><span>was a more pleasing <span style=color:#f92672>and</span> clearer way<span style=color:#f92672>.</span> When you get to the coding lab, please feel free to mess around
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>00</span>
</span></span><span style=display:flex><span>with the problems <span style=color:#f92672>and</span> see <span style=color:#66d9ef>if</span> you can get maybe even better looking graphs <span style=color:#f92672>in</span> these<span style=color:#f92672>.</span> Because
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>05</span>
</span></span><span style=display:flex><span>different LLMs have different strengths <span style=color:#f92672>and</span> weaknesses, sometimes I<span style=color:#e6db74>&#39;ll use different LLMs</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>09</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> the initial generation <span style=color:#f92672>and</span> <span style=color:#66d9ef>for</span> the reflection<span style=color:#f92672>.</span> So, <span style=color:#66d9ef>for</span> example, you may use one LLM to generate
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>the initial code, maybe open it as GPT<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>o <span style=color:#f92672>or</span> GPT<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span> <span style=color:#f92672>or</span> some model like that, <span style=color:#f92672>and</span> just prompt it like a
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>prompt like this to write Python code to generate visualization <span style=color:#f92672>and</span> so on<span style=color:#f92672>.</span> And then the reflection
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>prompts might be something like this, where you tell the LLM to play the role of an expert data
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>analyst that provides constructive feedback <span style=color:#f92672>and</span> then give it the version <span style=color:#ae81ff>1</span> of the code,
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>36</span>
</span></span><span style=display:flex><span>the part that was generated, maybe also the computational history from how the code was
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>generated, <span style=color:#f92672>and</span> ask it to critique it <span style=color:#66d9ef>for</span> specific criteria<span style=color:#f92672>.</span> Remember, when you give it specific
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>criteria like readability, clarity, <span style=color:#f92672>and</span> completeness, it helps the LLM better figure out what to <span style=color:#66d9ef>do</span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>And then ask it to write new code to implement your improvements<span style=color:#f92672>.</span> One thing you may find is that
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>56</span>
</span></span><span style=display:flex><span>sometimes using a reasoning model <span style=color:#66d9ef>for</span> reflection may work better than a non<span style=color:#f92672>-</span>reasoning model<span style=color:#f92672>.</span> So
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>when you<span style=color:#e6db74>&#39;re trying out different models for the initial generation and the reflection, these are</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>different configurations that you might toggle <span style=color:#f92672>or</span> try different combinations of<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>So when you get to the coding lab, I hope you have fun visualizing coffee sales<span style=color:#f92672>.</span> Now, when you<span style=color:#e6db74>&#39;re</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>building an application, one thing you may be wondering is, does reflection actually improve
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>22</span>
</span></span><span style=display:flex><span>performance on your specific application<span style=color:#960050;background-color:#1e0010>?</span> From various studies, reflection improves performance
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>by a little bit on some, by a lot on some others, <span style=color:#f92672>and</span> maybe barely any at all on some other
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>34</span>
</span></span><span style=display:flex><span>applications<span style=color:#f92672>.</span> And so it<span style=color:#e6db74>&#39;ll be useful to understand its impact on your application and also give you</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>39</span>
</span></span><span style=display:flex><span>guidance on how to tune either the initial generation <span style=color:#f92672>or</span> the reflection prompt to try to get
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>better performance<span style=color:#f92672>.</span> In the next video, let<span style=color:#e6db74>&#39;s take a look at evals or evaluations</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> reflection workflow<span style=color:#f92672>.</span> Let<span style=color:#e6db74>&#39;s go on to the next video.</span>
</span></span></code></pre></div><h2 id=24-evaluating-the-impact-of-reflection>2.4 Evaluating the impact of reflection</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-gdscript3 data-lang=gdscript3><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>Reflection often improves the performance of the system, but before I commit to keeping it,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>I would usually want to double check how much it actually improves the performance, because
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>09</span>
</span></span><span style=display:flex><span>it does slow down the system a little bit by needing to take an extra step<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>13</span>
</span></span><span style=display:flex><span>Let<span style=color:#e6db74>&#39;s take a look at evals for reflection workflows.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>Let<span style=color:#e6db74>&#39;s look at an example of using reflection to improve the database query that an LLM writes</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>to fetch data to answer questions<span style=color:#f92672>.</span> Let<span style=color:#e6db74>&#39;s say you run a retail store,</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> you may get questions like, which color product has the highest total sales<span style=color:#960050;background-color:#1e0010>?</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>To answer a question like this, you might have an LLM generate a database query<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>36</span>
</span></span><span style=display:flex><span>If you<span style=color:#e6db74>&#39;ve heard of database languages like SQL, SQL, it may generate a query in that type of</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>language<span style=color:#f92672>.</span> But <span style=color:#66d9ef>if</span> you<span style=color:#e6db74>&#39;re not familiar with SQL, don&#39;</span>t worry about it<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>But after writing a database query, instead of using that directly to fetch information from
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>the database, you may have an LLM, the same <span style=color:#f92672>or</span> different LLM, reflect on the version one database
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>and</span> update it to maybe an improved one, <span style=color:#f92672>and</span> then execute that database query against the
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>database to fetch information to finally have an LLM answer the question<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>So the question is, does using a second LLM to reflect <span style=color:#f92672>and</span> improve
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>on the database <span style=color:#f92672>or</span> SQL query actually improve the final output<span style=color:#960050;background-color:#1e0010>?</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>In order to evaluate this, I might collect a set of questions <span style=color:#f92672>or</span> set of prompts together with
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>ground truth answers<span style=color:#f92672>.</span> So maybe one would be, how many items are sold <span style=color:#f92672>in</span> May <span style=color:#ae81ff>2025</span><span style=color:#960050;background-color:#1e0010>?</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>What<span style=color:#e6db74>&#39;s the most expensive item in the inventory? How many styles are carried in my store?</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>33</span>
</span></span><span style=display:flex><span>And I write down <span style=color:#66d9ef>for</span> maybe <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>15</span> prompts, the ground truth answer<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>Then you can run this workflow without reflection<span style=color:#f92672>.</span> So without reflection would mean to take the SQL
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>46</span>
</span></span><span style=display:flex><span>query generated by the first LLM <span style=color:#f92672>and</span> to just see what answer it gives<span style=color:#f92672>.</span> And with reflection would
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>mean to take the database query generated after the second LLM has reflected on it to see what
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>answer that fetches from the database<span style=color:#f92672>.</span> And then we can measure the percentage of correct answers
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>03</span>
</span></span><span style=display:flex><span>from no reflection <span style=color:#f92672>and</span> with reflection<span style=color:#f92672>.</span> In this example, no reflection gets the answers
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>08</span>
</span></span><span style=display:flex><span>right <span style=color:#ae81ff>87</span><span style=color:#f92672>%</span> of the time, with reflection gets it right <span style=color:#ae81ff>95</span><span style=color:#f92672>%</span> of the time<span style=color:#f92672>.</span> And this would suggest that
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>reflection is meaningfully improving the quality of the database queries I<span style=color:#e6db74>&#39;m able to get to pull</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>out the correct answer<span style=color:#f92672>.</span> One thing that developers often end up doing as well is rewrite the reflection
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>prompt<span style=color:#f92672>.</span> So <span style=color:#66d9ef>for</span> example, <span style=color:#66d9ef>do</span> you want to add to reflection prompt an instruction to make the
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>database query run faster <span style=color:#f92672>or</span> make it clearer<span style=color:#960050;background-color:#1e0010>?</span> Or you may just have different ideas <span style=color:#66d9ef>for</span> how to
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>38</span>
</span></span><span style=display:flex><span>rewrite either the initial generation prompt <span style=color:#f92672>or</span> the reflection prompt<span style=color:#f92672>.</span> Once you put <span style=color:#f92672>in</span> place
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>44</span>
</span></span><span style=display:flex><span>evals like this, you can quickly try out different ideas <span style=color:#66d9ef>for</span> these prompts <span style=color:#f92672>and</span> measure the percentage
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>correct your system has as you change the prompts <span style=color:#f92672>in</span> order to get a sense of which prompts work
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>55</span>
</span></span><span style=display:flex><span>best <span style=color:#66d9ef>for</span> your application<span style=color:#f92672>.</span> So <span style=color:#66d9ef>if</span> you<span style=color:#e6db74>&#39;re trying out a lot of prompts, building evals is important.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>02</span>
</span></span><span style=display:flex><span>It really helps you have a systematic way to choose between the different prompts you might
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>07</span>
</span></span><span style=display:flex><span>be considering<span style=color:#f92672>.</span> But this example is one of when you can use objective evals because there is a
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>right answer<span style=color:#f92672>.</span> The number of items sold was <span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>301</span> <span style=color:#f92672>and</span> the answer is either right <span style=color:#f92672>or</span> wrong<span style=color:#f92672>.</span> How about
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>applications where you need more subjective rather than objective evaluations<span style=color:#960050;background-color:#1e0010>?</span> In the plotting
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>example that we saw <span style=color:#f92672>in</span> the last video, without reflection we had the stack bar graph, with reflection
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>we had this graph<span style=color:#f92672>.</span> But how <span style=color:#66d9ef>do</span> we know which plot is actually better<span style=color:#960050;background-color:#1e0010>?</span> I know I like the latter one
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>38</span>
</span></span><span style=display:flex><span>better, but with different graphs varying on different dimensions, how <span style=color:#66d9ef>do</span> we figure out which
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>43</span>
</span></span><span style=display:flex><span>one is better<span style=color:#960050;background-color:#1e0010>?</span> And measuring which of these plots is better is more of a subjective criteria rather
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>than a purely black <span style=color:#f92672>and</span> white objective criteria<span style=color:#f92672>.</span> So <span style=color:#66d9ef>for</span> these more subjective criteria, one thing
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>you might <span style=color:#66d9ef>do</span> is use an LLM as a judge<span style=color:#f92672>.</span> And maybe a basic approach to <span style=color:#66d9ef>do</span> this might be to feed both
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>plots into an LLM, a multi<span style=color:#f92672>-</span>modal LLM that can accept two images as input, <span style=color:#f92672>and</span> just ask it which image
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>is better<span style=color:#f92672>.</span> It turns out this doesn<span style=color:#e6db74>&#39;t work that well. I&#39;</span>ll share an even better idea <span style=color:#f92672>in</span> a second<span style=color:#f92672>.</span> But one
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>thing you could <span style=color:#66d9ef>do</span> might be to also give it some criteria by which to evaluate the two plots, such
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>as clarity, how nice looking they are, <span style=color:#f92672>and</span> so on<span style=color:#f92672>.</span> But it turns out that there<span style=color:#e6db74>&#39;s some known issues of</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>using LLMs to compare two inputs to tell you which one is better<span style=color:#f92672>.</span> First, it turns out the answers are
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>often <span style=color:#f92672>not</span> very good<span style=color:#f92672>.</span> It could be sensitive to the exact wording of the prompt of the LLM as a judge,
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>37</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> sometimes the rank ordering doesn<span style=color:#e6db74>&#39;t correspond that well to human expert judgment. And one</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>43</span>
</span></span><span style=display:flex><span>manifestation of this is many LLMs will have a position bias<span style=color:#f92672>.</span> Many LLMs, it turns out, will often
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>48</span>
</span></span><span style=display:flex><span>pick the first option more often than the second option<span style=color:#f92672>.</span> And <span style=color:#f92672>in</span> fact, I<span style=color:#e6db74>&#39;ve worked a lot of LLMs</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>54</span>
</span></span><span style=display:flex><span>where given two choices, whichever choice I present first, it will say the first choice is better<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>01</span>
</span></span><span style=display:flex><span>And maybe some LLMs prefer the second option, but I think most LLMs prefer the first option<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>06</span>
</span></span><span style=display:flex><span>Instead of asking an LLMs to compare a pair of inputs, grading with a rubric can give more
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span>consistent results<span style=color:#f92672>.</span> So, <span style=color:#66d9ef>for</span> example, you might prompt an LLM to tell it, given a single image,
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>18</span>
</span></span><span style=display:flex><span>assess the attached image against the quality rubric, <span style=color:#f92672>and</span> the rubric <span style=color:#f92672>or</span> grading criteria may
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>have clear criteria like does the plot have a clear title, are the access labels present,
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>is it an appropriate chart type, <span style=color:#f92672>and</span> so on, with a handful of criteria like this<span style=color:#f92672>.</span> And it turns out
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>that instead of asking the LLM to grade something on a scale of <span style=color:#ae81ff>1</span> to <span style=color:#ae81ff>5</span>, which it tends <span style=color:#f92672>not</span> to be
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>38</span>
</span></span><span style=display:flex><span>well calibrated on, <span style=color:#66d9ef>if</span> you instead give it, say, <span style=color:#ae81ff>5</span> binary criteria, <span style=color:#ae81ff>5</span><span style=color:#f92672>-</span><span style=color:#ae81ff>0</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> criteria, <span style=color:#f92672>and</span> have it give
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span> binary scores, <span style=color:#f92672>and</span> you add up those scores to get the number from <span style=color:#ae81ff>1</span> to <span style=color:#ae81ff>5</span> <span style=color:#f92672>or</span> <span style=color:#ae81ff>1</span> to <span style=color:#ae81ff>10</span> <span style=color:#66d9ef>if</span> you have
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>10</span> binary criteria, that tends to give more consistent results<span style=color:#f92672>.</span> And so <span style=color:#66d9ef>if</span> we<span style=color:#e6db74>&#39;re to gather a</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>handful, say <span style=color:#ae81ff>10</span><span style=color:#f92672>-</span><span style=color:#ae81ff>15</span> user queries <span style=color:#66d9ef>for</span> different visualizations that the user may want to have
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>of the coffee machine sales, then you can have it generate images without reflection <span style=color:#f92672>or</span> generate
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span>images with reflection, <span style=color:#f92672>and</span> use a rubric like this to score each of the images to then check
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>the degree to which <span style=color:#f92672>or</span> whether <span style=color:#f92672>or</span> <span style=color:#f92672>not</span> the images generated with reflection are really better than
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>the ones without reflection<span style=color:#f92672>.</span> And then once you<span style=color:#e6db74>&#39;ve built up a set of evals like this, if ever you</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>29</span>
</span></span><span style=display:flex><span>want to change the initial generation prompt <span style=color:#f92672>or</span> you want to change the reflection prompt, you can
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>33</span>
</span></span><span style=display:flex><span>also rerun this eval to see <span style=color:#66d9ef>if</span>, say, updating one of your prompts allows the system to generate images
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>40</span>
</span></span><span style=display:flex><span>that scores more points according to this rubric<span style=color:#f92672>.</span> And so this too gives you a way to keep on tuning
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>47</span>
</span></span><span style=display:flex><span>your prompts to get better <span style=color:#f92672>and</span> better performance<span style=color:#f92672>.</span> What you may find when building evaluations <span style=color:#66d9ef>for</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>53</span>
</span></span><span style=display:flex><span>reflection <span style=color:#f92672>or</span> <span style=color:#66d9ef>for</span> other agentic workflows is that when there is an objective criteria, code<span style=color:#f92672>-</span>based
</span></span><span style=display:flex><span><span style=color:#ae81ff>6</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>evaluation is usually easier to manage<span style=color:#f92672>.</span> And <span style=color:#f92672>in</span> the example that we saw with the database query, we
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>built up a database of ground truth examples <span style=color:#f92672>and</span> ground truth outputs <span style=color:#f92672>and</span> just wrote code to see
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>how often the system generated the right answer <span style=color:#f92672>in</span> a really objective evaluation metric<span style=color:#f92672>.</span> In contrast,
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> small subjective tasks, you might use an element as a judge but it usually takes a little
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>22</span>
</span></span><span style=display:flex><span>bit more tuning, such as having to think through what rubric you may want to use to get the LLM
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>as a judge to be well calibrated <span style=color:#f92672>or</span> to output reliable evals<span style=color:#f92672>.</span> So I hope that gives you a sense
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>33</span>
</span></span><span style=display:flex><span>of how to build evals to evaluate reflections <span style=color:#f92672>or</span> more generally even to evaluate different
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>38</span>
</span></span><span style=display:flex><span>agentic workflows<span style=color:#f92672>.</span> Knowing how to <span style=color:#66d9ef>do</span> evals well is really important <span style=color:#66d9ef>for</span> how you build agentic
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>45</span>
</span></span><span style=display:flex><span>workflows effectively <span style=color:#f92672>and</span> you hear me say more about this <span style=color:#f92672>in</span> later videos as well<span style=color:#f92672>.</span> But now that
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>you have a sense of how to use reflection, what I hope to <span style=color:#66d9ef>do</span> <span style=color:#f92672>in</span> the next video is a deep dive into
</span></span><span style=display:flex><span><span style=color:#ae81ff>7</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>one aspect of it, which is when you can get additional information from outside <span style=color:#f92672>and</span> this
</span></span><span style=display:flex><span><span style=color:#ae81ff>8</span>:<span style=color:#ae81ff>03</span>
</span></span><span style=display:flex><span>turns out to make reflection work much better<span style=color:#f92672>.</span> So <span style=color:#f92672>in</span> the final video of this module, let<span style=color:#e6db74>&#39;s take a</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>8</span>:<span style=color:#ae81ff>08</span>
</span></span><span style=display:flex><span>look at that technique <span style=color:#66d9ef>for</span> making your reflection workflows work much better<span style=color:#f92672>.</span> I<span style=color:#e6db74>&#39;ll see you in the</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>8</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>next video<span style=color:#f92672>.</span>
</span></span></code></pre></div><h2 id=25-using-external-feedback>2.5 Using external feedback</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-gdscript3 data-lang=gdscript3><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>03</span>
</span></span><span style=display:flex><span>Reflection with external feedback, <span style=color:#66d9ef>if</span> you can get it, is much more powerful than reflection
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>05</span>
</span></span><span style=display:flex><span>using the LLM as the only source of feedback<span style=color:#f92672>.</span> Let<span style=color:#e6db74>&#39;s take a look.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>09</span>
</span></span><span style=display:flex><span>When I<span style=color:#e6db74>&#39;m building an application, and if I&#39;</span>m just prompt engineering <span style=color:#66d9ef>for</span> direct generation
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span>of a zero<span style=color:#f92672>-</span>shot prompting, this is what performance might look like over time,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>18</span>
</span></span><span style=display:flex><span>where initially, as I tune the prompt, the performance improves <span style=color:#66d9ef>for</span> a <span style=color:#66d9ef>while</span>,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>but then after a <span style=color:#66d9ef>while</span>, it sort of plateaus <span style=color:#f92672>or</span> flattens out, <span style=color:#f92672>and</span> despite further engineering
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>the prompt, it<span style=color:#e6db74>&#39;s just hard to get that much better level of performance.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>So instead of wasting all this time on tuning the prompt, sometimes it<span style=color:#e6db74>&#39;d be better if only</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>earlier on <span style=color:#f92672>in</span> the process, I had started adding reflection, <span style=color:#f92672>and</span> sometimes that gives a bump <span style=color:#f92672>in</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>performance<span style=color:#f92672>.</span> Sometimes it<span style=color:#e6db74>&#39;s smaller, sometimes a bigger bump, but that adds complexity.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>46</span>
</span></span><span style=display:flex><span>But <span style=color:#66d9ef>if</span> I had started adding <span style=color:#f92672>in</span> reflection, maybe at this point <span style=color:#f92672>in</span> the process,
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>49</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> then started tuning the reflection prompt, then maybe I end up with a performance that
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>54</span>
</span></span><span style=display:flex><span>looks like this<span style=color:#f92672>.</span> But it turns out that <span style=color:#66d9ef>if</span> I<span style=color:#e6db74>&#39;m able to get external feedback,</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>so that the only source of new information isn<span style=color:#e6db74>&#39;t just an LLM reflecting on the same</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>03</span>
</span></span><span style=display:flex><span>information as it had before, but some new external information, then sometimes,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>08</span>
</span></span><span style=display:flex><span>as I <span style=color:#66d9ef>continue</span> to tune the prompts <span style=color:#f92672>and</span> tune the external feedback, you end up with
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span>an even much higher level of performance<span style=color:#f92672>.</span> So something to consider <span style=color:#66d9ef>if</span> you are working
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>on prompt engineering, <span style=color:#f92672>and</span> you feel that your efforts are seeing diminishing returns,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>that you<span style=color:#e6db74>&#39;re tuning a lot of prompts, but it&#39;</span>s just <span style=color:#f92672>not</span> getting that much better,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>then maybe consider <span style=color:#66d9ef>if</span> there<span style=color:#e6db74>&#39;s reflection, or even better, if there&#39;</span>s some external feedback
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>31</span>
</span></span><span style=display:flex><span>you can interject to bump the performance curve off this fattening out red line to maybe
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>36</span>
</span></span><span style=display:flex><span>some higher trajectory of performance improvement<span style=color:#f92672>.</span> Just as a reminder, we saw earlier,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>one source of feedback <span style=color:#66d9ef>for</span> <span style=color:#66d9ef>if</span> you<span style=color:#e6db74>&#39;re writing code would be if you were to just execute the code</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>47</span>
</span></span><span style=display:flex><span><span style=color:#f92672>and</span> see what output it generates, output <span style=color:#f92672>or</span> error messages, <span style=color:#f92672>and</span> feed that output back to the LLM
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>53</span>
</span></span><span style=display:flex><span>to let it have that new information to reflect, <span style=color:#f92672>and</span> then use that information to write a new
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>version of the code<span style=color:#f92672>.</span> Here are a few more examples of when software codes <span style=color:#f92672>or</span> tools can create new
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>05</span>
</span></span><span style=display:flex><span>information to help the reflection process<span style=color:#f92672>.</span> If you<span style=color:#e6db74>&#39;re using LLM to write emails, and it</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>sometimes mentions competitors<span style=color:#e6db74>&#39; names, then if you write codes or build a software tool</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>15</span>
</span></span><span style=display:flex><span>to just carry out pattern matching, maybe via regular expression pattern matching to search
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> competitors<span style=color:#e6db74>&#39; names in the output, then whenever you find a competitor&#39;</span>s name, you just feed that
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>25</span>
</span></span><span style=display:flex><span>back to the LLM as a criticism <span style=color:#f92672>or</span> as input<span style=color:#f92672>.</span> That<span style=color:#e6db74>&#39;s very useful information to tell it to just rewrite</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>the text without mentioning those competitors<span style=color:#f92672>.</span> Or as another example, you might use web search
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>39</span>
</span></span><span style=display:flex><span><span style=color:#f92672>or</span> look at other trusted sources <span style=color:#f92672>in</span> order to fact<span style=color:#f92672>-</span>check an essay<span style=color:#f92672>.</span> So <span style=color:#66d9ef>if</span> you<span style=color:#e6db74>&#39;re a research</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>44</span>
</span></span><span style=display:flex><span>agent that says the Taj Mahal was built <span style=color:#f92672>in</span> <span style=color:#ae81ff>1648</span>, technically the Taj Mahal was actually commissioned
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span><span style=color:#f92672>in</span> <span style=color:#ae81ff>1631</span>, <span style=color:#f92672>and</span> it was finished <span style=color:#f92672>in</span> <span style=color:#ae81ff>1648.</span> So maybe this isn<span style=color:#e6db74>&#39;t exactly incorrect, but it doesn&#39;</span>t
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>58</span>
</span></span><span style=display:flex><span>capture the accurate history either<span style=color:#f92672>.</span> In order to more accurately represent when this beautiful
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>building was built, <span style=color:#66d9ef>if</span> you <span style=color:#66d9ef>do</span> a web search to cuddle the snippet explaining exactly the period
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span>that the Taj Mahal was built <span style=color:#f92672>and</span> give that as additional input to your reflection agent, then
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>it may be able to use that to write a better version of the text on the history of the Taj Mahal<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>21</span>
</span></span><span style=display:flex><span>One last example, <span style=color:#66d9ef>if</span> you<span style=color:#e6db74>&#39;re using an LLM to write copy, maybe for a blog post or for a research</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>paper abstract, but what it writes is sometimes over the word limit<span style=color:#f92672>.</span> LLMs are still <span style=color:#f92672>not</span> very good
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>at following exact word limits<span style=color:#f92672>.</span> Then <span style=color:#66d9ef>if</span> you implement a word count <span style=color:#66d9ef>tool</span>, just write code to
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>37</span>
</span></span><span style=display:flex><span>count the exact number of words, <span style=color:#f92672>and</span> <span style=color:#66d9ef>if</span> it exceeds the word limit, then feed that word count back to
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>44</span>
</span></span><span style=display:flex><span>the LLM <span style=color:#f92672>and</span> ask it to try again<span style=color:#f92672>.</span> Then this helps it to more accurately hit the desired length of
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>51</span>
</span></span><span style=display:flex><span>the output you wanted to generate<span style=color:#f92672>.</span> So <span style=color:#f92672>in</span> each of these three examples, you can write a piece of
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>:<span style=color:#ae81ff>57</span>
</span></span><span style=display:flex><span>code to help find additional facts about the initial output to then give those facts, be it
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>04</span>
</span></span><span style=display:flex><span>that you found the competitor<span style=color:#e6db74>&#39;s name or information web search or the exact word count, to feed into</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>09</span>
</span></span><span style=display:flex><span>the reflection LLM <span style=color:#f92672>in</span> order to help it <span style=color:#66d9ef>do</span> a better job thinking about how to improve the output<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>Reflections are powerful too, <span style=color:#f92672>and</span> I hope you find it useful <span style=color:#f92672>in</span> a lot of your own work<span style=color:#f92672>.</span> In the next
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>23</span>
</span></span><span style=display:flex><span>module, we<span style=color:#e6db74>&#39;ll build on this to talk about tool use, where in addition to the handful of tool examples</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>29</span>
</span></span><span style=display:flex><span>you saw, you learn how to systematically get your LLM to call different functions, <span style=color:#f92672>and</span> this will make
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>35</span>
</span></span><span style=display:flex><span>your agenting applications much more powerful<span style=color:#f92672>.</span> I hope you enjoyed learning about reflection<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>:<span style=color:#ae81ff>41</span>
</span></span><span style=display:flex><span>I<span style=color:#e6db74>&#39;m going to now reflect on what you just learned. I hope to see you in the next video.</span>
</span></span></code></pre></div></div></div><div class="grid grid-cols-1 sm:grid-cols-2 gap-4 mt-10 border-t border-border pt-8"><a class="flex items-stretch gap-3 p-4 rounded-xl border border-border bg-surface transition-all duration-200 hover:shadow-md hover:border-accent/40 group no-underline text-inherit" href=/courses/andrew-ng-agentic-ai/lecture/lec-03/ aria-label="上一篇：「Scripts」Module 3: Tool Use"><div class="flex items-center text-2xl text-muted font-light group-hover:text-accent transition-colors">‹</div><div class="flex-1 min-w-0 flex flex-col justify-center"><div class="font-semibold text-base text-text line-clamp-2 group-hover:text-accent transition-colors">「Scripts」Module 3: Tool Use</div><div class="text-xs text-muted mt-1"><span>Andrew Ng</span>
·
<span>2025-10-17</span></div></div></a><a class="flex items-stretch gap-3 p-4 rounded-xl border border-border bg-surface transition-all duration-200 hover:shadow-md hover:border-accent/40 group no-underline text-inherit text-right" href=/courses/andrew-ng-agentic-ai/lecture/lec-01/ aria-label="下一篇：「Scripts」Module 1: Introduction to Agentic Workflows"><div class="flex-1 min-w-0 flex flex-col justify-center items-end"><div class="font-semibold text-base text-text line-clamp-2 group-hover:text-accent transition-colors">「Scripts」Module 1: Introduction to Agentic Workflows</div><div class="text-xs text-muted mt-1"><span>Andrew Ng</span>
·
<span>2025-10-17</span></div></div><div class="flex items-center text-2xl text-muted font-light group-hover:text-accent transition-colors">›</div></a></div><section class="comments mt-12 pt-8 border-t border-border" aria-label=Comments><div id=giscus_thread class=giscus data-repo=Linguage/Giscus data-repo-id=R_kgDOLxA-eA data-category=Announcements data-category-id=DIC_kwDOLxA-eM4Ce06R data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme-light=light data-theme-dark=dark data-lang=zh-CN></div><script>(function(){try{const e=document.getElementById("giscus_thread");if(!e)return;let n=null;try{const e=localStorage.getItem("theme");(e==="light"||e==="dark")&&(n=e)}catch{n=null}const o=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,i=n?n:o?"dark":"light",s=i==="dark"?e.dataset.themeDark||"dark":e.dataset.themeLight||"light";e.setAttribute("data-theme",s);const t=document.createElement("script");t.src="https://giscus.app/client.js",t.setAttribute("data-repo",e.dataset.repo||""),t.setAttribute("data-repo-id",e.dataset.repoId||""),t.setAttribute("data-category",e.dataset.category||"General"),t.setAttribute("data-category-id",e.dataset.categoryId||""),t.setAttribute("data-mapping",e.dataset.mapping||"pathname"),t.setAttribute("data-strict",String(e.dataset.strict||"0")),t.setAttribute("data-reactions-enabled",String(e.dataset.reactionsEnabled||"1")),t.setAttribute("data-emit-metadata",String(e.dataset.emitMetadata||"0")),t.setAttribute("data-input-position",e.dataset.inputPosition||"bottom"),t.setAttribute("data-theme",s),t.setAttribute("data-lang",e.dataset.lang||"zh-CN"),t.setAttribute("crossorigin","anonymous"),t.async=!0,e.parentNode.insertBefore(t,e.nextSibling)}catch{}})()</script></section></article><aside class="docs-toc sticky top-[84px] max-h-[calc(100vh-100px)] overflow-y-auto hidden xl:block md-theme-amp-toc" aria-label="On this page"><div class="toc-title uppercase mb-4">On this page</div><nav class="toc flex flex-col gap-1"><nav id=TableOfContents><ul><li><a href=#21-reflection-to-improve-outputs-of-a-task>2.1 Reflection to improve outputs of a task</a></li><li><a href=#22-why-not-just-direct-generation>2.2 why not just direct generation?</a></li><li><a href=#23-chart-generation-workflow>2.3 chart generation workflow</a></li><li><a href=#24-evaluating-the-impact-of-reflection>2.4 Evaluating the impact of reflection</a></li><li><a href=#25-using-external-feedback>2.5 Using external feedback</a></li></ul></nav></nav></aside></div><button class="fixed right-6 bottom-20 z-50 p-2 rounded-full bg-surface border border-border shadow-lg cursor-pointer opacity-0 transition-opacity duration-300 hover:bg-bg" id=backToTop aria-label="Back to top">↑</button><div id=mobileTocContainer class="xl:hidden fixed bottom-6 left-4 right-4 z-[80] flex flex-col items-center"><div id=mobileTocPanel class="w-full max-w-md bg-white/95 dark:bg-black/95 backdrop-blur-xl border border-slate-200 dark:border-white/10 rounded-xl shadow-2xl mb-3 max-h-[60vh] overflow-y-auto p-4 hidden transition-all duration-200 origin-bottom scale-95 opacity-0"><div class="flex justify-between items-center mb-4 sticky top-0 bg-white/95 dark:bg-black/95 backdrop-blur-xl pb-2 border-b border-slate-200 dark:border-white/10 -mx-4 px-4 pt-1 z-10"><h4 class="text-sm font-bold text-slate-900 dark:text-slate-100 flex items-center gap-2"><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="text-accent-2"><line x1="21" y1="10" x2="3" y2="10"/><line x1="21" y1="6" x2="3" y2="6"/><line x1="21" y1="14" x2="3" y2="14"/><line x1="21" y1="18" x2="3" y2="18"/></svg>
目录</h4><button id=mobileTocCloseBtn class="p-1 text-slate-500 hover:text-slate-900 dark:hover:text-slate-100 transition-colors">
<svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></button></div><nav class="toc text-sm leading-relaxed [&_a]:block [&_a]:py-1.5 [&_li]:my-1"><nav id=TableOfContents><ul><li><a href=#21-reflection-to-improve-outputs-of-a-task>2.1 Reflection to improve outputs of a task</a></li><li><a href=#22-why-not-just-direct-generation>2.2 why not just direct generation?</a></li><li><a href=#23-chart-generation-workflow>2.3 chart generation workflow</a></li><li><a href=#24-evaluating-the-impact-of-reflection>2.4 Evaluating the impact of reflection</a></li><li><a href=#25-using-external-feedback>2.5 Using external feedback</a></li></ul></nav></nav></div><button id=mobileTocToggleBtn class="w-full max-w-md bg-white/90 dark:bg-black/90 backdrop-blur-md border border-slate-200 dark:border-white/10 shadow-lg rounded-full px-5 py-3 flex items-center justify-between text-sm font-medium hover:bg-slate-50 dark:hover:bg-white/5 transition-all active:scale-[0.98]" aria-controls=mobileTocPanel aria-expanded=false aria-label="Toggle Table of Contents"><div class="flex items-center overflow-hidden mr-4"><div class="bg-slate-100 dark:bg-white/10 p-1.5 rounded-full mr-3 shrink-0"><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="text-slate-900 dark:text-slate-100"><line x1="8" y1="6" x2="21" y2="6"/><line x1="8" y1="12" x2="21" y2="12"/><line x1="8" y1="18" x2="21" y2="18"/><line x1="3" y1="6" x2="3.01" y2="6"/><line x1="3" y1="12" x2="3.01" y2="12"/><line x1="3" y1="18" x2="3.01" y2="18"/></svg></div><span id=mobileTocCurrentHeading class="truncate text-slate-900/90 dark:text-slate-100/90 font-medium text-left">目录</span></div><div id=mobileTocChevron class="text-slate-500 shrink-0 transition-transform duration-200"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 18l6-6-6-6"/></svg></div></button></div></main><script src="/script.js?v=20260121-08" defer></script><script defer src="/js/search-core.js?v=20260121-08"></script><script>window.SEARCH_INDEX_URL_LITE="/index-lite.json?v=20260121-08",window.SEARCH_INDEX_URL="/index.json?v=20260121-08"</script><script src="/js/search-advanced.js?v=20260121-08" defer></script><script defer src="/js/table-7char.js?v=20260121-08"></script><script defer src="/js/linkcard.js?v=20260121-08"></script><script defer src="/js/table-tooltips.js?v=20260121-08"></script><script defer src="/js/social-popup.js?v=20260121-08"></script><div id=social-popup-overlay aria-hidden=true class="fixed inset-0 z-[200] bg-black/60 backdrop-blur-sm flex items-center justify-center opacity-0 pointer-events-none transition-opacity duration-300"><div class="social-popup-card bg-surface p-6 rounded-2xl shadow-2xl max-w-sm w-[90%] relative transform scale-95 transition-transform duration-300" role=dialog aria-modal=true aria-labelledby=social-popup-title aria-describedby=social-popup-desc><button class="social-popup-close absolute top-3 right-3 p-2 rounded-full bg-muted/10 hover:bg-muted/20 text-muted transition-colors" type=button aria-label=关闭>×</button>
<img class="social-popup-image w-full h-auto rounded-lg mb-4" src alt><div class="social-popup-title text-xl font-semibold text-text mb-2" id=social-popup-title></div><div class="social-popup-desc text-sm text-muted" id=social-popup-desc></div></div></div></body></html>